{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 1**\n",
    "\n",
    "**Section 2**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### This notebook provides a poor man Map Reduce framework through command-line and python. Please note that I kept logging commented logging code to show my work."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Prepare your bio and include it in this HW submission. Please limit to 100 words. Count the words in your bio and print the length of your bio (in terms of words) in a separate cell.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "My name is Carlos Eduardo Rodriguez Castillo. I was born and raised in Caracas, Venezuela but currently live in Brooklyn, NY. I have been in New York City for the past eight years. I received my undergrad degree at Columbia University where I majored in Operations Research. I am in the May 2015 cohort of the MIDS program. I currently work at an adtech company called AppNexus as a team lead in our Services org. I enjoy being outside, riding my bike to the beach, go bouldering and rock climbing as well as going on skiing trips in the Rockies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The length of the bio is: 100\n"
     ]
    }
   ],
   "source": [
    "bio = \"My name is Carlos Eduardo Rodriguez Castillo. I was born and raised in Caracas, Venezuela but currently live in Brooklyn, NY. I have been in New York City for the past eight years. I received my undergrad degree at Columbia University where I majored in Operations Research. I am in the May 2015 cohort of the MIDS program. I currently work at an adtech company called AppNexus as a team lead in our Services org. I enjoy being outside, riding my bike to the beach, go bouldering and rock climbing as well as going on skiing trips in the Rockies.\"\n",
    "\n",
    "\n",
    "print \"The length of the bio is: %s\" % len(bio.split())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.0.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Define big data. Provide an example of a big data problem in your domain of expertise.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I consider big data to be data that has extreme volume, velocity and variety (or minimally 2 of these characteristics at the same time). I define extreme volume of data as an amount of data that cannot fit in a single machine to be processed in a meaningful way (e.g. several hundred gygabytes as a minimum). I define data that has extreme velocity as data that is generated (at the atomic record level) multiple times per second. I define data that has extreme variety as data that is multidimensional. A corollary of this definition is that big data requires special methods to be effectively processed and utilized in a meaningful way as traditional data processing methodologies are inadequate for big data.\n",
    "\n",
    "Ad tech, the industry that I work in, has no shortage of big data problems. One of them is the primary issue of counting and reporting impressions bought by media-buying campaigns. Given that campaigns can buy hundreds of thousands of impressions per day (velocity), and the fact that each impression has tens of fields to report on (variety), which make up for tens to hundreds of gigabytes of data created per day, soring and processing these logs quickly turns into an exercise of processing several terabytes of data (volume)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.0.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__In 500 words (English or pseudo code or a combination) describe how to estimate the bias, the variance, the irreduciable error for a test dataset T when using polynomial regression models of degree 1, 2,3, 4,5 are considered. How would you select a model?__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "For a test dataset __T__, the sum of the [squared] bias, the variance and the irreducible error can be estimated by calculating the prediction error of the model (which I will assume was computed using a separate development dataset __D__) on __T__.\n",
    "\n",
    "Punctually, given the constraints in the instructions, this can be achieved by separately fitting [polynomial regression models](https://en.wikipedia.org/wiki/Polynomial_regression) of degrees 1, 2, 3, 4, and 5 to __D__ and then having each model predict on __T__. The prediction error from each of the models (which in this case I will define as the [mean squared error](https://en.wikipedia.org/wiki/Mean_squared_error)) could then be ploted on a graph to indicate their relative order, which in turn gives us an (abeit potentially quite poor) estimate for the sum of the [squared] bias, the variance and the irreducible error as a function of the polynomial order of a regression model.\n",
    "\n",
    "__Polynomial Regression Models Fitting test data set T__\n",
    "![alt text]()\n",
    "__Prediction error for the different Polynomial Regression Models__\n",
    "![alt text]()\n",
    "\n",
    "The reason that the [squared] bias, variance and irreducible error can be estimated from the prediction error on __T__ is that the expected value of the unconditional prediction error is equivalent to the sum of these three values. Since we do not realistically have access to the full population of test data sets (from which __T__ was necessarily pulled), the best we can do is estimate the prediction error by sampling from the test data sets population (note that in the same fashion that we estimate from/sample test data sets, we analogously follow the process for development data sets such as __D__ to estimate the 'true' model for each of the selected polynomial orders). Since we only have access to the single test data set __T__, by definition our estimate for the expected prediction error (and thus for the variance, [squared] bias and irreducible error) will be poor; the larger the sample of test data sets to compute the estimate from, the more the estimate should approximate the true value for the unconditional expected prediction error. In order to get around this issue, I would attempt to generate a sample of development and test data sets by means of bootstrapping __D__ and __T__ respectively. This way we could simulate a larger sample of development and test data sets from which to estimate the expected unconditional prediction error.\n",
    "\n",
    "Finally, using the above graph as an input, I would select the model with the lowest prediction error on __T__ because, as I stated earlier we estimate that it would give us the lowest sum of bias and variance (we ignore irreducible error as by definition it cannot be generally reduced) which in turn returns the model with the best generalizable precision and accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Simple EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!wc -l enronemail_1h.txt  #100 email records\n",
    "!cut -f2 -d$'\\t' enronemail_1h.txt|wc  #extract second field which is SPAM flag\n",
    "!cut -f2 -d$'\\t' enronemail_1h.txt|head\n",
    "!head -n 100 enronemail_1h.txt|tail -1|less #an example SPAM email record"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Read through the provided control script (pNaiveBayes.sh) and all of its comments. When you are comfortable with their purpose and function, respond to the remaining homework questions below. A simple cell in the notebook with a print statmement with  a  \"done\" (print \"done\") string will suffice here. (don't forget to include the Question Number and the question in the cell as a markdown multiline comment!)__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print \"done\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Provide a mapper/reducer pair that, when executed by **pNaiveBayes.sh** will determine the number of occurrences of a single, user-specified word. Examine the word “assistance” and report your results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: mapper code for HW1.2\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "\n",
    "## setting up logger\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "## NOTE: make sure to set the logging directory path appropriately as the below one is custom!!!\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'map_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "\n",
    "with open (filename, \"r\") as myfile:\n",
    "    # Reading input file line by line\n",
    "    for line in myfile:\n",
    "        logging.debug('Line: %s'% line)\n",
    "        words = line.split()\n",
    "        for word in words:\n",
    "            # Doing minimal tocken clean up\n",
    "            logging.debug('Word: %s' % word)\n",
    "            word = word.rstrip(',')\n",
    "            word = word.rstrip(';')\n",
    "            word = word.rstrip(':')\n",
    "            word = word.rstrip('.')\n",
    "            word = word.rstrip('\"')\n",
    "            word = word.lstrip('\"')\n",
    "            # Scanning for our word of choice\n",
    "            for findword in findwords:\n",
    "                if word == findword:\n",
    "                    # increment when word encountered is word of choice\n",
    "                    logging.debug('Word: [%s] is equal to [%s]! Increment!' % (word, findword))\n",
    "                    count = count + 1\n",
    "    # This is a special case where findwords has a single entry\n",
    "    print \"%s %d\" % (findwords[0], count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: mapper code for HW1.2\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "\n",
    "## setting up logger\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "## NOTE: make sure to set the logging directory path appropriately as the below one is custom!!!\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'reduce_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collect mapper input\n",
    "logging.debug('Input file chunks: %s'% sys.argv)\n",
    "## removing the first element of sys.argv\n",
    "filenames = sys.argv[1:]\n",
    "\n",
    "sum = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    logging.debug('processing input file: %s'% filename)\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        for line in myfile:\n",
    "            inputs = line.split()\n",
    "            # This is the special case where there is a single word to be counted\n",
    "            word = inputs[0]\n",
    "            chunk_count = int(inputs[1])\n",
    "            logging.debug('Sum line: %s'% line)\n",
    "            sum = sum + int(chunk_count)\n",
    "            logging.debug('Sum: %s ' % sum)\n",
    "print \"%s\\t%d\" % (word, sum)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Overwrite mapper.py and reducer.py."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Jake Ryland Williams\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    " \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    " \n",
    "## a test set data of 100 messages\n",
    "#data=`cat enronemail_1h.txt | cut -d$'\\t' -f 3,4`\n",
    "data=\"enronemail_1h.txt\" \n",
    "    \n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    " \n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesindata\"\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesinchunk\"\n",
    " \n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    " \n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ##### debugging line\n",
    "#     echo \"MAPPING CHUNK: $datachunk\"\n",
    "    \n",
    "    ./mapper.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    \n",
    "    ##### debugging line\n",
    "#     echo \"FINISHED MAPPING CHUNK: $datachunk\"\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    " \n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$countfiles\"\n",
    " \n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "##### debugging line\n",
    "# echo \"REDUCING CHUNKS\"\n",
    "./reducer.py $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## print results\n",
    "\n",
    "cat $data.output\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "assistance\t10\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes.sh 2 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER: We see that the number of occurences of the word assistance is ten.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Provide a mapper/reducer pair that, when executed by *pNaiveBayes.sh* will classify the email messages by a single, user-specified word using the multinomial Naive Bayes Formulation. Examine the word “assistance” and report your results.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_2.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: mapper code for HW1.3\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "\n",
    "## setting up logger\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "## NOTE: make sure to set the logging directory path appropriately as the below one is custom!!!\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'map_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "\n",
    "## collecting user input\n",
    "filename = sys.argv[1]\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "logging.debug(\"The words to be processed are: %s\" % ','.join(findwords))\n",
    "\n",
    "## setting up JSON that contains emmitted information\n",
    "emit_dict = {}\n",
    "emit_dict['SPAM_total_counter'] = 0\n",
    "emit_dict['HAM_total_counter'] = 0\n",
    "emit_dict['words'] = {}\n",
    "\n",
    "## Initializing counts for all the words in the vocabulary\n",
    "for findword in findwords:\n",
    "    emit_dict['words'][findword] = {\"SPAM_count\":0,\"HAM_count\":0}\n",
    "    emit_dict['words'][findword] = {\"SPAM_count\":0,\"HAM_count\":0}\n",
    "\n",
    "count = 0\n",
    "total_count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## opening the chunk file for mapping\n",
    "with open (filename, \"r\") as myfile:\n",
    "    ## iterating line by line\n",
    "    for line in myfile:\n",
    "        HAM = True\n",
    "        line_components = line.split(\"\\t\")\n",
    "        output_variable = int(line_components[1])\n",
    "        ## check whether the training document is SPAM or HAM\n",
    "#         if int(output_variable) == 0:\n",
    "#             emit_dict['HAM_total_counter'] = emit_dict['HAM_total_counter'] + 1\n",
    "#         else:\n",
    "#             emit_dict['SPAM_total_counter'] = emit_dict['SPAM_total_counter'] + 1\n",
    "#             HAM = False\n",
    "        line = ' '.join([str(x) for x in line_components[2:]])\n",
    "        logging.debug('Line: %s'% line)\n",
    "        words = line.split()\n",
    "        ## iterating through words in document\n",
    "        for word in words:\n",
    "            ## minimally trimming words in document\n",
    "            logging.debug('Word: %s' % word)\n",
    "            word = word.rstrip(',')\n",
    "            word = word.rstrip(';')\n",
    "            word = word.rstrip(':')\n",
    "            word = word.rstrip('.')\n",
    "            word = word.rstrip('\"')\n",
    "            word = word.lstrip('\"')\n",
    "            ## incrementing count of word for each of the classes as necessary\n",
    "            for findword in findwords:\n",
    "                if word == findword:\n",
    "                    ## check whether the training document is SPAM or HAM\n",
    "                    if int(output_variable) == 0:\n",
    "                        emit_dict['HAM_total_counter'] = emit_dict['HAM_total_counter'] + 1\n",
    "                    else:\n",
    "                        emit_dict['SPAM_total_counter'] = emit_dict['SPAM_total_counter'] + 1\n",
    "                        HAM = False\n",
    "                    logging.debug('Word: [%s] is equal to [%s]! Increment!' % (word, findword))\n",
    "                    if HAM:\n",
    "                        emit_dict['words'][findword]['HAM_count'] = emit_dict['words'][findword]['HAM_count'] + 1\n",
    "                    else:\n",
    "                        emit_dict['words'][findword]['SPAM_count'] = emit_dict['words'][findword]['SPAM_count'] + 1\n",
    "    print json.dumps(emit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_2.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: reducer code for HW1.3\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pprint\n",
    "\n",
    "## setting up logger\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "## NOTE: make sure to set the logging directory path appropriately as the below one is custom!!!\n",
    "################################################################################################\n",
    "\n",
    "################################################################################################\n",
    "\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'reduce_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collecting mapper input\n",
    "logging.debug('Input file chunks: %s'% sys.argv)\n",
    "## taking the raw set of documents as an input\n",
    "raw_data = sys.argv[1]\n",
    "## taking the chunk files array as an input\n",
    "filenames = sys.argv[2:]\n",
    "\n",
    "## set up JSON that contains emmitted information\n",
    "reduced_dict = {}\n",
    "reduced_dict['SPAM_total_counter'] = 0\n",
    "reduced_dict['HAM_total_counter'] = 0\n",
    "reduced_dict['words'] = {}\n",
    "\n",
    "sum = 0\n",
    "accurate_count = 0\n",
    "inaccurate_count = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    logging.debug('processing input file: %s'% filename)\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        data = json.load(myfile)\n",
    "        logging.debug('MAP_DICT -> SPAM_total_counter: %s ' % data['SPAM_total_counter'])\n",
    "        reduced_dict['SPAM_total_counter'] = reduced_dict['SPAM_total_counter'] + data['SPAM_total_counter']\n",
    "        logging.debug('REDUCED_DICT -> SPAM_total_counter: %s ' % reduced_dict['SPAM_total_counter'])\n",
    "        logging.debug('MAP_DICT -> HAM_total_counter: %s ' % data['HAM_total_counter'])\n",
    "        reduced_dict['HAM_total_counter'] = reduced_dict['HAM_total_counter'] + data['HAM_total_counter']\n",
    "        logging.debug('REDUCED_DICT -> HAM_total_counter: %s ' % reduced_dict['HAM_total_counter'])\n",
    "        for word in data['words']:\n",
    "            if word in reduced_dict['words'].keys():\n",
    "                logging.debug('%s -> HAM_counter: %s ' % (word,reduced_dict['words'][word]['HAM_count']))\n",
    "                reduced_dict['words'][word]['HAM_count'] = reduced_dict['words'][word]['HAM_count'] + data['words'][word]['HAM_count']\n",
    "                reduced_dict['words'][word]['SPAM_count'] = reduced_dict['words'][word]['SPAM_count'] + data['words'][word]['SPAM_count']\n",
    "            else:\n",
    "                reduced_dict['words'][word] = data['words'][word]\n",
    "                \n",
    "for word in reduced_dict['words'].keys():\n",
    "    \n",
    "    reduced_dict['words'][word]['SPAM_cond_prob'] = float(reduced_dict['words'][word]['SPAM_count']) / float(reduced_dict['SPAM_total_counter'])\n",
    "    reduced_dict['words'][word]['HAM_cond_prob'] = float(reduced_dict['words'][word]['HAM_count']) / float(reduced_dict['HAM_total_counter'])\n",
    "\n",
    "total_words = reduced_dict['SPAM_total_counter'] + reduced_dict['HAM_total_counter']\n",
    "reduced_dict['SPAM_prob_total'] = float(reduced_dict['SPAM_total_counter']) / float(total_words)\n",
    "reduced_dict['HAM_prob_total'] = float(reduced_dict['HAM_total_counter']) / float(total_words)\n",
    "\n",
    "for word in reduced_dict['words'].keys():\n",
    "    reduced_dict['words'][word]['HAM_prob_cond_doc'] = float(reduced_dict['HAM_prob_total']) * float(reduced_dict['words'][word]['HAM_cond_prob'])\n",
    "    reduced_dict['words'][word]['SPAM_prob_cond_doc'] = float(reduced_dict['SPAM_prob_total']) * float(reduced_dict['words'][word]['SPAM_cond_prob'])\n",
    "    reduced_dict['words'][word]['IS_SPAM'] = (1 if reduced_dict['words'][word]['SPAM_prob_cond_doc'] > reduced_dict['words'][word]['HAM_prob_cond_doc'] else 0)\n",
    "\n",
    "total_documents = 0\n",
    "\n",
    "## running classification now that model has been fitted\n",
    "with open (raw_data, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        total_documents = total_documents + 1\n",
    "        found_vocabulary = False\n",
    "        line_components = line.split(\"\\t\")\n",
    "        output_variable = int(line_components[1])\n",
    "        variable_id = line_components[0]\n",
    "        line = ' '.join([str(x) for x in line_components[2:]])\n",
    "        logging.debug('Line: %s'% line)\n",
    "        words = line.split()\n",
    "        for findword in reduced_dict['words'].keys():\n",
    "            for word in words:\n",
    "                logging.debug('Word: %s' % word)\n",
    "                word = word.rstrip(',')\n",
    "                word = word.rstrip(';')\n",
    "                word = word.rstrip(':')\n",
    "                word = word.rstrip('.')\n",
    "                word = word.rstrip('\"')\n",
    "                word = word.lstrip('\"')\n",
    "                if word == findword:\n",
    "                    found_vocabulary = True\n",
    "                    if output_variable == reduced_dict['words'][word]['IS_SPAM']:\n",
    "                        accurate_count = accurate_count + 1\n",
    "                    else:\n",
    "                        inaccurate_count = inaccurate_count + 1\n",
    "                    break          \n",
    "        if not found_vocabulary:\n",
    "            if reduced_dict['SPAM_prob_total'] > reduced_dict['HAM_prob_total']:\n",
    "                predited_outcome = 1\n",
    "            else:\n",
    "                predited_outcome = 0\n",
    "            if output_variable == predited_outcome:\n",
    "                accurate_count = accurate_count + 1\n",
    "            else:\n",
    "                inaccurate_count = inaccurate_count + 1\n",
    "        print \"%s\\t%d\\t%d\" % (variable_id, output_variable, predited_outcome)\n",
    "print \"Accurate count: %s\" % accurate_count\n",
    "print \"Inaccurate count: %s\" % inaccurate_count\n",
    "print \"accurate_count type: %s\" % type(accurate_count)\n",
    "print \"Multinomial Naive Bayes accuracy: %.2f\" % (float(int(accurate_count)) / float(total_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes_2.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes_2.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Carlos Eduardo Rodriguez Castillo (original author: Jake Ryland Williams)\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    " \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    " \n",
    "## a test set data of 100 messages\n",
    "#data=`cat enronemail_1h.txt | cut -d$'\\t' -f 3,4`\n",
    "data=\"enronemail_1h.txt\" \n",
    "    \n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    " \n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesindata\"\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesinchunk\"\n",
    " \n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    " \n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ##### debugging line\n",
    "#     echo \"MAPPING CHUNK: $datachunk\"\n",
    "    \n",
    "    ./mapper_2.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    \n",
    "    ##### debugging line\n",
    "#     echo \"FINISHED MAPPING CHUNK: $datachunk\"\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    " \n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$countfiles\"\n",
    " \n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "##### debugging line\n",
    "# echo \"REDUCING CHUNKS\"\n",
    "./reducer_2.py $data $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## print results\n",
    "\n",
    "cat $data.output\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes_2.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t1\n",
      "0001.1999-12-10.kaminski\t0\t1\n",
      "0001.2000-01-17.beck\t0\t1\n",
      "0001.2000-06-06.lokay\t0\t1\n",
      "0001.2001-02-07.kitchen\t0\t1\n",
      "0001.2001-04-02.williams\t0\t1\n",
      "0002.1999-12-13.farmer\t0\t1\n",
      "0002.2001-02-07.kitchen\t0\t1\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\n",
      "0002.2003-12-18.GP\t1\t1\n",
      "0002.2004-08-01.BG\t1\t1\n",
      "0003.1999-12-10.kaminski\t0\t1\n",
      "0003.1999-12-14.farmer\t0\t1\n",
      "0003.2000-01-17.beck\t0\t1\n",
      "0003.2001-02-08.kitchen\t0\t1\n",
      "0003.2003-12-18.GP\t1\t1\n",
      "0003.2004-08-01.BG\t1\t1\n",
      "0004.1999-12-10.kaminski\t0\t1\n",
      "0004.1999-12-14.farmer\t0\t1\n",
      "0004.2001-04-02.williams\t0\t1\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\n",
      "0004.2004-08-01.BG\t1\t1\n",
      "0005.1999-12-12.kaminski\t0\t1\n",
      "0005.1999-12-14.farmer\t0\t1\n",
      "0005.2000-06-06.lokay\t0\t1\n",
      "0005.2001-02-08.kitchen\t0\t1\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\n",
      "0005.2003-12-18.GP\t1\t1\n",
      "0006.1999-12-13.kaminski\t0\t1\n",
      "0006.2001-02-08.kitchen\t0\t1\n",
      "0006.2001-04-03.williams\t0\t1\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\n",
      "0006.2003-12-18.GP\t1\t1\n",
      "0006.2004-08-01.BG\t1\t1\n",
      "0007.1999-12-13.kaminski\t0\t1\n",
      "0007.1999-12-14.farmer\t0\t1\n",
      "0007.2000-01-17.beck\t0\t1\n",
      "0007.2001-02-09.kitchen\t0\t1\n",
      "0007.2003-12-18.GP\t1\t1\n",
      "0007.2004-08-01.BG\t1\t1\n",
      "0008.2001-02-09.kitchen\t0\t1\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\n",
      "0008.2003-12-18.GP\t1\t1\n",
      "0008.2004-08-01.BG\t1\t1\n",
      "0009.1999-12-13.kaminski\t0\t1\n",
      "0009.1999-12-14.farmer\t0\t1\n",
      "0009.2000-06-07.lokay\t0\t1\n",
      "0009.2001-02-09.kitchen\t0\t1\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\n",
      "0009.2003-12-18.GP\t1\t1\n",
      "0010.1999-12-14.farmer\t0\t1\n",
      "0010.1999-12-14.kaminski\t0\t1\n",
      "0010.2001-02-09.kitchen\t0\t1\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\n",
      "0010.2003-12-18.GP\t1\t1\n",
      "0010.2004-08-01.BG\t1\t1\n",
      "0011.1999-12-14.farmer\t0\t1\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\n",
      "0011.2003-12-18.GP\t1\t1\n",
      "0011.2004-08-01.BG\t1\t1\n",
      "0012.1999-12-14.farmer\t0\t1\n",
      "0012.1999-12-14.kaminski\t0\t1\n",
      "0012.2000-01-17.beck\t0\t1\n",
      "0012.2000-06-08.lokay\t0\t1\n",
      "0012.2001-02-09.kitchen\t0\t1\n",
      "0012.2003-12-19.GP\t1\t1\n",
      "0013.1999-12-14.farmer\t0\t1\n",
      "0013.1999-12-14.kaminski\t0\t1\n",
      "0013.2001-04-03.williams\t0\t1\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\n",
      "0013.2004-08-01.BG\t1\t1\n",
      "0014.1999-12-14.kaminski\t0\t1\n",
      "0014.1999-12-15.farmer\t0\t1\n",
      "0014.2001-02-12.kitchen\t0\t1\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\n",
      "0014.2003-12-19.GP\t1\t1\n",
      "0014.2004-08-01.BG\t1\t1\n",
      "0015.1999-12-14.kaminski\t0\t1\n",
      "0015.1999-12-15.farmer\t0\t1\n",
      "0015.2000-06-09.lokay\t0\t1\n",
      "0015.2001-02-12.kitchen\t0\t1\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\n",
      "0015.2003-12-19.GP\t1\t1\n",
      "0016.1999-12-15.farmer\t0\t1\n",
      "0016.2001-02-12.kitchen\t0\t1\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\n",
      "0016.2003-12-19.GP\t1\t1\n",
      "0016.2004-08-01.BG\t1\t1\n",
      "0017.1999-12-14.kaminski\t0\t1\n",
      "0017.2000-01-17.beck\t0\t1\n",
      "0017.2001-04-03.williams\t0\t1\n",
      "0017.2003-12-18.GP\t1\t1\n",
      "0017.2004-08-01.BG\t1\t1\n",
      "0017.2004-08-02.BG\t1\t1\n",
      "0018.1999-12-14.kaminski\t0\t1\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\n",
      "0018.2003-12-18.GP\t1\t1\n",
      "Accurate count: 44\n",
      "Inaccurate count: 56\n",
      "accurate_count type: <type 'int'>\n",
      "Multinomial Naive Bayes accuracy: 0.44\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes_2.sh 2 \"assistance\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER: We see that our accuracy is poor low (44%).__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.4"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__Provide a mapper/reducer pair that, when executed by *pNaiveBayes.sh* will classify the email messages by a list of one or more user-specified words. Examine the words “assistance”, “valium”, and “enlargementWithATypo” and report your results (accuracy).__\n",
    "\n",
    "*__For this exercise I will use the same mapper code that I wrote for HW1.3. That said, I need to enhance the reducer code such that it takes into consideration the probabilities of multiple vocabulary words in computing the final predicted class for each document.__*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_3.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: reducer code for HW1.4\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pprint\n",
    "from math import log\n",
    "\n",
    "## setting up logger\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'reduce_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collecting mapper input\n",
    "logging.debug('Input file chunks: %s'% sys.argv)\n",
    "## taking the raw set of documents as an input\n",
    "raw_data = sys.argv[1]\n",
    "## taking the chunk files array as an input\n",
    "filenames = sys.argv[2:]\n",
    "\n",
    "## set up JSON that contains emmitted information\n",
    "reduced_dict = {}\n",
    "reduced_dict['SPAM_total_counter'] = 0\n",
    "reduced_dict['HAM_total_counter'] = 0\n",
    "reduced_dict['words'] = {}\n",
    "\n",
    "sum = 0\n",
    "accurate_count = 0\n",
    "inaccurate_count = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    logging.debug('processing input file: %s'% filename)\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        data = json.load(myfile)\n",
    "        logging.debug('MAP_DICT -> SPAM_total_counter: %s ' % data['SPAM_total_counter'])\n",
    "        reduced_dict['SPAM_total_counter'] = reduced_dict['SPAM_total_counter'] + data['SPAM_total_counter']\n",
    "        logging.debug('REDUCED_DICT -> SPAM_total_counter: %s ' % reduced_dict['SPAM_total_counter'])\n",
    "        logging.debug('MAP_DICT -> HAM_total_counter: %s ' % data['HAM_total_counter'])\n",
    "        reduced_dict['HAM_total_counter'] = reduced_dict['HAM_total_counter'] + data['HAM_total_counter']\n",
    "        logging.debug('REDUCED_DICT -> HAM_total_counter: %s ' % reduced_dict['HAM_total_counter'])\n",
    "        for word in data['words']:\n",
    "            if word in reduced_dict['words'].keys():\n",
    "                logging.debug('%s -> HAM_counter: %s ' % (word,reduced_dict['words'][word]['HAM_count']))\n",
    "                reduced_dict['words'][word]['HAM_count'] = reduced_dict['words'][word]['HAM_count'] + data['words'][word]['HAM_count']\n",
    "                reduced_dict['words'][word]['SPAM_count'] = reduced_dict['words'][word]['SPAM_count'] + data['words'][word]['SPAM_count']\n",
    "            else:\n",
    "                reduced_dict['words'][word] = data['words'][word]\n",
    "                \n",
    "for word in reduced_dict['words'].keys():\n",
    "    \n",
    "    reduced_dict['words'][word]['SPAM_cond_prob'] = float(reduced_dict['words'][word]['SPAM_count']) / float(reduced_dict['SPAM_total_counter'])\n",
    "    reduced_dict['words'][word]['HAM_cond_prob'] = float(reduced_dict['words'][word]['HAM_count']) / float(reduced_dict['HAM_total_counter'])\n",
    "\n",
    "total_words = reduced_dict['SPAM_total_counter'] + reduced_dict['HAM_total_counter']\n",
    "reduced_dict['SPAM_prob_total'] = float(reduced_dict['SPAM_total_counter']) / float(total_words)\n",
    "reduced_dict['HAM_prob_total'] = float(reduced_dict['HAM_total_counter']) / float(total_words)\n",
    "\n",
    "HAM_prob_cond_doc = log(reduced_dict['HAM_prob_total'])\n",
    "SPAM_prob_cond_doc = log(reduced_dict['SPAM_prob_total'])\n",
    "\n",
    "total_documents = 0\n",
    "\n",
    "## running classification now that model has been fitted\n",
    "with open (raw_data, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        total_documents = total_documents + 1\n",
    "        found_vocabulary = False\n",
    "        line_components = line.split(\"\\t\")\n",
    "        output_variable = int(line_components[1])\n",
    "        variable_id = line_components[0]\n",
    "        line = ' '.join([str(x) for x in line_components[2:]])\n",
    "        logging.debug('Line: %s'% line)\n",
    "        words = line.split()\n",
    "        for findword in reduced_dict['words'].keys():\n",
    "            for word in words:\n",
    "                logging.debug('Word: %s' % word)\n",
    "                word = word.rstrip(',')\n",
    "                word = word.rstrip(';')\n",
    "                word = word.rstrip(':')\n",
    "                word = word.rstrip('.')\n",
    "                word = word.rstrip('\"')\n",
    "                word = word.lstrip('\"')\n",
    "                if word == findword:\n",
    "                    if reduced_dict['words'][word]['HAM_cond_prob'] > 0:\n",
    "                        HAM_prob_cond_doc = HAM_prob_cond_doc + log(reduced_dict['words'][word]['HAM_cond_prob'])\n",
    "                    if reduced_dict['words'][word]['SPAM_cond_prob'] > 0:\n",
    "                        SPAM_prob_cond_doc = SPAM_prob_cond_doc  + log(reduced_dict['words'][word]['SPAM_cond_prob'])\n",
    "                    found_vocabulary = True\n",
    "        if found_vocabulary:\n",
    "            if HAM_prob_cond_doc >= SPAM_prob_cond_doc:\n",
    "                predicted_outcome = 0\n",
    "            else:\n",
    "                predicted_outcome = 1      \n",
    "        if not found_vocabulary:\n",
    "            if reduced_dict['HAM_prob_total'] >= reduced_dict['SPAM_prob_total']:\n",
    "                predicted_outcome = 0\n",
    "            else:\n",
    "                predicted_outcome = 1\n",
    "        if output_variable == predicted_outcome:\n",
    "            accurate_count = accurate_count + 1\n",
    "        else:\n",
    "            inaccurate_count = inaccurate_count + 1\n",
    "        print \"%s\\t%d\\t%d\" % (variable_id, output_variable, predicted_outcome)\n",
    "print \"Accurate count: %s\" % accurate_count\n",
    "print \"Inaccurate count: %s\" % inaccurate_count\n",
    "print \"accurate_count type: %s\" % type(accurate_count)\n",
    "print \"Multinomial Naive Bayes accuracy: %.2f\" % (float(int(accurate_count)) / float(total_documents))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes_3.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes_3.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Carlos Eduardo Rodriguez Castillo (original author: Jake Ryland Williams)\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    " \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    " \n",
    "## a test set data of 100 messages\n",
    "#data=`cat enronemail_1h.txt | cut -d$'\\t' -f 3,4`\n",
    "data=\"enronemail_1h.txt\" \n",
    "    \n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    " \n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesindata\"\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesinchunk\"\n",
    " \n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    " \n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ##### debugging line\n",
    "#     echo \"MAPPING CHUNK: $datachunk\"\n",
    "    \n",
    "    ./mapper_2.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    \n",
    "    ##### debugging line\n",
    "#     echo \"FINISHED MAPPING CHUNK: $datachunk\"\n",
    "#     echo \"THE WORDLIST IS $wordlist\"\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "##### debugging lines\n",
    "# echo \"Content of transmitted data (raw):\"\n",
    "# for file in $data.chunk.*.counts; do\n",
    "#   cat $file\n",
    "# done\n",
    " \n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## debugging lines\n",
    "# echo \"$countfiles\"\n",
    "# echo \"Content of transmitted data:\"\n",
    "\n",
    "# for file in $countfiles; do\n",
    "#   cat $file\n",
    "# done\n",
    " \n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "##### debugging line\n",
    "# echo \"REDUCING CHUNKS\"\n",
    "./reducer_3.py $data $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## print results\n",
    "\n",
    "cat $data.output\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes_3.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t1\r\n",
      "0001.1999-12-10.kaminski\t0\t1\r\n",
      "0001.2000-01-17.beck\t0\t1\r\n",
      "0001.2000-06-06.lokay\t0\t1\r\n",
      "0001.2001-02-07.kitchen\t0\t1\r\n",
      "0001.2001-04-02.williams\t0\t1\r\n",
      "0002.1999-12-13.farmer\t0\t1\r\n",
      "0002.2001-02-07.kitchen\t0\t1\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t1\r\n",
      "0003.1999-12-14.farmer\t0\t1\r\n",
      "0003.2000-01-17.beck\t0\t1\r\n",
      "0003.2001-02-08.kitchen\t0\t1\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t1\r\n",
      "0004.1999-12-14.farmer\t0\t1\r\n",
      "0004.2001-04-02.williams\t0\t1\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t1\r\n",
      "0005.1999-12-14.farmer\t0\t1\r\n",
      "0005.2000-06-06.lokay\t0\t1\r\n",
      "0005.2001-02-08.kitchen\t0\t1\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t1\r\n",
      "0006.2001-02-08.kitchen\t0\t1\r\n",
      "0006.2001-04-03.williams\t0\t1\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t1\r\n",
      "0007.1999-12-14.farmer\t0\t1\r\n",
      "0007.2000-01-17.beck\t0\t1\r\n",
      "0007.2001-02-09.kitchen\t0\t1\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t1\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t1\r\n",
      "0009.1999-12-14.farmer\t0\t1\r\n",
      "0009.2000-06-07.lokay\t0\t1\r\n",
      "0009.2001-02-09.kitchen\t0\t1\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t0\r\n",
      "0010.1999-12-14.farmer\t0\t1\r\n",
      "0010.1999-12-14.kaminski\t0\t1\r\n",
      "0010.2001-02-09.kitchen\t0\t1\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0010.2003-12-18.GP\t1\t1\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t1\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t0\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t1\r\n",
      "0012.1999-12-14.kaminski\t0\t1\r\n",
      "0012.2000-01-17.beck\t0\t1\r\n",
      "0012.2000-06-08.lokay\t0\t1\r\n",
      "0012.2001-02-09.kitchen\t0\t1\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t1\r\n",
      "0013.1999-12-14.kaminski\t0\t1\r\n",
      "0013.2001-04-03.williams\t0\t1\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t0\r\n",
      "0014.1999-12-14.kaminski\t0\t1\r\n",
      "0014.1999-12-15.farmer\t0\t1\r\n",
      "0014.2001-02-12.kitchen\t0\t1\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t1\r\n",
      "0015.1999-12-15.farmer\t0\t1\r\n",
      "0015.2000-06-09.lokay\t0\t1\r\n",
      "0015.2001-02-12.kitchen\t0\t1\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t1\r\n",
      "0016.2001-02-12.kitchen\t0\t1\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t0\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t1\r\n",
      "0017.2000-01-17.beck\t0\t1\r\n",
      "0017.2001-04-03.williams\t0\t1\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t1\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t0\r\n",
      "0018.2003-12-18.GP\t1\t0\r\n",
      "Accurate count: 36\r\n",
      "Inaccurate count: 64\r\n",
      "accurate_count type: <type 'int'>\r\n",
      "Multinomial Naive Bayes accuracy: 0.36\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes_3.sh 2 \"assistance valium enlargementWithATypo\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER: We see that our accuracy is very poor, 36%.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Provide a mapper/reducer pair that, when executed by pNaiveBayes.sh will classify the email messages by all words present.__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper_3.py\n",
    "#!/usr/bin/python\n",
    "## mapper.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: mapper code for HW1.5\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "\n",
    "## setting up logger\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'map_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "\n",
    "## collecting user input\n",
    "filename = sys.argv[1]\n",
    "# findwords = re.split(\" \",sys.argv[2].lower())\n",
    "findwords = re.split(\" \",sys.argv[2].lower())\n",
    "logging.debug(\"The words to be processed are: %s\" % ','.join(findwords))\n",
    "\n",
    "## setting up JSON that contains emmitted information\n",
    "emit_dict = {}\n",
    "emit_dict['SPAM_total_counter'] = 0\n",
    "emit_dict['HAM_total_counter'] = 0\n",
    "emit_dict['words'] = {}\n",
    "\n",
    "if findwords[0] != \"*\":\n",
    "    logging.debug(\"Illegal word argument for this exercise(%s)! Please exclusively define the vocabulary as '*'\" % findwords)\n",
    "    print \"Illegal word argument for this exercise(%s)! Please exclusively define the vocabulary as '*'\" % findwords\n",
    "    sys.exit()\n",
    "\n",
    "## Initializing counts for all the words in the vocabulary\n",
    "# for findword in findwords:\n",
    "#     emit_dict['words'][findword] = {\"SPAM_count\":0,\"HAM_count\":0}\n",
    "#     emit_dict['words'][findword] = {\"SPAM_count\":0,\"HAM_count\":0}\n",
    "\n",
    "count = 0\n",
    "total_count = 0\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## opening the chunk file for mapping\n",
    "with open (filename, \"r\") as myfile:\n",
    "    ## iterating line by line\n",
    "    for line in myfile:\n",
    "        HAM = True\n",
    "        line_components = line.split(\"\\t\")\n",
    "        output_variable = line_components[1]\n",
    "        line = ' '.join([str(x) for x in line_components[2:]])\n",
    "        logging.debug('Line: %s'% line)\n",
    "        words = line.split()\n",
    "        ## iterating through words in document\n",
    "        for word in words:\n",
    "            ## trimming words in document\n",
    "            logging.debug('Word: %s' % word)\n",
    "            word = word.rstrip(',')\n",
    "            word = word.rstrip(';')\n",
    "            word = word.rstrip(':')\n",
    "            word = word.rstrip('.')\n",
    "            word = word.rstrip('\"')\n",
    "            word = word.lstrip('\"')\n",
    "            if int(output_variable) == 0:\n",
    "                emit_dict['HAM_total_counter'] = emit_dict['HAM_total_counter'] + 1\n",
    "            else:\n",
    "                emit_dict['SPAM_total_counter'] = emit_dict['SPAM_total_counter'] + 1\n",
    "                HAM = False\n",
    "            if word not in emit_dict['words'].keys():\n",
    "                emit_dict['words'][word] = {\"SPAM_count\":0,\"HAM_count\":0}\n",
    "            if HAM:\n",
    "                emit_dict['words'][word]['HAM_count'] = emit_dict['words'][word]['HAM_count'] + 1\n",
    "            else:\n",
    "                emit_dict['words'][word]['SPAM_count'] = emit_dict['words'][word]['SPAM_count'] + 1\n",
    "    print json.dumps(emit_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper_3.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer_4.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer_4.py\n",
    "#!/usr/bin/python\n",
    "## reducer.py\n",
    "## Author: Carlos Eduardo Rodriguez Castillo\n",
    "## Description: reducer code for HW1.4\n",
    "import sys\n",
    "import re\n",
    "import json\n",
    "import logging\n",
    "import pprint\n",
    "from math import log\n",
    "\n",
    "## setting up logger\n",
    "################################################################################################\n",
    "## NOTE: make sure to set the logging directory path appropriately as the below one is custom!!!\n",
    "################################################################################################\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'reduce_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "## collecting mapper input\n",
    "logging.debug('Input file chunks: %s'% sys.argv)\n",
    "## taking the raw set of documents as an input\n",
    "raw_data = sys.argv[1]\n",
    "## taking the chunk files array as an input\n",
    "filenames = sys.argv[2:]\n",
    "\n",
    "## set up JSON that contains emmitted information\n",
    "reduced_dict = {}\n",
    "reduced_dict['SPAM_total_counter'] = 0\n",
    "reduced_dict['HAM_total_counter'] = 0\n",
    "reduced_dict['words'] = {}\n",
    "\n",
    "sum = 0\n",
    "accurate_count = 0\n",
    "inaccurate_count = 0\n",
    "total_documents = 0\n",
    "\n",
    "for filename in filenames:\n",
    "    logging.debug('processing input file: %s'% filename)\n",
    "    with open (filename, \"r\") as myfile:\n",
    "        data = json.load(myfile)\n",
    "        logging.debug('MAP_DICT -> SPAM_total_counter: %s ' % data['SPAM_total_counter'])\n",
    "        reduced_dict['SPAM_total_counter'] = reduced_dict['SPAM_total_counter'] + data['SPAM_total_counter']\n",
    "        logging.debug('REDUCED_DICT -> SPAM_total_counter: %s ' % reduced_dict['SPAM_total_counter'])\n",
    "        logging.debug('MAP_DICT -> HAM_total_counter: %s ' % data['HAM_total_counter'])\n",
    "        reduced_dict['HAM_total_counter'] = reduced_dict['HAM_total_counter'] + data['HAM_total_counter']\n",
    "        logging.debug('REDUCED_DICT -> HAM_total_counter: %s ' % reduced_dict['HAM_total_counter'])\n",
    "        for word in data['words']:\n",
    "            if word in reduced_dict['words'].keys():\n",
    "                logging.debug('%s -> HAM_counter: %s ' % (word,reduced_dict['words'][word]['HAM_count']))\n",
    "                \n",
    "                ## assigning the word counts\n",
    "                reduced_dict['words'][word]['HAM_count'] = reduced_dict['words'][word]['HAM_count'] + data['words'][word]['HAM_count']\n",
    "                reduced_dict['words'][word]['SPAM_count'] = reduced_dict['words'][word]['SPAM_count'] + data['words'][word]['SPAM_count']\n",
    "                \n",
    "                ## adding 1 to the counter of the words in each of the classes as a Laplace smoother\n",
    "                if reduced_dict['words'][word]['HAM_count'] == 0:\n",
    "                    reduced_dict['words'][word]['HAM_count'] = reduced_dict['words'][word]['HAM_count'] + data['words'][word]['HAM_count'] + 1\n",
    "                if reduced_dict['words'][word]['SPAM_count'] == 0:\n",
    "                    reduced_dict['words'][word]['SPAM_count'] = reduced_dict['words'][word]['SPAM_count'] + data['words'][word]['SPAM_count'] + 1\n",
    "            else:\n",
    "                reduced_dict['words'][word] = data['words'][word]\n",
    "                if reduced_dict['words'][word]['HAM_count'] == 0:\n",
    "                    reduced_dict['words'][word]['HAM_count'] = reduced_dict['words'][word]['HAM_count'] + data['words'][word]['HAM_count'] + 1\n",
    "                if reduced_dict['words'][word]['SPAM_count'] == 0:\n",
    "                    reduced_dict['words'][word]['SPAM_count'] = reduced_dict['words'][word]['SPAM_count'] + data['words'][word]['SPAM_count'] + 1\n",
    "                \n",
    "for word in reduced_dict['words'].keys():\n",
    "    \n",
    "    reduced_dict['words'][word]['SPAM_cond_prob'] = float(reduced_dict['words'][word]['SPAM_count']) / float(reduced_dict['SPAM_total_counter'])\n",
    "    reduced_dict['words'][word]['HAM_cond_prob'] = float(reduced_dict['words'][word]['HAM_count']) / float(reduced_dict['HAM_total_counter'])\n",
    "\n",
    "total_words = reduced_dict['SPAM_total_counter'] + reduced_dict['HAM_total_counter']\n",
    "reduced_dict['SPAM_prob_total'] = float(reduced_dict['SPAM_total_counter']) / float(total_words)\n",
    "reduced_dict['HAM_prob_total'] = float(reduced_dict['HAM_total_counter']) / float(total_words)\n",
    "\n",
    "HAM_prob_cond_doc = log(reduced_dict['HAM_prob_total'])\n",
    "SPAM_prob_cond_doc = log(reduced_dict['SPAM_prob_total'])\n",
    "\n",
    "## running classification now that model has been fitted\n",
    "count = 0\n",
    "with open (raw_data, \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        total_documents = total_documents + 1\n",
    "        HAM_prob_cond_doc = log(reduced_dict['HAM_prob_total'])\n",
    "        SPAM_prob_cond_doc = log(reduced_dict['SPAM_prob_total'])\n",
    "        count = count + 1\n",
    "        found_vocabulary = False\n",
    "        line_components = line.split(\"\\t\")\n",
    "        variable_id = line_components[0]\n",
    "        output_variable = int(line_components[1])\n",
    "        line = ' '.join([str(x) for x in line_components[2:]])\n",
    "        logging.debug('Line: %s'% line)\n",
    "#         print \"Processing line: %s\" % line\n",
    "        words = line.split()\n",
    "#         for findword in reduced_dict['words'].keys():\n",
    "        for word in words:\n",
    "            logging.debug('Word: %s' % word)\n",
    "            word = word.rstrip(',')\n",
    "            word = word.rstrip(';')\n",
    "            word = word.rstrip(':')\n",
    "            word = word.rstrip('.')\n",
    "            word = word.rstrip('\"')\n",
    "            word = word.lstrip('\"')\n",
    "            #Here we are checking that we exclusively take the log of non-zero numbers\n",
    "#             if word == findword:\n",
    "            if reduced_dict['words'][word]['HAM_cond_prob'] > 0:\n",
    "                HAM_prob_cond_doc = HAM_prob_cond_doc + log(reduced_dict['words'][word]['HAM_cond_prob'])\n",
    "            if reduced_dict['words'][word]['SPAM_cond_prob'] > 0:\n",
    "                SPAM_prob_cond_doc = SPAM_prob_cond_doc  + log(reduced_dict['words'][word]['SPAM_cond_prob'])\n",
    "            if word in reduced_dict['words'].keys():\n",
    "                found_vocabulary = True\n",
    "        if found_vocabulary:\n",
    "            if HAM_prob_cond_doc >= SPAM_prob_cond_doc:\n",
    "                predicted_outcome = 0\n",
    "            else:\n",
    "                predicted_outcome = 1      \n",
    "        if not found_vocabulary:\n",
    "#             print \"Pretty sure this can never happen!!!\"\n",
    "            if reduced_dict['HAM_prob_total'] >= reduced_dict['SPAM_prob_total']:\n",
    "                predicted_outcome = 0\n",
    "            else:\n",
    "                predicted_outcome = 1\n",
    "        if output_variable == predicted_outcome:\n",
    "            accurate_count = accurate_count + 1\n",
    "        else:\n",
    "            inaccurate_count = inaccurate_count + 1\n",
    "#         print \"True outcome: %.2f Prediction: %.2f\\nHAM_PROB: %.2f SPAM_PROB: %.2f\" % (output_variable, predicted_outcome, HAM_prob_cond_doc, SPAM_prob_cond_doc)\n",
    "        print \"%s\\t%d\\t%d\" % (variable_id, output_variable, predicted_outcome)\n",
    "print \"Accurate count: %s\" % accurate_count\n",
    "print \"Inaccurate count: %s\" % inaccurate_count\n",
    "print \"Accurate_count type: %s\" % type(accurate_count)\n",
    "print \"Multinomial Naive Bayes accuracy: %.2f\" % (float(int(accurate_count)) / float(total_documents))\n",
    "# print reduced_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer_4.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pNaiveBayes_4.sh\n"
     ]
    }
   ],
   "source": [
    "%%writefile pNaiveBayes_4.sh\n",
    "## pNaiveBayes.sh\n",
    "## Author: Carlos Eduardo Rodriguez Castillo (original author: Jake Ryland Williams)\n",
    "## Usage: pNaiveBayes.sh m wordlist\n",
    "## Input:\n",
    "##       m = number of processes (maps), e.g., 4\n",
    "##       wordlist = a space-separated list of words in quotes, e.g., \"the and of\"\n",
    "##\n",
    "## Instructions: Read this script and its comments closely.\n",
    "##               Do your best to understand the purpose of each command,\n",
    "##               and focus on how arguments are supplied to mapper.py/reducer.py,\n",
    "##               as this will determine how the python scripts take input.\n",
    "##               When you are comfortable with the unix code below,\n",
    "##               answer the questions on the LMS for HW1 about the starter code.\n",
    " \n",
    "## collect user input\n",
    "m=$1 ## the number of parallel processes (maps) to run\n",
    "wordlist=$2 ## if set to \"*\", then all words are used\n",
    " \n",
    "## a test set data of 100 messages\n",
    "#data=`cat enronemail_1h.txt | cut -d$'\\t' -f 3,4`\n",
    "data=\"enronemail_1h.txt\"\n",
    "    \n",
    "## the full set of data (33746 messages)\n",
    "# data=\"enronemail.txt\" \n",
    " \n",
    "## 'wc' determines the number of lines in the data\n",
    "## 'perl -pe' regex strips the piped wc output to a number\n",
    "linesindata=`wc -l $data | perl -pe 's/^.*?(\\d+).*?$/$1/'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesindata\"\n",
    "\n",
    "## determine the lines per chunk for the desired number of processes\n",
    "linesinchunk=`echo \"$linesindata/$m+1\" | bc`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$linesinchunk\"\n",
    " \n",
    "## split the original file into chunks by line\n",
    "split -l $linesinchunk $data $data.chunk.\n",
    " \n",
    "## assign python mappers (mapper.py) to the chunks of data\n",
    "## and emit their output to temporary files\n",
    "for datachunk in $data.chunk.*; do\n",
    "    ## feed word list to the python mapper here and redirect STDOUT to a temporary file on disk\n",
    "    ####\n",
    "    ####\n",
    "    ##### debugging line\n",
    "#     echo \"MAPPING CHUNK: $datachunk\"\n",
    "    \n",
    "    ./mapper_3.py $datachunk \"$wordlist\" > $datachunk.counts &\n",
    "    \n",
    "    ##### debugging line\n",
    "#     echo \"FINISHED MAPPING CHUNK: $datachunk\"\n",
    "    #echo \"THE WORDLIST IS $wordlist\"\n",
    "    ####\n",
    "    ####\n",
    "done\n",
    "## wait for the mappers to finish their work\n",
    "wait\n",
    "\n",
    "#echo \"Content of transmitted data (raw):\"\n",
    "\n",
    "#for file in $data.chunk.*.counts; do\n",
    "#   cat $file\n",
    "# done\n",
    " \n",
    "## 'ls' makes a list of the temporary count files\n",
    "## 'perl -pe' regex replaces line breaks with spaces\n",
    "countfiles=`\\ls $data.chunk.*.counts | perl -pe 's/\\n/ /'`\n",
    "\n",
    "## debugging line\n",
    "# echo \"$countfiles\"\n",
    "# echo \"Content of transmitted data:\"\n",
    "\n",
    "# for file in $countfiles; do\n",
    "#   cat $file\n",
    "# done\n",
    " \n",
    "## feed the list of countfiles to the python reducer and redirect STDOUT to disk\n",
    "####\n",
    "####\n",
    "##### debugging line\n",
    "# echo \"REDUCING CHUNKS\"\n",
    "./reducer_4.py $data $countfiles > $data.output\n",
    "####\n",
    "####\n",
    "\n",
    "## print results\n",
    "\n",
    "# Commenting out for now\n",
    "cat $data.output\n",
    "\n",
    "## clean up the data chunks and temporary count files\n",
    "\\rm $data.chunk.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pNaiveBayes_4.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0001.1999-12-10.farmer\t0\t1\r\n",
      "0001.1999-12-10.kaminski\t0\t0\r\n",
      "0001.2000-01-17.beck\t0\t0\r\n",
      "0001.2000-06-06.lokay\t0\t0\r\n",
      "0001.2001-02-07.kitchen\t0\t0\r\n",
      "0001.2001-04-02.williams\t0\t0\r\n",
      "0002.1999-12-13.farmer\t0\t0\r\n",
      "0002.2001-02-07.kitchen\t0\t0\r\n",
      "0002.2001-05-25.SA_and_HP\t1\t1\r\n",
      "0002.2003-12-18.GP\t1\t1\r\n",
      "0002.2004-08-01.BG\t1\t1\r\n",
      "0003.1999-12-10.kaminski\t0\t0\r\n",
      "0003.1999-12-14.farmer\t0\t0\r\n",
      "0003.2000-01-17.beck\t0\t0\r\n",
      "0003.2001-02-08.kitchen\t0\t0\r\n",
      "0003.2003-12-18.GP\t1\t1\r\n",
      "0003.2004-08-01.BG\t1\t1\r\n",
      "0004.1999-12-10.kaminski\t0\t0\r\n",
      "0004.1999-12-14.farmer\t0\t0\r\n",
      "0004.2001-04-02.williams\t0\t0\r\n",
      "0004.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0004.2004-08-01.BG\t1\t1\r\n",
      "0005.1999-12-12.kaminski\t0\t0\r\n",
      "0005.1999-12-14.farmer\t0\t0\r\n",
      "0005.2000-06-06.lokay\t0\t0\r\n",
      "0005.2001-02-08.kitchen\t0\t0\r\n",
      "0005.2001-06-23.SA_and_HP\t1\t1\r\n",
      "0005.2003-12-18.GP\t1\t1\r\n",
      "0006.1999-12-13.kaminski\t0\t0\r\n",
      "0006.2001-02-08.kitchen\t0\t0\r\n",
      "0006.2001-04-03.williams\t0\t0\r\n",
      "0006.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0006.2003-12-18.GP\t1\t1\r\n",
      "0006.2004-08-01.BG\t1\t1\r\n",
      "0007.1999-12-13.kaminski\t0\t0\r\n",
      "0007.1999-12-14.farmer\t0\t0\r\n",
      "0007.2000-01-17.beck\t0\t0\r\n",
      "0007.2001-02-09.kitchen\t0\t0\r\n",
      "0007.2003-12-18.GP\t1\t1\r\n",
      "0007.2004-08-01.BG\t1\t1\r\n",
      "0008.2001-02-09.kitchen\t0\t0\r\n",
      "0008.2001-06-12.SA_and_HP\t1\t1\r\n",
      "0008.2001-06-25.SA_and_HP\t1\t1\r\n",
      "0008.2003-12-18.GP\t1\t1\r\n",
      "0008.2004-08-01.BG\t1\t1\r\n",
      "0009.1999-12-13.kaminski\t0\t0\r\n",
      "0009.1999-12-14.farmer\t0\t0\r\n",
      "0009.2000-06-07.lokay\t0\t0\r\n",
      "0009.2001-02-09.kitchen\t0\t0\r\n",
      "0009.2001-06-26.SA_and_HP\t1\t1\r\n",
      "0009.2003-12-18.GP\t1\t1\r\n",
      "0010.1999-12-14.farmer\t0\t0\r\n",
      "0010.1999-12-14.kaminski\t0\t0\r\n",
      "0010.2001-02-09.kitchen\t0\t0\r\n",
      "0010.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0010.2003-12-18.GP\t1\t0\r\n",
      "0010.2004-08-01.BG\t1\t1\r\n",
      "0011.1999-12-14.farmer\t0\t0\r\n",
      "0011.2001-06-28.SA_and_HP\t1\t1\r\n",
      "0011.2001-06-29.SA_and_HP\t1\t1\r\n",
      "0011.2003-12-18.GP\t1\t1\r\n",
      "0011.2004-08-01.BG\t1\t1\r\n",
      "0012.1999-12-14.farmer\t0\t0\r\n",
      "0012.1999-12-14.kaminski\t0\t0\r\n",
      "0012.2000-01-17.beck\t0\t0\r\n",
      "0012.2000-06-08.lokay\t0\t0\r\n",
      "0012.2001-02-09.kitchen\t0\t0\r\n",
      "0012.2003-12-19.GP\t1\t1\r\n",
      "0013.1999-12-14.farmer\t0\t0\r\n",
      "0013.1999-12-14.kaminski\t0\t0\r\n",
      "0013.2001-04-03.williams\t0\t0\r\n",
      "0013.2001-06-30.SA_and_HP\t1\t1\r\n",
      "0013.2004-08-01.BG\t1\t1\r\n",
      "0014.1999-12-14.kaminski\t0\t0\r\n",
      "0014.1999-12-15.farmer\t0\t0\r\n",
      "0014.2001-02-12.kitchen\t0\t0\r\n",
      "0014.2001-07-04.SA_and_HP\t1\t1\r\n",
      "0014.2003-12-19.GP\t1\t1\r\n",
      "0014.2004-08-01.BG\t1\t1\r\n",
      "0015.1999-12-14.kaminski\t0\t0\r\n",
      "0015.1999-12-15.farmer\t0\t0\r\n",
      "0015.2000-06-09.lokay\t0\t0\r\n",
      "0015.2001-02-12.kitchen\t0\t0\r\n",
      "0015.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0015.2003-12-19.GP\t1\t1\r\n",
      "0016.1999-12-15.farmer\t0\t0\r\n",
      "0016.2001-02-12.kitchen\t0\t0\r\n",
      "0016.2001-07-05.SA_and_HP\t1\t1\r\n",
      "0016.2001-07-06.SA_and_HP\t1\t1\r\n",
      "0016.2003-12-19.GP\t1\t1\r\n",
      "0016.2004-08-01.BG\t1\t1\r\n",
      "0017.1999-12-14.kaminski\t0\t0\r\n",
      "0017.2000-01-17.beck\t0\t0\r\n",
      "0017.2001-04-03.williams\t0\t0\r\n",
      "0017.2003-12-18.GP\t1\t1\r\n",
      "0017.2004-08-01.BG\t1\t0\r\n",
      "0017.2004-08-02.BG\t1\t1\r\n",
      "0018.1999-12-14.kaminski\t0\t0\r\n",
      "0018.2001-07-13.SA_and_HP\t1\t1\r\n",
      "0018.2003-12-18.GP\t1\t1\r\n",
      "Accurate count: 97\r\n",
      "Inaccurate count: 3\r\n",
      "Accurate_count type: <type 'int'>\r\n",
      "Multinomial Naive Bayes accuracy: 0.97\r\n"
     ]
    }
   ],
   "source": [
    "!./pNaiveBayes_4.sh 5 \"*\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER: We saw a fairly high accuracy of 97%.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW1.6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Benchmark your code with the Python SciKit-Learn implementation of multinomial Naive Bayes.__\n",
    "\n",
    "__A. Run the Multinomial Naive Bayes algorithm (using default settings) from SciKit-Learn over the same training data used in HW1.5 and report the Training error (please note some data preparation might be needed to get the Multinomial Naive Bayes algorithm from SkiKit-Learn to run over this dataset)__\n",
    "\n",
    "__B. Run the Bernoulli Naive Bayes algorithm from SciKit-Learn (using default settings) over the same training data used in HW1.5 and report the Training error__\n",
    "\n",
    "__C. Run the Multinomial Naive Bayes algorithm you developed for HW1.5 over the same data used HW1.5 and report the Training error__\n",
    "\n",
    "__D. Please prepare a table to present your results__\n",
    "\n",
    "__E. Explain/justify any differences in terms of training error rates over the dataset in HW1.5 between your Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn (Hint: smoothing, which we will discuss in next lecture)__\n",
    "\n",
    "__F. Discuss the performance differences in terms of training error rates over the dataset in HW1.5 between the Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HW1.6 A. Multinomial Naive Bayes algorithm (using default settings)\n",
      "The training error is 0.00\n",
      "HW1.6 B. Bernoulli Naive Bayes algorithm (using default settings)\n",
      "The training error is 0.00\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB, BernoulliNB\n",
    "\n",
    "# SK-learn libraries for evaluation.\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import classification_report\n",
    "\n",
    "# SK-learn library for importing the newsgroup data.\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "\n",
    "# SK-learn libraries for feature extraction from text.\n",
    "from sklearn.feature_extraction.text import *\n",
    "\n",
    "import sys\n",
    "import re\n",
    "import logging\n",
    "import pprint\n",
    "from math import log\n",
    "\n",
    "## setting up logger\n",
    "logging_directory_path = '/home/crodriguez1/W261/HW1/log'\n",
    "logging_file = logging_directory_path + \"/\" + 'sklearn_log.log'\n",
    "log_format = '%(levelname)s\\n%(asctime)s.%(msecs)-3d filename:%(filename)-20s line:%(lineno)-5d \\n%(message)s\\n\\n'\n",
    "log_date_format = '%H:%M:%S'\n",
    "logging.basicConfig(filename = logging_file,\n",
    "    stream = sys.stderr,\n",
    "    level = logging.DEBUG,\n",
    "    format = log_format,\n",
    "    datefmt = log_date_format)\n",
    "suds_logger = logging.getLogger(\"suds\")\n",
    "suds_logger.propagate = False\n",
    "\n",
    "WORD_RE = re.compile(r\"[\\w']+\")\n",
    "\n",
    "train_data = []\n",
    "train_labels = []\n",
    "\n",
    "## Training error calculating function\n",
    "def training_error(labels, prediction):\n",
    "    errors = 0\n",
    "    num_observations = len(labels)\n",
    "    for truth, prediction in zip(train_labels ,MultiNB_predicted):\n",
    "        logging.debug(\"Truth: %s | Prediction: %s\"% (truth, prediction))\n",
    "        if truth != prediction:\n",
    "            errors = errors + 1\n",
    "    print \"The training error is %.2f\" % (float(errors)/float(num_observations))\n",
    "\n",
    "with open (\"enronemail_1h.txt\", \"r\") as myfile:\n",
    "    for line in myfile:\n",
    "        line_components = line.split(\"\\t\")\n",
    "        output_variable = int(line_components[1])\n",
    "        line = ' '.join([str(x) for x in line_components[2:]])\n",
    "        train_data.extend([line])\n",
    "        train_labels.extend([output_variable])\n",
    "# Initialize CountVectorizer and fit and transform the data\n",
    "cv = CountVectorizer()\n",
    "cv_train = cv.fit_transform(train_data)\n",
    "\n",
    "## Create and fit MultinomialNB classifier (default settings)\n",
    "MultiNB = MultinomialNB()\n",
    "MultiNB.fit(cv_train,train_labels)\n",
    "MultiNB_predicted = MultiNB.predict(cv_train)\n",
    "\n",
    "## Create and fit MultinomialNB classifier (default settings)\n",
    "BernoulliNB = BernoulliNB()\n",
    "BernoulliNB.fit(cv_train,train_labels)\n",
    "BernoulliNB_predicted = BernoulliNB.predict(cv_train)\n",
    "\n",
    "print \"ANSWER:\"\n",
    "print \"HW1.6 A. Multinomial Naive Bayes algorithm (using default settings)\"\n",
    "training_error(train_labels, MultiNB_predicted)\n",
    "print \"HW1.6 B. Bernoulli Naive Bayes algorithm (using default settings)\"\n",
    "training_error(train_labels, BernoulliNB_predicted)\n",
    "print \"HW1.6 C. From the results in HW1.5 we see that the training error for the Multinomial NB is 3%\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "__HW1.6 E. I believe that the difference in the training error rates between my Multinomial Naive Bayes implementation (in Map Reduce) versus the Multinomial Naive Bayes implementation in SciKit-Learn is that I believe that my implementation has slightly less bias than that which was implemented by SciKit-Learn given that my implementation used a smoothing factor and the SciKit-Learn alternative did not. As such my results brought the probability for the classes closer together, which opens up the posibility for lower accuracy (but better generalizability).__\n",
    "\n",
    "__HW1.6 F. I believe that the (lack) of performance difference in terms of training error rates over the dataset in HW1.5 between the  Multinomial Naive Bayes implementation in SciKit-Learn with the  Bernoulli Naive Bayes implementation in SciKit-Learn is due to the fact that both models highly overfitted to the training data (using all the words in the corpus as a vocabulary AND predicting on the training data).__"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
