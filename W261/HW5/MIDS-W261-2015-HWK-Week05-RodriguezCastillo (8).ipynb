{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale \n",
    "\n",
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 5**\n",
    "\n",
    "**Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### ANSWER:\n",
    "\n",
    "__A data warehouse is a large storage facility for data that can come from a wide variety of sources and that can be in a wide variety of formats.__\n",
    "\n",
    "__A Star schema is a commonly used and simple schema used to define how data is stored in a data storage facility (be it a database or a data warehouse for example).__ The Star schema organizes data records into fact tables that may reference one to many other dimension tables where fact tables tend to store records that represent events whereas dimension tables tend to store records that represent characteristics of objects that are involved in events recorded in fact tables.\n",
    "\n",
    "The Star schema is widely used to represent data in a myriad of business applications. __Specifically, it is a very effective schema for the purposes of giving structure to data used by an OLTP (online transaction processing) system.__ This is in contrast to data used by OLAP (online analytical processing) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "- In the database world what is 3NF? Does machine learning use data in 3NF? If so why?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "In the database world, __3NF is the third of a standard set of normalization steps that are applied to databases in order to (1) reduce data duplication in the database and (2) ensure that all records__ (and as such any and all attributes for each record) __in the database can be unambiguously identified through a key__ (where the key is either a single attribute or a set of attributes).\n",
    "\n",
    "__For the most part, machine learning does not use data in 3NF__; the reason being that the reduction in data duplication offered by 3NF actually stops the underlying distributed and sequential machine learning framework from properly identifying and subsequewntly processing records.\n",
    "\n",
    "- In what form does ML consume data?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "Taking ML to mean machine learning, ML consumes data that is in a raw, denormalized format. Punctually, this would be logs produced by applications, users, or a combination of both.\n",
    "\n",
    "- Why would one use log files that are denormalized?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "One would use log files that are denormalized for several of reasons. On one hand, denormalized logs provide the machine learning framework the most amount of context to operate on. That is, the more data is provided in each log or record, the more options the data scientist has to produce features for the ML framework to learn on.\n",
    "\n",
    "On another hand, given the distributed and sequential nature of the MapReduce framework that supports most of modern ML at scale, denormalized log data has the representational redundancy that would permit a framework of this nature to identify, differentiate between and process records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "I choose the table with the URLs (URL ID and URL name) as the Left table in the hashside join because it is considerably smaller than the URL ID and USER ID visits; by definition we wish to store the smallest of the tables being merged as the in-memory stored table. See below for the number of rows resulting from each of the joins:\n",
    "\n",
    "| |Inner Join|Left Outer Join|Right Outer Join|\n",
    "|---|---|---|---|\n",
    "|Number of rows|98654|98663|98654| |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Preprocessing/housekeeping\n",
    "#####################################\n",
    "\n",
    "## Create local directories for data\n",
    "\n",
    "#!mkdir data\n",
    "#!mkdir src\n",
    "\n",
    "## Download data for 5.2\n",
    "\n",
    "#!wget \"https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AADCHtrJ4CBCDO1po_OAWg0ia/anonymous-msweb.data?dl=0#\"\n",
    "#!mv \"anonymous-msweb.data?dl=0\" data/anonymous-msweb.data\n",
    "#!egrep \"^A,\" data/anonymous-msweb.data > data/URL_table.txt\n",
    "#!head data/URL_table.txt\n",
    "#!wc -l data/URL_table.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/preprocess.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Single node data preprocessing for HW4.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "url_dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    elements = line.split(\",\")\n",
    "    ## When a line starts with C we start \n",
    "    ## recording data for the customer\n",
    "    if elements[0] == 'C':\n",
    "        visitor_data = elements[0] + ',' + elements[2]\n",
    "        continue\n",
    "    ## When a line starts with V we start \n",
    "    ## recording data for the visit to a URL\n",
    "    elif elements[0] == 'V':\n",
    "        ## this is formatted as 'V,[URL_ID]\n",
    "        visit_data = elements[0] + ',' + elements[1]\n",
    "        ## this is formatted as 'V,[URL_ID],C,[USER_ID]\n",
    "        processed_line = visit_data + ',' + visitor_data\n",
    "        print processed_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/preprocess.py\n",
    "!./src/preprocess.py < data/anonymous-msweb.data \\\n",
    "> data/preprocessed_anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/InnerJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/InnerJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class InnerJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "            )\n",
    "        ]\n",
    "    in_memory_hash = {}\n",
    "    def mapper_init(self):\n",
    "        ## Upon initializing a mapper\n",
    "        ## we load into memory a dictionary\n",
    "        ## that holds the URL table\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = 'http://www.microsoft.com' + row[4].strip('\"')\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        ## for each record being processed by the mapper\n",
    "        ## we retrieve the URL in the in-memory hash\n",
    "        ## based on the URL ID of the record\n",
    "        url = self.in_memory_hash[url_id]\n",
    "        yield url_id, (url, user_id)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    InnerJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/InnerJoinMRjob.py\n",
    "!./src/InnerJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_InnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/RightOuterJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/RightOuterJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class RightOuterJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    def mapper_init(self):\n",
    "        ## Upon initializing a mapper\n",
    "        ## we load into memory a dictionary\n",
    "        ## that holds the URL table\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = 'http://www.microsoft.com' + row[4].strip('\"')\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        ## Since this is a Right Outer Join\n",
    "        ## we only emmit a result if there is a record\n",
    "        ## in our URL hash for the record being processed\n",
    "        ## in this case, the result is identical to the Inner Join\n",
    "        try:\n",
    "            url = self.in_memory_hash[url_id]\n",
    "        except KeyError:\n",
    "            sys.stderr.write(\"Did not find a URL for URL ID: %s\"%url_id)\n",
    "            url = \"NULL\"\n",
    "        yield url_id, (url, user_id)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    RightOuterJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808/output...\n",
      "Removing temp directory /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/RightOuterJoinMRjob.py\n",
    "!./src/RightOuterJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_RightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/LeftOuterJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/LeftOuterJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class LeftOuterJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k1,1 -k2,2r\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 1,\n",
    "            'mapreduce.job.maps': 1\n",
    "            \n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    visited_hash = {}\n",
    "    def mapper_init(self):\n",
    "        ## Upon initializing a mapper\n",
    "        ## we load into memory a dictionary\n",
    "        ## that holds the URL table\n",
    "        ## We also keep track of whether a URL\n",
    "        ## has been visited\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = {\"url\":'http://www.microsoft.com' + row[4].strip('\"'),\"visited\":0}\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        ## for each record processed\n",
    "        ## we mark the associated URL as \n",
    "        ## being visited\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        self.in_memory_hash[url_id][\"visited\"] = 1\n",
    "        try:\n",
    "            url = self.in_memory_hash[url_id][\"url\"]\n",
    "        except KeyError:\n",
    "            sys.stderr.write(\"Did not find a URL for URL ID: %s\\n\"%url_id)\n",
    "            url = \"NULL\"\n",
    "        yield (1, (url_id, url, user_id))\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        ## If there are URLs that have not been visited\n",
    "        ## we keep track to publish in the output table\n",
    "        ## for the Left Outer Join\n",
    "        for key in self.in_memory_hash.keys():\n",
    "            if self.in_memory_hash[key][\"visited\"] == 0:\n",
    "                yield (99, (key, self.in_memory_hash[key][\"url\"], \"NULL\"))\n",
    "        \n",
    "    def reducer(self, visited, visit_tuples):\n",
    "        ## In the reducer I publish all the matches but\n",
    "        ## also all the records in the URL table \n",
    "        ## that did not have a match\n",
    "        prev_url_id = \"\"\n",
    "        if int(visited) == 99:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                if prev_url_id != url_id:\n",
    "                    prev_url_id = url_id\n",
    "                    yield url_id, (url,\"NULL\")\n",
    "        else:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                uid = vt[2]\n",
    "                yield url_id, (url, uid)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    LeftOuterJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194/output...\n",
      "Removing temp directory /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/LeftOuterJoinMRjob.py\n",
    "!./src/LeftOuterJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_LeftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################\n",
      "##     INNER JOIN ROWS    ##\n",
      "############################\n",
      "98654 output_InnerJoin\n",
      "############################\n",
      "##  LEFT OUTER JOIN ROWS  ##\n",
      "############################\n",
      "98663 output_LeftOuterJoin\n",
      "############################\n",
      "## RIGHT OUTER JOIN ROWS  ##\n",
      "############################\n",
      "98654 output_RightOuterJoin\n"
     ]
    }
   ],
   "source": [
    "## Comparing results below\n",
    "\n",
    "!echo \"############################\"\n",
    "!echo \"##     INNER JOIN ROWS    ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_InnerJoin\n",
    "!echo \"############################\"\n",
    "!echo \"##  LEFT OUTER JOIN ROWS  ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_LeftOuterJoin\n",
    "!echo \"############################\"\n",
    "!echo \"## RIGHT OUTER JOIN ROWS  ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_RightOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AMSWER:\n",
    "- The longest 5-gram by number of characters is **AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR** with **159 characters**.\n",
    "- The top 10 most frequent words (unigrams) in decreasing order of frequency are:\n",
    "\n",
    "|Word|Frequency|\n",
    "|---|---|\n",
    "|the|5490815394|\n",
    "|of|3698583299|\n",
    "|to|2227866570|\n",
    "|in|1421312776|\n",
    "|a|1361123022|\n",
    "|and|1149577477|\n",
    "|that|802921147|\n",
    "|is|758328796|\n",
    "|be|688707130|\n",
    "|as|492170314|\n",
    "\n",
    "- The 20 most densely appearing words in decreasing order of relative frequency are:\n",
    "\n",
    "|Word|Relative Frequency|\n",
    "|---|---|\n",
    "|xxxx|11.5572916667|\n",
    "|blah|8.0741599073|\n",
    "|nnn|7.53333333333|\n",
    "|na|6.20174913142|\n",
    "|oooooooooooooooo|4.921875|\n",
    "|nd|4.85430572724|\n",
    "|llll|4.51162790698|\n",
    "|oooooo|4.16965001336|\n",
    "|ooooo|3.85863719347|\n",
    "|lillelu|3.76245210728|\n",
    "|madarassy|3.57692307692|\n",
    "|pfeffermann|3.57692307692|\n",
    "|meteoritical|3.56|\n",
    "|xxxxxxxx|3.5|\n",
    "|beep|3.22903885481|\n",
    "|latha|3.18867924528|\n",
    "|iyengar|2.91911764706|\n",
    "|counterfeiteth|2.825|\n",
    "|nonmorular|2.81981981982|\n",
    "|nonsquamous|2.81981981982|\n",
    "\n",
    "- The 20 least densely appearing words in decreasing order of relative frequency are:\n",
    "\n",
    "|Word|Relative Frequency|\n",
    "|---|---|\n",
    "|zwingst|1.0|\n",
    "|zwirnen|1.0|\n",
    "|zwischenstaatlicher|1.0|\n",
    "|zwitterionic|1.0|\n",
    "|zwt|1.0|\n",
    "|zwyn|1.0|\n",
    "|zx|1.0|\n",
    "|zxcvframeqasfuc|1.0|\n",
    "|zydeco|1.0|\n",
    "|zydom|1.0|\n",
    "|zygmunt|1.0|\n",
    "|zygomaticofacial|1.0|\n",
    "|zygomaticotemporal|1.0|\n",
    "|zygosity|1.0|\n",
    "|zylindrischen|1.0|\n",
    "|zymelman|1.0|\n",
    "|zymogens|1.0|\n",
    "|zymophore|1.0|\n",
    "|zymosan|1.0|\n",
    "|zymosis|1.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Preprocessing/housekeeping\n",
    "#####################################\n",
    "\n",
    "# !wget \"https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0#\"\n",
    "# !mv \"googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0\" data/googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "# !head -n 10 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt > data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "# !cat data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class Longest5ngramJob(MRJob):\n",
    "    \n",
    "    # Settings for sorting\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        ## Note that we are explicitly setting \n",
    "        ## multiple reducers, we are considering the \n",
    "        ## full output of the mappers as a key for the\n",
    "        ## shuffle phase, setting partitioners based on \n",
    "        ## artificial partition keys and we are sorting\n",
    "        ## based on the length of the 5-grams\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\",\n",
    "             'mapreduce.job.reduces': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    ## Notice that we are emmitting artificial \n",
    "    ## keys to ensure that the partitioners can \n",
    "    ## key off of them and guarantee sorted output\n",
    "    def mapper(self, _, line):\n",
    "        ngram = line.split(\"\\t\")[0]\n",
    "        key = ngram\n",
    "        value = len(ngram)\n",
    "        if int(value) >= 6000:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 3000:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 1500:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 750:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 375:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 186:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 93:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 45:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 25:\n",
    "            yield 'i', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'j', key+\"\\t\"+str(value)\n",
    "\n",
    "    ## I ignore the key as it was only used\n",
    "    ## for sorting\n",
    "    def reducer(self,key,value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Longest5ngramJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_1/ --recursive\n",
    "!python ~/w261/HW5/src/eda_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_1 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output\n",
    "!mkdir ~/w261/HW5/output/EDA_1\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_1/ output/EDA_1 --recursive\n",
    "!head -n 1  ~/w261/HW5/output/EDA_1/pa* | sort -k2,2nr | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_2.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class UnigramCounterJob(MRJob):\n",
    "    \n",
    "    # Settings for sorting\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        ## Note that we are explicitly setting \n",
    "        ## multiple reducers, we are considering the \n",
    "        ## full output of the mappers as a key for the\n",
    "        ## shuffle phase, setting partitioners based on \n",
    "        ## artificial partition keys and we are sorting\n",
    "        ## based on the word frequencies\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\",\n",
    "             'mapreduce.job.reduces': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        ngram_elements = line.split(\"\\t\")\n",
    "        ngram = ngram_elements[0]\n",
    "        count = int(ngram_elements[1])\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram.lower(), str(count)\n",
    "\n",
    "    def combiner(self, key, value):\n",
    "        temp_sum = 0\n",
    "        for v in value:\n",
    "            temp_sum += int(v)\n",
    "        yield key, str(temp_sum)\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        temp_sum = 0\n",
    "        for v in value:\n",
    "            temp_sum += int(v)\n",
    "        yield key, str(temp_sum)\n",
    "    \n",
    "    def sortMapper(self, key, value):\n",
    "        if int(value) >= 6000:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 3000:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 1500:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 750:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 375:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 186:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 93:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 45:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 25:\n",
    "            yield 'i', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'j', key+\"\\t\"+str(value)\n",
    "\n",
    "    ## I ignore the key as it was only used\n",
    "    ## for sorting    \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    UnigramCounterJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_2.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_2/ --recursive\n",
    "!python ~/w261/HW5/src/eda_2.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_2 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output/EDA_2\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_2/ output/EDA_2 --recursive\n",
    "!head -n 10 ~/w261/HW5/output/EDA_2/pa* | sort -k2,2nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class WordFrequencyJob(MRJob):\n",
    "    \n",
    "    # Settings for sorting\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        ## Note that we are explicitly setting \n",
    "        ## multiple reducers, we are considering the \n",
    "        ## full output of the mappers as a key for the\n",
    "        ## shuffle phase, setting partitioners based on \n",
    "        ## artificial partition keys and we are sorting\n",
    "        ## based on the word relative frequencies\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\",\n",
    "             'mapreduce.job.reduces': 7,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        ngram_elements = line.split(\"\\t\")\n",
    "        ngram = ngram_elements[0]\n",
    "        count = int(ngram_elements[1])\n",
    "        page_count = int(ngram_elements[2])\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram.lower(), str(count)+\"\\t\"+str(page_count)\n",
    "    \n",
    "    def reducer (self, key, value):\n",
    "        total_count, total_page_count = 0, 0\n",
    "        for val in value:\n",
    "            val = val.split(\"\\t\")\n",
    "            total_count += int(val[0])\n",
    "            total_page_count += int(val[1])\n",
    "        yield key, str(float(total_count)/float(total_page_count))\n",
    "    \n",
    "    def sortMapper(self, key, value):\n",
    "        if float(value) >= 2:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 1:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.5:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.25:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.125:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.0625:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "            \n",
    "    ## I ignore the key as it was only used\n",
    "    ## for sorting   \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WordFrequencyJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_3.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_3/ --recursive\n",
    "!python ~/w261/HW5/src/eda_3.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_3 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output/EDA_3\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_3/ output/EDA_3 --recursive\n",
    "!head -n 20 ~/w261/HW5/output/EDA_3/pa* | sort -k2,2nr | head -n 20\n",
    "!tail -n 20 ~/w261/HW5/output/EDA_3/pa* | sort -k2,2nr | tail -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/eda_3.py\n",
    "# !./src/eda_3.py -r local \\\n",
    "#  data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "#     > output_eda3_1\n",
    "# !head -n 20 output_eda3_1\n",
    "# !tail -n 20 output_eda3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33\n",
      "1    33\n",
      "2    29\n",
      "3    28\n",
      "4    27\n",
      "5    26\n",
      "6    24\n",
      "7    23\n",
      "8    22\n",
      "9    17\n",
      "Name: 1, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd37a871850>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd37ae9e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"output_eda1\",\n",
    "                 sep='\\t',\n",
    "                header=None)\n",
    "\n",
    "counts = pd.Series(df[1])\n",
    "print counts\n",
    "counts.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3.1 OPTIONAL Question:\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5, Y:1}\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using co-occurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "For the purposes of completing this exercise I used an __AWS EMR cluster with 15 m3.xlarge nodes__.\n",
    "\n",
    "I opted for computing Cosine similarity and Jaccard similarity as defined in table one, page five of [this paper](http://stanford.edu/~rezab/papers/disco.pdf).\n",
    "\n",
    "As far as runtime for the different jobs, please see below:\n",
    "\n",
    "|MR Job|CPU time spent (ms)|GC time elapsed (ms)|\n",
    "|---|---|---|\n",
    "|Stripe Construction|1,235,070|76,579|\n",
    "|Inverted Index Construction|307,260|51,058|\n",
    "|Cosine Similarity Synonyms Construction|18,890,970|90,239|\n",
    "|Jaccard Similarity Synonyms Construction|18,567,340|88,243|\n",
    "|Cosine Similarity Synonyms Sort|1,703,220|58,250|\n",
    "|Jaccard Similarity Synonyms Sort|1,706,380|58,293|\n",
    "\n",
    "\n",
    "As far as systems tests are concerned, __It is important to note that, while the systems data set was useful to test base functionality, there were critical issues and cases that remained uncovered by the systems test data set; the main of which were memory errors encountered when processing the production data set.__\n",
    "\n",
    "The results obtained for the systems test data set were as follows:\n",
    "\n",
    "__Inverted Index Construction__\n",
    "\n",
    "    \"M\"\t{\"C\": {\"length\": 4, \"sim\": 0.5}}\n",
    "    \"N\"\t{\"C\": {\"length\": 4, \"sim\": 0.5}}\n",
    "    \"X\"\t{\"A\": {\"length\": 3, \"sim\": 0.57735026918962584}, \"B\": {\"length\": 2, \"sim\": 0.70710678118654746}}\n",
    "    \"Y\"\t{\"A\": {\"length\": 3, \"sim\": 0.57735026918962584}, \"C\": {\"length\": 4, \"sim\": 0.5}, \"B\": {\"length\": 2, \"sim\": 0.70710678118654746}}\n",
    "    \"Z\"\t{\"A\": {\"length\": 3, \"sim\": 0.57735026918962584}, \"C\": {\"length\": 4, \"sim\": 0.5}}\n",
    "    \n",
    "__Cosine Similarity Construction__\n",
    "\n",
    "    \"A.B\"\t0.81649658092772592\n",
    "    \"A.C\"\t0.57735026918962584\n",
    "    \"B.C\"\t0.35355339059327373\n",
    "\n",
    "__Jaccard Similarity Construction__\n",
    "\n",
    "    \"B.C\"\t0.2\n",
    "    \"A.B\"\t0.6666666666666666\n",
    "    \"A.C\"\t0.4\n",
    "\n",
    "The results for the production data set jobs can be found in s3:\n",
    "\n",
    "|MR Job|Results S3 Location|\n",
    "|---|---|\n",
    "|Stripe Construction|s3://cerc-w261/HW5/5-2/output/Stripes/|\n",
    "|Inverted Index Construction|s3://cerc-w261/HW5/5-2/output/InvertedIndices/|\n",
    "|Cosine Similarity Synonyms Construction|s3://cerc-w261/HW5/5-2/output/CosineSim/|\n",
    "|Jaccard Similarity Synonyms Construction|s3://cerc-w261/HW5/5-2/output/JaccardSim/|\n",
    "|Cosine Similarity Synonyms Sort|s3://cerc-w261/HW5/5-2/output/SortedCosineSim/|\n",
    "|Jaccard Similarity Synonyms Sort|s3://cerc-w261/HW5/5-2/output/SortedJaccardSim/|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/createStripesJob_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/createStripesJob_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "import urllib\n",
    "\n",
    "\n",
    "class createStripesJob_1(MRJob):\n",
    "\n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.stripeMapper_init,\n",
    "                   mapper=self.stripeMapper,\n",
    "                   reducer=self.stripeReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    ## upon initializing each mapper I load into memory\n",
    "    ## stopwords, the top 10,000 words in descending order by frequency \n",
    "    ## and the 9,001-10,000 words in descending order by frequency as \n",
    "    ## the vocabulary\n",
    "    def stripeMapper_init(self):\n",
    "        with open('stopwords.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                self.stopwords.add(line)\n",
    "\n",
    "        with open('top10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.top10000words.add(word)\n",
    "\n",
    "        with open('ranked9001-10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "    def stripeMapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in self.top10000words and token1 not in self.stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in self.vocabulary and token2 not in self.stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    yield token1, stripe\n",
    "    def stripeReducer(self, key, value):\n",
    "        agg_stripe = {}\n",
    "        doc = key\n",
    "        for stripe in value:\n",
    "            for key,value in stripe.iteritems():\n",
    "                if key in agg_stripe.keys():\n",
    "                    agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "                else:\n",
    "                    agg_stripe[key] = int(value)\n",
    "        yield doc, agg_stripe\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    createStripesJob_1().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Job to create stripes from n-grams\n",
    "!chmod a+x src/createStripesJob_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/Stripes/ --recursive\n",
    "!python ~/w261/HW5/src/createStripesJob_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/Stripes \\\n",
    "        --file='/home/cloudera/w261/HW5/data/stopwords.txt#stopwords.txt' \\\n",
    "        --file='s3://cerc-w261/HW5/5-3/data/top10000words/top10000words.txt#top10000words.txt' \\\n",
    "        --file='s3://cerc-w261/HW5/5-3/data/9001-10000words/9001-10000words.txt#ranked9001-10000words.txt' \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/invertedIndexJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/invertedIndexJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "class invertedIndexJob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.invertedIndexMapper,\n",
    "                reducer=self.invertedIndexReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def invertedIndexMapper(self, _, line):\n",
    "        inverted_index = {}\n",
    "        line = line.split(\"\\t\")\n",
    "        key = line[0]\n",
    "        line = line[1]\n",
    "        doc = key\n",
    "        stripe = line\n",
    "        stripe = ast.literal_eval(stripe)\n",
    "        length = len(stripe)\n",
    "        norm_length = sqrt(length)\n",
    "        for k in stripe.keys():\n",
    "            stripe[k] = 1\n",
    "            stripe[k] = float(stripe[k]) / float(norm_length)\n",
    "            if k not in inverted_index.keys():\n",
    "                inverted_index[k] = {doc:{\"sim\":stripe[k]}}\n",
    "            else:\n",
    "                inverted_index[k][doc][\"sim\"] = stripe[k]\n",
    "        for k2, v2 in inverted_index.iteritems():\n",
    "            v2[doc][\"length\"] = length\n",
    "            yield k2, v2\n",
    "\n",
    "    def invertedIndexReducer(self, key, line):\n",
    "        posting_list = {}\n",
    "        doc = key\n",
    "        for stripe in line:\n",
    "        ## under the assumption that we have properly constructed\n",
    "        ## our inputs, we simply aggregate them as a single dict\n",
    "        ## in the value\n",
    "            for key,value in stripe.iteritems():\n",
    "                posting_list[key] = value\n",
    "        yield doc, posting_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    invertedIndexJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Job to create inverted indices from Stripes\n",
    "!chmod a+x src/invertedIndexJob.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/InvertedIndices/ --recursive\n",
    "!python ~/w261/HW5/src/invertedIndexJob.py -r emr s3://cerc-w261/HW5/5-2/output/Stripes/ \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/InvertedIndices \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cosineSimJob_simple.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cosineSimJob_simple.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "import itertools\n",
    "\n",
    "class cosineSimJob_simple(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.cosineSimMapper,\n",
    "                reducer=self.cosineSimReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def cosineSimMapper(self, _, line):\n",
    "        ## We are working with a try, except\n",
    "        ## framework to avoid artifacts that would\n",
    "        ## through off the entire EMR job\n",
    "        try:\n",
    "            line = line.split(\"\\t\")\n",
    "            value = line[1]\n",
    "            sys.stderr.write(\"THIS IS THE RAW LINE\\n\")\n",
    "            sys.stderr.write(value+\"\\n\")\n",
    "            value = ast.literal_eval(value)\n",
    "            \n",
    "            for subset in itertools.combinations(sorted(set(value)), 2):\n",
    "                sys.stderr.write(\"WENT INSIDE THE ITERATOR\")\n",
    "                k = str(subset[0])+\".\"+str(subset[1])\n",
    "                v = \"1,\"+str(value[subset[0]][\"length\"])+\",\"+str(value[subset[1]][\"length\"])\n",
    "                sys.stderr.write(\"KEY VALUE PAIR YIELDED: \"+k+\"\\t\"+v+\"\\n\\n\")\n",
    "                yield k, v\n",
    "\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def cosineSimReducer(self, key, value):\n",
    "        try:\n",
    "            cosine_similarity = 0\n",
    "            cardinality_key_intersection = 0\n",
    "            cardinality_key_1 = 0\n",
    "            cardinality_key_2 = 0\n",
    "            sys.stderr.write(\"THE KEY IS \"+key+\"\\n\\n\")\n",
    "            for v in value:\n",
    "                sys.stderr.write(\"THIS IS ONE OF THE VALUES\"+v+\"\\n\\n\")\n",
    "                v = v.split(\",\")\n",
    "                sys.stderr.write(\"CARDINALITY_1:\"+v[1]+\"\\n\\n\")\n",
    "                sys.stderr.write(\"CARDINALITY_2:\"+v[2]+\"\\n\\n\")\n",
    "                sys.stderr.write(\"TEMP_CARDINALITY_JOIN:\"+v[0]+\"\\n\\n\")\n",
    "                cardinality_key_1 = int(v[1])\n",
    "                cardinality_key_2 = int(v[2])\n",
    "                cardinality_key_intersection += int(v[0])\n",
    "            cosine_similarity = float(cardinality_key_intersection) / (sqrt(float(cardinality_key_1)) * sqrt(float(cardinality_key_2)))\n",
    "            sys.stderr.write(\"KV YIELDED: \"+key+\"\\t\"+str(cosine_similarity))\n",
    "            yield key, cosine_similarity\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    cosineSimJob_simple().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"A.B\"\t0.81649658092772592\r\n",
      "\"A.C\"\t0.57735026918962584\r\n",
      "\"B.C\"\t0.35355339059327373\r\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/cosineSimJob_simple.py\n",
    "!python /home/cloudera/w261/HW5/src/invertedIndexJob.py -r local \\\n",
    "data/systems_test_HW54.txt > toy_inverted_index\n",
    "# !cat toy_inverted_index\n",
    "!python ~/w261/HW5/src/cosineSimJob_simple.py -r hadoop \\\n",
    "toy_inverted_index > toy_cosine_output_simple\n",
    "!cat toy_cosine_output_simple\n",
    "# !cat data/systems_test_HW54.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Job to compute cosine similarities from inverted indices\n",
    "!chmod a+x src/cosineSimJob_simple.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/CosineSim/ --recursive\n",
    "!python ~/w261/HW5/src/cosineSimJob_simple.py -r emr \\\n",
    "s3://cerc-w261/HW5/5-2/output/InvertedIndices \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/CosineSim \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/jaccardSimJob_simple.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/jaccardSimJob_simple.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "import itertools\n",
    "\n",
    "class jaccardSimJob_simple(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.jaccardSimMapper,\n",
    "                reducer=self.jaccardSimReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def jaccardSimMapper(self, _, line):\n",
    "        ## We are working with a try, except\n",
    "        ## framework to avoid artifacts that would\n",
    "        ## through off the entire EMR job\n",
    "        try:\n",
    "            line = line.split(\"\\t\")\n",
    "            value = line[1]\n",
    "            sys.stderr.write(\"THIS IS THE RAW LINE\\n\")\n",
    "            sys.stderr.write(value+\"\\n\")\n",
    "            value = ast.literal_eval(value)\n",
    "            \n",
    "            for subset in itertools.combinations(sorted(set(value)), 2):\n",
    "                sys.stderr.write(\"WENT INSIDE THE ITERATOR\")\n",
    "                k = str(subset[0])+\".\"+str(subset[1])\n",
    "                v = \"1,\"+str(value[subset[0]][\"length\"])+\",\"+str(value[subset[1]][\"length\"])\n",
    "                sys.stderr.write(\"KEY VALUE PAIR YIELDED: \"+k+\"\\t\"+v+\"\\n\\n\")\n",
    "                yield k, v\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def jaccardSimReducer(self, key, value):\n",
    "        try:\n",
    "            jaccard_similarity = 0\n",
    "            cardinality_key_intersection = 0\n",
    "            cardinality_key_1 = 0\n",
    "            cardinality_key_2 = 0\n",
    "            sys.stderr.write(\"THE KEY IS \"+key+\"\\n\\n\")\n",
    "            for v in value:\n",
    "                sys.stderr.write(\"THIS IS ONE OF THE VALUES\"+v+\"\\n\\n\")\n",
    "                v = v.split(\",\")\n",
    "                sys.stderr.write(\"CARDINALITY_1:\"+v[1]+\"\\n\\n\")\n",
    "                sys.stderr.write(\"CARDINALITY_2:\"+v[2]+\"\\n\\n\")\n",
    "                sys.stderr.write(\"TEMP_CARDINALITY_JOIN:\"+v[0]+\"\\n\\n\")\n",
    "                cardinality_key_1 = int(v[1])\n",
    "                cardinality_key_2 = int(v[2])\n",
    "                cardinality_key_intersection += int(v[0])\n",
    "            jaccard_similarity = float(cardinality_key_intersection) / (float(cardinality_key_1) + float(cardinality_key_2) - float(cardinality_key_intersection))\n",
    "            sys.stderr.write(\"KV YIELDED: \"+key+\"\\t\"+str(jaccard_similarity))\n",
    "            yield key, jaccard_similarity\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    jaccardSimJob_simple().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"Z\"\t{\"A\": {\"length\": 3, \"sim\": 0.5773502691896258}, \"C\": {\"length\": 4, \"sim\": 0.5}}\n",
      "\"M\"\t{\"C\": {\"length\": 4, \"sim\": 0.5}}\n",
      "\"N\"\t{\"C\": {\"length\": 4, \"sim\": 0.5}}\n",
      "\"X\"\t{\"A\": {\"length\": 3, \"sim\": 0.5773502691896258}, \"B\": {\"length\": 2, \"sim\": 0.7071067811865475}}\n",
      "\"Y\"\t{\"A\": {\"length\": 3, \"sim\": 0.5773502691896258}, \"C\": {\"length\": 4, \"sim\": 0.5}, \"B\": {\"length\": 2, \"sim\": 0.7071067811865475}}\n",
      "\"A.B\"\t0.66666666666666663\n",
      "\"A.C\"\t0.40000000000000002\n",
      "\"B.C\"\t0.20000000000000001\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/jaccardSimJob_simple.py\n",
    "!python /home/cloudera/w261/HW5/src/invertedIndexJob.py -r local \\\n",
    "data/systems_test_HW54.txt > toy_inverted_index\n",
    "!cat toy_inverted_index\n",
    "!python ~/w261/HW5/src/jaccardSimJob_simple.py -r hadoop \\\n",
    "toy_inverted_index > toy_jaccard_output_simple\n",
    "!cat toy_jaccard_output_simple\n",
    "!cat data/systems_test_HW54.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Job to compute jaccard similarities from inverted indices\n",
    "!chmod a+x src/jaccardSimJob_simple.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/JaccardSim/ --recursive\n",
    "!python ~/w261/HW5/src/jaccardSimJob_simple.py -r emr \\\n",
    "s3://cerc-w261/HW5/5-2/output/InvertedIndices \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/JaccardSim \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sorting the word pairs by similarity__\n",
    "\n",
    "Next, I deliberately have a separate MapReduce job to sort the similarity scores computed for both Jaccard and cosine similarity. The reason that I have separated this job from the framework that computes the similarities is that I want the internal protocol for computing the similarities to be JSON, but for the purposes of sorting I want to use RawProtocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/SortSimilarityJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/SortSimilarityJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class SortSimilarityJob(MRJob):\n",
    "    \n",
    "    # The following three settings are your sorting best friends:\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 8,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def sortMapper(self, _, line):\n",
    "        key, value = line.split(\"\\t\")\n",
    "        #key = ngram\n",
    "        if float(value) >= 1:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.5:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.25:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.125:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.0625:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.03125:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.015625:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SortSimilarityJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MRJob to sort the Jaccard similarities\n",
    "!chmod a+x src/SortSimilarityJob.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/SortedJaccardSim/ --recursive\n",
    "!python ~/w261/HW5/src/SortSimilarityJob.py -r emr s3://cerc-w261/HW5/5-2/output/JaccardSim/ \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/SortedJaccardSim \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MRJob to sort the Cosine similarities\n",
    "!chmod a+x src/SortSimilarityJob.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/SortedCosineSim/ --recursive\n",
    "!python ~/w261/HW5/src/SortSimilarityJob.py -r emr s3://cerc-w261/HW5/5-2/output/CosineSim/ \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/SortedCosineSim \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MRJob to sort the Jaccard similarities (systems test data set)\n",
    "!chmod a+x src/SortSimilarityJob.py\n",
    "!python ~/w261/HW5/src/SortSimilarityJob.py -r hadoop \\\n",
    "toy_jaccard_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [\"A\", \"B\"]\t0.6666666666666666\t\n",
    "    [\"A\", \"C\"]\t0.4\t\n",
    "    [\"B\", \"C\"]\t0.2\t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## MRJob to sort the Cosine similarities (systems test data set)\n",
    "!chmod a+x src/SortSimilarityJob.py\n",
    "!python ~/w261/HW5/src/SortSimilarityJob.py -r hadoop \\\n",
    "toy_cosine_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    [\"A\", \"B\"]\t0.816496580927726\t\n",
    "    [\"A\", \"C\"]\t0.5773502691896258\t\n",
    "    [\"B\", \"C\"]\t0.35355339059327373\t"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/EvaluationJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/EvaluationJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "from nltk.corpus import wordnet as wn\n",
    "\n",
    "def synonyms(string):\n",
    "    syndict = {}\n",
    "    for i,j in enumerate(wn.synsets(string)):\n",
    "        syns = j.lemma_names()\n",
    "        for syn in syns:\n",
    "            syndict.setdefault(syn,1)\n",
    "    return syndict.keys()\n",
    "\n",
    "class EvaluationJob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        pair,_ = line.split(\"\\t\")\n",
    "        word1, word2 = pair.split(\".\")\n",
    "        key, value = line.split(\"\\t\")\n",
    "        yield (word1, [word2])\n",
    "        \n",
    "    def reducer(self, key, value):\n",
    "        all_syns =  set( reduce(lambda x,y: set(x).union(set(y)), value) )\n",
    "        nltk_syns = set( set( a.lower() for a in synonyms(key.replace('\"', '')) ) )\n",
    "        intersection = all_syns.intersection(nltk_syns)\n",
    "        \n",
    "        # calculate precision, recall and the f1-score\n",
    "        precision = float(len(intersection)) / float(len(all_syns)) if all_syns else 1\n",
    "        recall = float(len(intersection)) / float(len(nltk_syns)) if nltk_syns else 1\n",
    "        f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "        \n",
    "        yield (key, '{}-{}-{}'.format(precision, recall, f1_score) )\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    EvaluationJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/EvaluationJob.cloudera.20160621.065057.400101\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/EvaluationJob.cloudera.20160621.065057.400101/output...\n",
      "Removing temp directory /tmp/EvaluationJob.cloudera.20160621.065057.400101...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/EvaluationJob.py\n",
    "!python src/EvaluationJob.py -r local \\\n",
    "data/CosineSimTop1000/sorted_cosine_sims \\\n",
    "    > CosineSimTop1000_eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/EvaluationJob.cloudera.20160621.065243.929291\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/EvaluationJob.cloudera.20160621.065243.929291/output...\n",
      "Removing temp directory /tmp/EvaluationJob.cloudera.20160621.065243.929291...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/EvaluationJob.py\n",
    "!python src/EvaluationJob.py -r local \\\n",
    "data/JaccardSimTop1000/sorted_jaccard_sims \\\n",
    "    > JaccardSimTop1000_eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"\\\\\\\"ablest\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abnormal\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"ab\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abandon\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abandoned\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abandonment\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abbey\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abdomen\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abdominal\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"ability\\\\\\\"\"\t\"0.0-1-0.0\"\r\n"
     ]
    }
   ],
   "source": [
    "!head CosineSimTop1000_eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"\\\"\\\\\\\"ablest\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abnormal\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"ab\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abandon\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abandoned\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abandonment\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abbey\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abdomen\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abdominal\\\\\\\"\"\t\"0.0-1-0.0\"\r\n",
      "\"\\\"\\\\\\\"abilities\\\\\\\"\"\t\"0.0-1-0.0\"\r\n"
     ]
    }
   ],
   "source": [
    "!head JaccardSimTop1000_eval.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO PRECISION: 0.0000000000\n",
      "MACRO RECALL: 1.0000000000\n",
      " MACRO F1 SCORE: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "f = open('CosineSimTop1000_eval.txt', 'r')\n",
    "macro_precision = 0\n",
    "macro_recall = 0\n",
    "macro_f1_score = 0\n",
    "total = 0\n",
    "for line in f:\n",
    "    _, scores = line.split(\"\\t\")\n",
    "    p, r, f1 = scores.split(\"-\")\n",
    "    p = p.replace('\"','')\n",
    "    r = r.replace('\"','')\n",
    "    f1 = f1.replace('\"','')\n",
    "    macro_precision += float(p)\n",
    "    macro_recall += float(r)\n",
    "    macro_f1_score += float(f1)\n",
    "    total += 1\n",
    "macro_precision = float(macro_precision) / total\n",
    "macro_recall = float(macro_recall) / total\n",
    "macro_f1_score = float(macro_f1_score) / total\n",
    "\n",
    "print \"\"\"MACRO PRECISION: %.10f\\nMACRO RECALL: %.10f\\n MACRO F1 SCORE: %.10f\"\"\"%(macro_precision, macro_recall, macro_f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MACRO PRECISION: 0.0000000000\n",
      "MACRO RECALL: 1.0000000000\n",
      " MACRO F1 SCORE: 0.0000000000\n"
     ]
    }
   ],
   "source": [
    "f = open('JaccardSimTop1000_eval.txt', 'r')\n",
    "macro_precision = 0\n",
    "macro_recall = 0\n",
    "macro_f1_score = 0\n",
    "total = 0\n",
    "for line in f:\n",
    "    _, scores = line.split(\"\\t\")\n",
    "    p, r, f1 = scores.split(\"-\")\n",
    "    p = p.replace('\"','')\n",
    "    r = r.replace('\"','')\n",
    "    f1 = f1.replace('\"','')\n",
    "    macro_precision += float(p)\n",
    "    macro_recall += float(r)\n",
    "    macro_f1_score += float(f1)\n",
    "    total += 1\n",
    "macro_precision = float(macro_precision) / total\n",
    "macro_recall = float(macro_recall) / total\n",
    "macro_f1_score = float(macro_f1_score) / total\n",
    "\n",
    "print \"\"\"MACRO PRECISION: %.10f\\nMACRO RECALL: %.10f\\n MACRO F1 SCORE: %.10f\"\"\"%(macro_precision, macro_recall, macro_f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.6 (Optional)\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.7 (Optional)\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.6 (Optional)\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hw 5.7 (Optional)\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
