{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale \n",
    "\n",
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 5**\n",
    "\n",
    "**Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### ANSWER:\n",
    "\n",
    "__A data warehouse is a large storage facility for data that can come from a wide variety of sources and that can be in a wide variety of formats.__\n",
    "\n",
    "__A Star schema is a commonly used and simple schema used to define how data is stored in a data storage facility (be it a database or a data warehouse for example).__ The Star schema organizes data records into fact tables that may reference one to many other dimension tables where fact tables tend to store records that represent events whereas dimension tables tend to store records that represent characteristics of objects that are involved in events recorded in fact tables.\n",
    "\n",
    "The Star schema is widely used to represent data in a myriad of business applications. __Specifically, it is a very effective schema for the purposes of giving structure to data used by an OLTP (online transaction processing) system.__ This is in contrast to data used by OLAP (online analytical processing) systems."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "- In the database world what is 3NF? Does machine learning use data in 3NF? If so why?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "In the database world, __3NF is the third of a standard set of normalization steps that are applied to databases in order to (1) reduce data duplication in the database and (2) ensure that all records__ (and as such any and all attributes for each record) __in the database can be unambiguously identified through a key__ (where the key is either a single attribute or a set of attributes).\n",
    "\n",
    "__For the most part, machine learning does not use data in 3NF__; the reason being that the reduction in data duplication offered by 3NF actually stops the underlying distributed and sequential machine learning framework from properly identifying and subsequewntly processing records.\n",
    "\n",
    "- In what form does ML consume data?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "Taking ML to mean machine learning, ML consumes data that is in a raw, denormalized format. Punctually, this would be logs produced by applications, users, or a combination of both.\n",
    "\n",
    "- Why would one use log files that are denormalized?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "One would use log files that are denormalized for several of reasons. On one hand, denormalized logs provide the machine learning framework the most amount of context to operate on. That is, the more data is provided in each log or record, the more options the data scientist has to produce features for the ML framework to learn on.\n",
    "\n",
    "On another hand, given the distributed and sequential nature of the MapReduce framework that supports most of modern ML at scale, denormalized log data has the representational redundancy that would permit a framework of this nature to identify, differentiate between and process records."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "* In the database world, 3NF is a format of ...\n",
    "* ML consumes data in a de-normalized format...\n",
    "* One would use log files that are denormalized..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "I choose the table with the URLs (URL ID and URL name) as the Left table in the hashside join because it is considerably smaller than the URL ID and USER ID visits; by definition we wish to store the smallest of the tables being merged as the in-memory stored table. See below for the number of rows resulting from each of the joins:\n",
    "\n",
    "| |Inner Join|Left Outer Join|Right Outer Join|\n",
    "|---|---|---|---|\n",
    "|Number of rows|98654|98663|98654| |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Preprocessing/housekeeping\n",
    "#####################################\n",
    "\n",
    "## Create local directories for data\n",
    "\n",
    "#!mkdir data\n",
    "#!mkdir src\n",
    "\n",
    "## Download data for 5.2\n",
    "\n",
    "#!wget \"https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AADCHtrJ4CBCDO1po_OAWg0ia/anonymous-msweb.data?dl=0#\"\n",
    "#!mv \"anonymous-msweb.data?dl=0\" data/anonymous-msweb.data\n",
    "#!egrep \"^A,\" data/anonymous-msweb.data > data/URL_table.txt\n",
    "#!head data/URL_table.txt\n",
    "#!wc -l data/URL_table.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/preprocess.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Single node data preprocessing for HW4.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "url_dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    elements = line.split(\",\")\n",
    "    ## When a line starts with C we start \n",
    "    ## recording data for the customer\n",
    "    if elements[0] == 'C':\n",
    "        visitor_data = elements[0] + ',' + elements[2]\n",
    "        continue\n",
    "    ## When a line starts with V we start \n",
    "    ## recording data for the visit to a URL\n",
    "    elif elements[0] == 'V':\n",
    "        ## this is formatted as 'V,[URL_ID]\n",
    "        visit_data = elements[0] + ',' + elements[1]\n",
    "        ## this is formatted as 'V,[URL_ID],C,[USER_ID]\n",
    "        processed_line = visit_data + ',' + visitor_data\n",
    "        print processed_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/preprocess.py\n",
    "!./src/preprocess.py < data/anonymous-msweb.data \\\n",
    "> data/preprocessed_anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/InnerJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/InnerJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class InnerJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "            )\n",
    "        ]\n",
    "    in_memory_hash = {}\n",
    "    def mapper_init(self):\n",
    "        ## Upon initializing a mapper\n",
    "        ## we load into memory a dictionary\n",
    "        ## that holds the URL table\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = 'http://www.microsoft.com' + row[4].strip('\"')\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        ## for each record being processed by the mapper\n",
    "        ## we retrieve the URL in the in-memory hash\n",
    "        ## based on the URL ID of the record\n",
    "        url = self.in_memory_hash[url_id]\n",
    "        yield url_id, (url, user_id)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    InnerJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/InnerJoinMRjob.py\n",
    "!./src/InnerJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_InnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/RightOuterJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/RightOuterJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class RightOuterJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    def mapper_init(self):\n",
    "        ## Upon initializing a mapper\n",
    "        ## we load into memory a dictionary\n",
    "        ## that holds the URL table\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = 'http://www.microsoft.com' + row[4].strip('\"')\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        ## Since this is a Right Outer Join\n",
    "        ## we only emmit a result if there is a record\n",
    "        ## in our URL hash for the record being processed\n",
    "        ## in this case, the result is identical to the Inner Join\n",
    "        try:\n",
    "            url = self.in_memory_hash[url_id]\n",
    "        except KeyError:\n",
    "            sys.stderr.write(\"Did not find a URL for URL ID: %s\"%url_id)\n",
    "            url = \"NULL\"\n",
    "        yield url_id, (url, user_id)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    RightOuterJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808/output...\n",
      "Removing temp directory /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/RightOuterJoinMRjob.py\n",
    "!./src/RightOuterJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_RightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/LeftOuterJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/LeftOuterJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class LeftOuterJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k1,1 -k2,2r\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 1,\n",
    "            'mapreduce.job.maps': 1\n",
    "            \n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    visited_hash = {}\n",
    "    def mapper_init(self):\n",
    "        ## Upon initializing a mapper\n",
    "        ## we load into memory a dictionary\n",
    "        ## that holds the URL table\n",
    "        ## We also keep track of whether a URL\n",
    "        ## has been visited\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = {\"url\":'http://www.microsoft.com' + row[4].strip('\"'),\"visited\":0}\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        ## for each record processed\n",
    "        ## we mark the associated URL as \n",
    "        ## being visited\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        self.in_memory_hash[url_id][\"visited\"] = 1\n",
    "        try:\n",
    "            url = self.in_memory_hash[url_id][\"url\"]\n",
    "        except KeyError:\n",
    "            sys.stderr.write(\"Did not find a URL for URL ID: %s\\n\"%url_id)\n",
    "            url = \"NULL\"\n",
    "        yield (1, (url_id, url, user_id))\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        ## If there are URLs that have not been visited\n",
    "        ## we keep track to publish in the output table\n",
    "        ## for the Left Outer Join\n",
    "        for key in self.in_memory_hash.keys():\n",
    "            if self.in_memory_hash[key][\"visited\"] == 0:\n",
    "                yield (99, (key, self.in_memory_hash[key][\"url\"], \"NULL\"))\n",
    "        \n",
    "    def reducer(self, visited, visit_tuples):\n",
    "        ## In the reducer I publish all the matches but\n",
    "        ## also all the records in the URL table \n",
    "        ## that did not have a match\n",
    "        prev_url_id = \"\"\n",
    "        if int(visited) == 99:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                if prev_url_id != url_id:\n",
    "                    prev_url_id = url_id\n",
    "                    yield url_id, (url,\"NULL\")\n",
    "        else:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                uid = vt[2]\n",
    "                yield url_id, (url, uid)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    LeftOuterJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194/output...\n",
      "Removing temp directory /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/LeftOuterJoinMRjob.py\n",
    "!./src/LeftOuterJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_LeftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################\n",
      "##     INNER JOIN ROWS    ##\n",
      "############################\n",
      "98654 output_InnerJoin\n",
      "############################\n",
      "##  LEFT OUTER JOIN ROWS  ##\n",
      "############################\n",
      "98663 output_LeftOuterJoin\n",
      "############################\n",
      "## RIGHT OUTER JOIN ROWS  ##\n",
      "############################\n",
      "98654 output_RightOuterJoin\n"
     ]
    }
   ],
   "source": [
    "## Comparing results below\n",
    "\n",
    "!echo \"############################\"\n",
    "!echo \"##     INNER JOIN ROWS    ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_InnerJoin\n",
    "!echo \"############################\"\n",
    "!echo \"##  LEFT OUTER JOIN ROWS  ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_LeftOuterJoin\n",
    "!echo \"############################\"\n",
    "!echo \"## RIGHT OUTER JOIN ROWS  ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_RightOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AMSWER:\n",
    "- The longest 5-gram by number of characters is **AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR** with **159 characters**.\n",
    "- The top 10 most frequent words (unigrams) in decreasing order of frequency are:\n",
    "\n",
    "|Word|Frequency|\n",
    "|---|---|\n",
    "|the|5490815394|\n",
    "|of|3698583299|\n",
    "|to|2227866570|\n",
    "|in|1421312776|\n",
    "|a|1361123022|\n",
    "|and|1149577477|\n",
    "|that|802921147|\n",
    "|is|758328796|\n",
    "|be|688707130|\n",
    "|as|492170314|\n",
    "\n",
    "- The 20 most densely appearing words in decreasing order of relative frequency are:\n",
    "\n",
    "|Word|Relative Frequency|\n",
    "|---|---|\n",
    "|xxxx|11.5572916667|\n",
    "|blah|8.0741599073|\n",
    "|nnn|7.53333333333|\n",
    "|na|6.20174913142|\n",
    "|oooooooooooooooo|4.921875|\n",
    "|nd|4.85430572724|\n",
    "|llll|4.51162790698|\n",
    "|oooooo|4.16965001336|\n",
    "|ooooo|3.85863719347|\n",
    "|lillelu|3.76245210728|\n",
    "|madarassy|3.57692307692|\n",
    "|pfeffermann|3.57692307692|\n",
    "|meteoritical|3.56|\n",
    "|xxxxxxxx|3.5|\n",
    "|beep|3.22903885481|\n",
    "|latha|3.18867924528|\n",
    "|iyengar|2.91911764706|\n",
    "|counterfeiteth|2.825|\n",
    "|nonmorular|2.81981981982|\n",
    "|nonsquamous|2.81981981982|\n",
    "\n",
    "- The 20 least densely appearing words in decreasing order of relative frequency are:\n",
    "\n",
    "|Word|Relative Frequency|\n",
    "|---|---|\n",
    "|zwingst|1.0|\n",
    "|zwirnen|1.0|\n",
    "|zwischenstaatlicher|1.0|\n",
    "|zwitterionic|1.0|\n",
    "|zwt|1.0|\n",
    "|zwyn|1.0|\n",
    "|zx|1.0|\n",
    "|zxcvframeqasfuc|1.0|\n",
    "|zydeco|1.0|\n",
    "|zydom|1.0|\n",
    "|zygmunt|1.0|\n",
    "|zygomaticofacial|1.0|\n",
    "|zygomaticotemporal|1.0|\n",
    "|zygosity|1.0|\n",
    "|zylindrischen|1.0|\n",
    "|zymelman|1.0|\n",
    "|zymogens|1.0|\n",
    "|zymophore|1.0|\n",
    "|zymosan|1.0|\n",
    "|zymosis|1.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Preprocessing/housekeeping\n",
    "#####################################\n",
    "\n",
    "# !wget \"https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0#\"\n",
    "# !mv \"googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0\" data/googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "# !head -n 10 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt > data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "# !cat data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class Longest5ngramJob(MRJob):\n",
    "    \n",
    "    # Settings for sorting\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        ## Note that we are explicitly setting \n",
    "        ## multiple reducers, we are considering the \n",
    "        ## full output of the mappers as a key for the\n",
    "        ## shuffle phase, setting partitioners based on \n",
    "        ## artificial partition keys and we are sorting\n",
    "        ## based on the length of the 5-grams\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\",\n",
    "             'mapreduce.job.reduces': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    ## Notice that we are emmitting artificial \n",
    "    ## keys to ensure that the partitioners can \n",
    "    ## key off of them and guarantee sorted output\n",
    "    def mapper(self, _, line):\n",
    "        ngram = line.split(\"\\t\")[0]\n",
    "        key = ngram\n",
    "        value = len(ngram)\n",
    "        if int(value) >= 6000:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 3000:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 1500:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 750:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 375:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 186:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 93:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 45:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 25:\n",
    "            yield 'i', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'j', key+\"\\t\"+str(value)\n",
    "\n",
    "    ## I ignore the key as it was only used\n",
    "    ## for sorting\n",
    "    def reducer(self,key,value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Longest5ngramJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_1/ --recursive\n",
    "!python ~/w261/HW5/src/eda_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_1 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output\n",
    "!mkdir ~/w261/HW5/output/EDA_1\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_1/ output/EDA_1 --recursive\n",
    "!head -n 1  ~/w261/HW5/output/EDA_1/pa* | sort -k2,2nr | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_2.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class UnigramCounterJob(MRJob):\n",
    "    \n",
    "    # Settings for sorting\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        ## Note that we are explicitly setting \n",
    "        ## multiple reducers, we are considering the \n",
    "        ## full output of the mappers as a key for the\n",
    "        ## shuffle phase, setting partitioners based on \n",
    "        ## artificial partition keys and we are sorting\n",
    "        ## based on the word frequencies\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\",\n",
    "             'mapreduce.job.reduces': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        ngram_elements = line.split(\"\\t\")\n",
    "        ngram = ngram_elements[0]\n",
    "        count = int(ngram_elements[1])\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram.lower(), str(count)\n",
    "\n",
    "    def combiner(self, key, value):\n",
    "        temp_sum = 0\n",
    "        for v in value:\n",
    "            temp_sum += int(v)\n",
    "        yield key, str(temp_sum)\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "        temp_sum = 0\n",
    "        for v in value:\n",
    "            temp_sum += int(v)\n",
    "        yield key, str(temp_sum)\n",
    "    \n",
    "    def sortMapper(self, key, value):\n",
    "        if int(value) >= 6000:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 3000:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 1500:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 750:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 375:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 186:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 93:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 45:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 25:\n",
    "            yield 'i', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'j', key+\"\\t\"+str(value)\n",
    "\n",
    "    ## I ignore the key as it was only used\n",
    "    ## for sorting    \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    UnigramCounterJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_2.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_2/ --recursive\n",
    "!python ~/w261/HW5/src/eda_2.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_2 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output/EDA_2\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_2/ output/EDA_2 --recursive\n",
    "!head -n 10 ~/w261/HW5/output/EDA_2/pa* | sort -k2,2nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class WordFrequencyJob(MRJob):\n",
    "    \n",
    "    # Settings for sorting\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        ## Note that we are explicitly setting \n",
    "        ## multiple reducers, we are considering the \n",
    "        ## full output of the mappers as a key for the\n",
    "        ## shuffle phase, setting partitioners based on \n",
    "        ## artificial partition keys and we are sorting\n",
    "        ## based on the word relative frequencies\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\",\n",
    "             'mapreduce.job.reduces': 7,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        ngram_elements = line.split(\"\\t\")\n",
    "        ngram = ngram_elements[0]\n",
    "        count = int(ngram_elements[1])\n",
    "        page_count = int(ngram_elements[2])\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram.lower(), str(count)+\"\\t\"+str(page_count)\n",
    "    \n",
    "    def reducer (self, key, value):\n",
    "        total_count, total_page_count = 0, 0\n",
    "        for val in value:\n",
    "            val = val.split(\"\\t\")\n",
    "            total_count += int(val[0])\n",
    "            total_page_count += int(val[1])\n",
    "        yield key, str(float(total_count)/float(total_page_count))\n",
    "    \n",
    "    def sortMapper(self, key, value):\n",
    "        if float(value) >= 2:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 1:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.5:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.25:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.125:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.0625:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "            \n",
    "    ## I ignore the key as it was only used\n",
    "    ## for sorting   \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WordFrequencyJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_3.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_3/ --recursive\n",
    "!python ~/w261/HW5/src/eda_3.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_3 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output/EDA_3\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_3/ output/EDA_3 --recursive\n",
    "!head -n 20 ~/w261/HW5/output/EDA_3/pa* | sort -k2,2nr | head -n 20\n",
    "!tail -n 20 ~/w261/HW5/output/EDA_3/pa* | sort -k2,2nr | tail -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/eda_3.py\n",
    "# !./src/eda_3.py -r local \\\n",
    "#  data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "#     > output_eda3_1\n",
    "# !head -n 20 output_eda3_1\n",
    "# !tail -n 20 output_eda3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33\n",
      "1    33\n",
      "2    29\n",
      "3    28\n",
      "4    27\n",
      "5    26\n",
      "6    24\n",
      "7    23\n",
      "8    22\n",
      "9    17\n",
      "Name: 1, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd37a871850>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd37ae9e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"output_eda1\",\n",
    "                 sep='\\t',\n",
    "                header=None)\n",
    "\n",
    "counts = pd.Series(df[1])\n",
    "print counts\n",
    "counts.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3.1 OPTIONAL Question:\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using co-occurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -n 100 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "> data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/createStripesJob_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/createStripesJob_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "import urllib\n",
    "\n",
    "\n",
    "class createStripesJob_1(MRJob):\n",
    "\n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.stripeMapper_init,\n",
    "                   mapper=self.stripeMapper,\n",
    "                   reducer=self.stripeReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    ## upon initializing each mapper I load into memory\n",
    "    ## stopwords, the top 10,000 words in descending order by frequency \n",
    "    ## and the 9,001-10,000 words in descending order by frequency as \n",
    "    ## the vocabulary\n",
    "    def stripeMapper_init(self):\n",
    "        #with open('/home/cloudera/w261/HW5/data/stopwords.txt', 'r') as f:\n",
    "        with open('stopwords.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                self.stopwords.add(line)\n",
    "\n",
    "        #with open('/home/cloudera/w261/HW5/data/top10000words.txt', 'r') as f:\n",
    "        with open('top10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.top10000words.add(word)\n",
    "\n",
    "        #with open('/home/cloudera/w261/HW5/data/ranked9001-10000words.txt','r') as f:\n",
    "        with open('ranked9001-10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "    def stripeMapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in self.top10000words and token1 not in self.stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in self.vocabulary and token2 not in self.stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    yield token1, stripe\n",
    "    def stripeReducer(self, key, value):\n",
    "        agg_stripe = {}\n",
    "        doc = key\n",
    "        for stripe in value:\n",
    "            for key,value in stripe.iteritems():\n",
    "                if key in agg_stripe.keys():\n",
    "                    agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "                else:\n",
    "                    agg_stripe[key] = int(value)\n",
    "        yield doc, agg_stripe\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    createStripesJob_1().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/createStripesJob_1.py\n",
    "# !./src/createStripesJob_1.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes_31\n",
    "# !cat output_stripes_31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/_SUCCESS\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00009\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00010\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00000\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00001\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00002\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00011\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00005\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00006\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00004\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00003\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00007\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00013\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00008\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00014\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00015\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00012\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00018\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00017\n",
      "delete: s3://cerc-w261/HW5/5-2/output/Stripes/part-00016\n",
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/createStripesJob_1.cloudera.20160619.155139.402372\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/createStripesJob_1.cloudera.20160619.155139.402372/files/...\n",
      "Created new cluster j-20CXUT3YVC2CU\n",
      "Waiting for step 1 of 1 (s-36J093MYEYUMM) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40555/cluster\n",
      "  RUNNING for 7.0s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 39.6s\n",
      "  RUNNING for 70.3s\n",
      "  RUNNING for 100.9s\n",
      "  RUNNING for 131.5s\n",
      "  RUNNING for 162.7s\n",
      "  RUNNING for 193.4s\n",
      "  RUNNING for 224.7s\n",
      "  RUNNING for 255.2s\n",
      "  RUNNING for 286.2s\n",
      "  RUNNING for 317.1s\n",
      "  RUNNING for 348.2s\n",
      "  RUNNING for 379.0s\n",
      "  RUNNING for 410.3s\n",
      "  RUNNING for 441.6s\n",
      "  RUNNING for 472.8s\n",
      "  RUNNING for 503.8s\n",
      "  RUNNING for 534.4s\n",
      "  RUNNING for 565.7s\n",
      "  RUNNING for 596.9s\n",
      "  RUNNING for 628.1s\n",
      "  RUNNING for 659.2s\n",
      "  RUNNING for 690.2s\n",
      "  RUNNING for 721.0s\n",
      "  RUNNING for 751.5s\n",
      "  RUNNING for 782.2s\n",
      "  RUNNING for 813.2s\n",
      "  RUNNING for 843.9s\n",
      "  RUNNING for 875.2s\n",
      "  RUNNING for 906.3s\n",
      "  RUNNING for 936.9s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-36J093MYEYUMM on ec2-54-149-73-34.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-73-34.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-36J093MYEYUMM/syslog.2016-06-19-15\n",
      "  Parsing step log: ssh://ec2-54-149-73-34.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-36J093MYEYUMM/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=193249506\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=779997687\n",
      "\t\tFILE: Number of bytes written=1760883004\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=193249506\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=189\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10236637440\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=25426883520\n",
      "\t\tTotal time spent by all map tasks (ms)=7108776\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=319894920\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8828779\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=794590110\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7108776\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8828779\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9492280\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=66048\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=1793168524\n",
      "\t\tMap output materialized bytes=958908837\n",
      "\t\tMap output records=59524410\n",
      "\t\tMerged Map outputs=3610\n",
      "\t\tPhysical memory (bytes) snapshot=106481156096\n",
      "\t\tReduce input groups=9307\n",
      "\t\tReduce input records=59524410\n",
      "\t\tReduce output records=9307\n",
      "\t\tReduce shuffle bytes=958908837\n",
      "\t\tShuffled Maps =3610\n",
      "\t\tSpilled Records=119048820\n",
      "\t\tTotal committed heap usage (bytes)=129774387200\n",
      "\t\tVirtual memory (bytes) snapshot=435302092800\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/createStripesJob_1.cloudera.20160619.155139.402372/...\n",
      "Removing temp directory /tmp/createStripesJob_1.cloudera.20160619.155139.402372...\n",
      "Removing log files in s3://mrjob-4b78edd1b5baad75/tmp/logs/j-20CXUT3YVC2CU/...\n",
      "Terminating cluster: j-20CXUT3YVC2CU\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createStripesJob_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/Stripes/ --recursive\n",
    "!python ~/w261/HW5/src/createStripesJob_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/Stripes \\\n",
    "        --file='/home/cloudera/w261/HW5/data/stopwords.txt#stopwords.txt' \\\n",
    "        --file='s3://cerc-w261/HW5/5-3/data/top10000words/top10000words.txt#top10000words.txt' \\\n",
    "        --file='s3://cerc-w261/HW5/5-3/data/9001-10000words/9001-10000words.txt#ranked9001-10000words.txt' \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/createStripesJob_1.cloudera.20160618.183937.349766\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/createStripesJob_1.cloudera.20160618.183937.349766/files/...\n",
      "Created new cluster j-M6JFM1QD6CQH\n",
      "Waiting for step 1 of 1 (s-KZQTN4GYH01J) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40833/cluster\n",
      "  RUNNING for 30.5s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 63.4s\n",
      "  RUNNING for 94.5s\n",
      "  RUNNING for 125.8s\n",
      "  RUNNING for 156.9s\n",
      "  RUNNING for 188.2s\n",
      "  RUNNING for 219.6s\n",
      "  RUNNING for 250.7s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-KZQTN4GYH01J on ec2-54-149-72-203.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-72-203.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-KZQTN4GYH01J/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=154769\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1634725\n",
      "\t\tFILE: Number of bytes written=26625581\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=154769\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=189\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6505516800\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3810674880\n",
      "\t\tTotal time spent by all map tasks (ms)=4517720\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=203297400\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1323151\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=119083590\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4517720\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1323151\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=778980\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=51008\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=5469662\n",
      "\t\tMap output materialized bytes=2995328\n",
      "\t\tMap output records=214937\n",
      "\t\tMerged Map outputs=3610\n",
      "\t\tPhysical memory (bytes) snapshot=100265967616\n",
      "\t\tReduce input groups=157\n",
      "\t\tReduce input records=214937\n",
      "\t\tReduce output records=157\n",
      "\t\tReduce shuffle bytes=2995328\n",
      "\t\tShuffled Maps =3610\n",
      "\t\tSpilled Records=429874\n",
      "\t\tTotal committed heap usage (bytes)=113174904832\n",
      "\t\tVirtual memory (bytes) snapshot=435395162112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/createStripesJob_1.cloudera.20160618.183937.349766/...\n",
      "Removing temp directory /tmp/createStripesJob_1.cloudera.20160618.183937.349766...\n",
      "Removing log files in s3://mrjob-4b78edd1b5baad75/tmp/logs/j-M6JFM1QD6CQH/...\n",
      "Killing our SSH tunnel (pid 15765)\n",
      "Terminating cluster: j-M6JFM1QD6CQH\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createStripesJob_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/Stripes/ --recursive\n",
    "!python ~/w261/HW5/src/createStripesJob_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/Stripes \\\n",
    "        --file='/home/cloudera/w261/HW5/data/stopwords.txt#stopwords.txt' \\\n",
    "        --file='/home/cloudera/w261/HW5/data/top10000words.txt#top10000words.txt' \\\n",
    "        --file='/home/cloudera/w261/HW5/data/ranked9001-10000words.txt#ranked9001-10000words.txt' \\\n",
    "        --no-output\n",
    "# !mkdir ~/w261/HW5/output/Stripes\n",
    "# !aws s3 cp s3://cerc-w261/HW5/5-2/output/Stripes/ output/Stripes --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/cosineSimJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cosineSimJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "class cosineSimJob(MRJob):\n",
    "    \n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.invertedIndexMapper,\n",
    "                reducer=self.invertedIndexReducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.cosineSimMapper,\n",
    "                reducer=self.cosineSimReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def invertedIndexMapper(self, _, line):\n",
    "        inverted_index = {}\n",
    "        line = line.split(\"\\t\")\n",
    "        key = line[0]\n",
    "        line = line[1]\n",
    "        doc = key\n",
    "        stripe = line\n",
    "        stripe = ast.literal_eval(stripe)\n",
    "        length = len(stripe)\n",
    "        norm_length = sqrt(length)\n",
    "        for k in stripe.keys():\n",
    "            stripe[k] = 1\n",
    "            stripe[k] = float(stripe[k]) / float(norm_length)\n",
    "            if k not in inverted_index.keys():\n",
    "                inverted_index[k] = {doc:{\"sim\":stripe[k]}}\n",
    "            else:\n",
    "                inverted_index[k][doc][\"sim\"] = stripe[k]\n",
    "        for k2, v2 in inverted_index.iteritems():\n",
    "            v2[doc][\"length\"] = length\n",
    "            yield k2, v2\n",
    "    def invertedIndexReducer(self, key, line):\n",
    "        posting_list = {}\n",
    "        doc = key\n",
    "        for stripe in line:\n",
    "        ## under the assumption that we have properly constructed\n",
    "        ## our inputs, we simply aggregate them as a single dict\n",
    "        ## in the value\n",
    "            for key,value in stripe.iteritems():\n",
    "                posting_list[key] = value\n",
    "        yield doc, posting_list\n",
    "\n",
    "    def JaccardSimMapper(self, key, value):\n",
    "        yield 1,1\n",
    "\n",
    "    def JaccardSimReducer(self, key, value):\n",
    "        yield 1,1\n",
    "\n",
    "    def cosineSimMapper(self, key, value):\n",
    "        ## We are working with a try, except\n",
    "        ## framework to avoid artifacts that would\n",
    "        ## through off the entire EMR job\n",
    "        try:\n",
    "            CosineSim = set()\n",
    "            for key1, value1 in value.iteritems():\n",
    "                for key2, value2 in value.iteritems():\n",
    "                    if key1 != key2:\n",
    "                        if frozenset([key1, key2]) not in CosineSim:\n",
    "                            if key1 < key2:\n",
    "                                #CosineSim[key1+key2] = value1[\"sim\"] * value2[\"sim\"]\n",
    "                                CosineSim.add(frozenset([key1, key2]))\n",
    "                                cosine_similarity = float(value1[\"sim\"]) * float(value2[\"sim\"])\n",
    "                                yield (key1, key2), cosine_similarity\n",
    "                            else:\n",
    "                                #CosineSim[key2+key1] = value1[\"sim\"] * value2[\"sim\"]\n",
    "                                CosineSim.add(frozenset([key1, key2]))\n",
    "                                cosine_similarity = float(value1[\"sim\"]) * float(value2[\"sim\"])\n",
    "                                yield (key2, key1), cosine_similarity\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def cosineSimReducer(self, key, value):\n",
    "        try:\n",
    "            cosine_similarity = 0\n",
    "            for v in value:\n",
    "                cosine_similarity += float(v)\n",
    "            yield key, cosine_similarity\n",
    "        except:\n",
    "            pass\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cosineSimJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/cosineSimJob.cloudera.20160619.194812.839811\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/cosineSimJob.cloudera.20160619.194812.839811/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 2...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/cosineSimJob.py\n",
    "#!aws s3 rm s3://cerc-w261/HW5/5-2/output/CosineSim/ --recursive\n",
    "!python ~/w261/HW5/src/cosineSimJob.py -r hadoop data/Stripes \\\n",
    "    --output-dir=output_CosineSim \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/cosineSimJob.cloudera.20160619.190854.851232\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/cosineSimJob.cloudera.20160619.190854.851232/files/...\n",
      "Created new cluster j-236YMDTUDEI7N\n",
      "Waiting for step 1 of 2 (s-8FO8BTIJTZ38) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40630/cluster\n",
      "  RUNNING for 2.3s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 34.7s\n",
      "  RUNNING for 65.4s\n",
      "  RUNNING for 96.3s\n",
      "  RUNNING for 127.2s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-8FO8BTIJTZ38 on ec2-54-191-19-6.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-191-19-6.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-8FO8BTIJTZ38/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=194347349\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=747694226\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=400331535\n",
      "\t\tFILE: Number of bytes written=620874193\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=6100\n",
      "\t\tHDFS: Number of bytes written=747694226\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=203\n",
      "\t\tHDFS: Number of write operations=54\n",
      "\t\tS3: Number of bytes read=194347349\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=62\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=62\n",
      "\t\tLaunched reduce tasks=27\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5346822240\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2247696000\n",
      "\t\tTotal time spent by all map tasks (ms)=3713071\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=167088195\n",
      "\t\tTotal time spent by all reduce tasks (ms)=780450\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=70240500\n",
      "\t\tTotal vcore-seconds taken by all map tasks=3713071\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=780450\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1830600\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=22036\n",
      "\t\tInput split bytes=6100\n",
      "\t\tMap input records=9307\n",
      "\t\tMap output bytes=883023789\n",
      "\t\tMap output materialized bytes=211384811\n",
      "\t\tMap output records=12179862\n",
      "\t\tMerged Map outputs=1647\n",
      "\t\tPhysical memory (bytes) snapshot=37612015616\n",
      "\t\tReduce input groups=9307\n",
      "\t\tReduce input records=12179862\n",
      "\t\tReduce output records=9307\n",
      "\t\tReduce shuffle bytes=211384811\n",
      "\t\tShuffled Maps =1647\n",
      "\t\tSpilled Records=24359724\n",
      "\t\tTotal committed heap usage (bytes)=41365274624\n",
      "\t\tVirtual memory (bytes) snapshot=207308599296\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-22BIEEOS5T8MB) to complete...\n",
      "  RUNNING for 58.2s\n",
      "  RUNNING for 89.1s\n",
      "  RUNNING for 120.4s\n",
      "  RUNNING for 151.3s\n",
      "  RUNNING for 181.9s\n",
      "  RUNNING for 212.7s\n",
      "  RUNNING for 243.9s\n",
      "  RUNNING for 275.1s\n",
      "  RUNNING for 305.7s\n",
      "  RUNNING for 336.2s\n",
      "  RUNNING for 367.5s\n",
      "  RUNNING for 398.5s\n",
      "  RUNNING for 429.8s\n",
      "  RUNNING for 460.5s\n",
      "  RUNNING for 491.4s\n",
      "  RUNNING for 522.6s\n",
      "  RUNNING for 553.4s\n",
      "  RUNNING for 584.3s\n",
      "  RUNNING for 624.8s\n",
      "  RUNNING for 655.4s\n",
      "  RUNNING for 686.5s\n",
      "  RUNNING for 717.3s\n",
      "  RUNNING for 748.1s\n",
      "  RUNNING for 779.3s\n",
      "  RUNNING for 810.5s\n",
      "  RUNNING for 841.4s\n",
      "  RUNNING for 872.6s\n",
      "  RUNNING for 903.8s\n",
      "  RUNNING for 940.4s\n",
      "  RUNNING for 971.1s\n",
      "  RUNNING for 1002.7s\n",
      "  RUNNING for 1034.0s\n",
      "  RUNNING for 1065.1s\n",
      "  RUNNING for 1096.7s\n",
      "  RUNNING for 1128.1s\n",
      "  FAILED\n",
      "Cluster j-236YMDTUDEI7N is TERMINATING: Shut down as step failed\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-22BIEEOS5T8MB on ec2-54-191-19-6.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-191-19-6.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-22BIEEOS5T8MB/syslog\n",
      "Counters: 14\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=54\n",
      "\t\tFailed map tasks=102\n",
      "\t\tKilled map tasks=54\n",
      "\t\tLaunched map tasks=156\n",
      "\t\tOther local map tasks=100\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=84524225760\n",
      "\t\tTotal time spent by all map tasks (ms)=58697379\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=2641382055\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=58697379\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "Scanning logs for probable cause of failure...\n",
      "Looking for task logs in /mnt/var/log/hadoop/userlogs/application_1466363594194_0002 on ec2-54-191-19-6.us-west-2.compute.amazonaws.com and task/core nodes...\n",
      "  Parsing task syslog: ssh://ec2-54-191-19-6.us-west-2.compute.amazonaws.com!172.31.10.237/mnt/var/log/hadoop/userlogs/application_1466363594194_0002/container_1466363594194_0002_01_000155/syslog\n",
      "  Parsing task syslog: ssh://ec2-54-191-19-6.us-west-2.compute.amazonaws.com!172.31.10.237/mnt/var/log/hadoop/userlogs/application_1466363594194_0002/container_1466363594194_0002_01_000154/syslog\n",
      "  Parsing task stderr: ssh://ec2-54-191-19-6.us-west-2.compute.amazonaws.com!172.31.10.237/mnt/var/log/hadoop/userlogs/application_1466363594194_0002/container_1466363594194_0002_01_000154/stderr\n",
      "Probable cause of failure:\n",
      "\n",
      "R/W/S=4/48953/0 in:0=4/5 [rec/s] out:9790=48953/5 [rec/s]\n",
      "minRecWrittenToEnableSkip_=9223372036854775807 HOST=null\n",
      "USER=hadoop\n",
      "HADOOP_USER=null\n",
      "last tool output: |[\"\\\"dead\\\"\", \"\\\"satient\\\"\"]\t0.000257249598076202970446357453453|\n",
      "\n",
      "java.io.IOException: Broken pipe\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:345)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:65)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:170)\n",
      "\n",
      "(from lines 36-59 of ssh://ec2-54-191-19-6.us-west-2.compute.amazonaws.com!172.31.10.237/mnt/var/log/hadoop/userlogs/application_1466363594194_0002/container_1466363594194_0002_01_000154/syslog)\n",
      "\n",
      "while reading input from lines 13351683-26703364 of hdfs://172.31.11.179:9000/tmp/mrjob/cosineSimJob.cloudera.20160619.190854.851232/step-output/0000/part-00023\n",
      "\n",
      "\n",
      "Step 2 of 2 failed\n",
      "Killing our SSH tunnel (pid 4878)\n",
      "Terminating cluster: j-236YMDTUDEI7N\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/cosineSimJob.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/CosineSim/ --recursive\n",
    "!python ~/w261/HW5/src/cosineSimJob.py -r emr s3://cerc-w261/HW5/5-2/output/Stripes/ \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/CosineSim \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/_SUCCESS\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00009\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00010\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00000\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00011\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00013\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00004\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00002\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00012\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00008\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00005\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00003\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00007\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00006\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00014\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00015\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00016\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00018\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00017\n",
      "delete: s3://cerc-w261/HW5/5-2/output/CosineSim/part-00001\n",
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/cosineSimJob.cloudera.20160619.171628.987411\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/cosineSimJob.cloudera.20160619.171628.987411/files/...\n",
      "Created new cluster j-3IJO8S0OHE0U8\n",
      "Waiting for step 1 of 2 (s-RTCCIDEDC3JD) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40097/cluster\n",
      "  RUNNING for 3.2s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 36.5s\n",
      "  RUNNING for 67.8s\n",
      "  RUNNING for 98.9s\n",
      "  RUNNING for 129.7s\n",
      "  RUNNING for 160.2s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-RTCCIDEDC3JD on ec2-54-187-71-33.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-187-71-33.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-RTCCIDEDC3JD/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=193841975\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=747694226\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=400369134\n",
      "\t\tFILE: Number of bytes written=641710376\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4700\n",
      "\t\tHDFS: Number of bytes written=747694226\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=151\n",
      "\t\tHDFS: Number of write operations=38\n",
      "\t\tS3: Number of bytes read=193841975\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=47\n",
      "\t\tLaunched map tasks=47\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4707833760\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=2065345920\n",
      "\t\tTotal time spent by all map tasks (ms)=3269329\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=147119805\n",
      "\t\tTotal time spent by all reduce tasks (ms)=717134\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=64542060\n",
      "\t\tTotal vcore-seconds taken by all map tasks=3269329\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=717134\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1737950\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=19223\n",
      "\t\tInput split bytes=4700\n",
      "\t\tMap input records=9307\n",
      "\t\tMap output bytes=883023789\n",
      "\t\tMap output materialized bytes=234483842\n",
      "\t\tMap output records=12179862\n",
      "\t\tMerged Map outputs=893\n",
      "\t\tPhysical memory (bytes) snapshot=29200269312\n",
      "\t\tReduce input groups=9307\n",
      "\t\tReduce input records=12179862\n",
      "\t\tReduce output records=9307\n",
      "\t\tReduce shuffle bytes=234483842\n",
      "\t\tShuffled Maps =893\n",
      "\t\tSpilled Records=24359724\n",
      "\t\tTotal committed heap usage (bytes)=35994468352\n",
      "\t\tVirtual memory (bytes) snapshot=153792425984\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-1BTSN52TARS23) to complete...\n",
      "  RUNNING for 56.5s\n",
      "  RUNNING for 88.0s\n",
      "  RUNNING for 119.1s\n",
      "  RUNNING for 150.4s\n",
      "  RUNNING for 181.7s\n",
      "  RUNNING for 212.9s\n",
      "  RUNNING for 244.0s\n",
      "  RUNNING for 275.0s\n",
      "  RUNNING for 305.9s\n",
      "  RUNNING for 336.4s\n",
      "  RUNNING for 367.5s\n",
      "  RUNNING for 398.5s\n",
      "  RUNNING for 429.6s\n",
      "  RUNNING for 461.1s\n",
      "  RUNNING for 492.4s\n",
      "  RUNNING for 523.3s\n",
      "  RUNNING for 554.5s\n",
      "  RUNNING for 585.1s\n",
      "  RUNNING for 615.7s\n",
      "  RUNNING for 646.8s\n",
      "  RUNNING for 677.7s\n",
      "  RUNNING for 709.3s\n",
      "  RUNNING for 740.9s\n",
      "  RUNNING for 772.4s\n",
      "  RUNNING for 803.0s\n",
      "  RUNNING for 833.9s\n",
      "  RUNNING for 864.9s\n",
      "  RUNNING for 895.7s\n",
      "  RUNNING for 926.4s\n",
      "  RUNNING for 957.9s\n",
      "  RUNNING for 988.6s\n",
      "  RUNNING for 1020.1s\n",
      "  RUNNING for 1051.7s\n",
      "  RUNNING for 1082.8s\n",
      "  FAILED\n",
      "Cluster j-3IJO8S0OHE0U8 is RUNNING: Running step\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-1BTSN52TARS23 on ec2-54-187-71-33.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-187-71-33.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-1BTSN52TARS23/syslog\n",
      "Counters: 14\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=38\n",
      "\t\tFailed map tasks=73\n",
      "\t\tKilled map tasks=38\n",
      "\t\tLaunched map tasks=111\n",
      "\t\tOther local map tasks=71\n",
      "\t\tRack-local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=59301012960\n",
      "\t\tTotal time spent by all map tasks (ms)=41181259\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1853156655\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=41181259\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=0\n",
      "\t\tPhysical memory (bytes) snapshot=0\n",
      "\t\tVirtual memory (bytes) snapshot=0\n",
      "Scanning logs for probable cause of failure...\n",
      "Looking for task logs in /mnt/var/log/hadoop/userlogs/application_1466356850433_0002 on ec2-54-187-71-33.us-west-2.compute.amazonaws.com and task/core nodes...\n",
      "  Parsing task syslog: ssh://ec2-54-187-71-33.us-west-2.compute.amazonaws.com!172.31.2.128/mnt/var/log/hadoop/userlogs/application_1466356850433_0002/container_1466356850433_0002_01_000111/syslog\n",
      "  Parsing task syslog: ssh://ec2-54-187-71-33.us-west-2.compute.amazonaws.com!172.31.2.128/mnt/var/log/hadoop/userlogs/application_1466356850433_0002/container_1466356850433_0002_01_000110/syslog\n",
      "  Parsing task stderr: ssh://ec2-54-187-71-33.us-west-2.compute.amazonaws.com!172.31.2.128/mnt/var/log/hadoop/userlogs/application_1466356850433_0002/container_1466356850433_0002_01_000110/stderr\n",
      "Probable cause of failure:\n",
      "\n",
      "R/W/S=2/244605/0 in:0=2/22 [rec/s] out:11118=244605/22 [rec/s]\n",
      "minRecWrittenToEnableSkip_=9223372036854775807 HOST=null\n",
      "USER=hadoop\n",
      "HADOOP_USER=null\n",
      "last tool output: |[\"\\\"deals\\\"\"\"\", \"\\\"poetical\\\"\"]\t0.000501145801583958962184738565|\n",
      "\n",
      "java.io.IOException: Broken pipe\n",
      "\tat java.io.FileOutputStream.writeBytes(Native Method)\n",
      "\tat java.io.FileOutputStream.write(FileOutputStream.java:345)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.BufferedOutputStream.write(BufferedOutputStream.java:122)\n",
      "\tat java.io.DataOutputStream.write(DataOutputStream.java:107)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeUTF8(TextInputWriter.java:72)\n",
      "\tat org.apache.hadoop.streaming.io.TextInputWriter.writeValue(TextInputWriter.java:51)\n",
      "\tat org.apache.hadoop.streaming.PipeMapper.map(PipeMapper.java:106)\n",
      "\tat org.apache.hadoop.mapred.MapRunner.run(MapRunner.java:65)\n",
      "\tat org.apache.hadoop.streaming.PipeMapRunner.run(PipeMapRunner.java:34)\n",
      "\tat org.apache.hadoop.mapred.MapTask.runOldMapper(MapTask.java:432)\n",
      "\tat org.apache.hadoop.mapred.MapTask.run(MapTask.java:343)\n",
      "\tat org.apache.hadoop.mapred.YarnChild$2.run(YarnChild.java:175)\n",
      "\tat java.security.AccessController.doPrivileged(Native Method)\n",
      "\tat javax.security.auth.Subject.doAs(Subject.java:415)\n",
      "\tat org.apache.hadoop.security.UserGroupInformation.doAs(UserGroupInformation.java:1548)\n",
      "\tat org.apache.hadoop.mapred.YarnChild.main(YarnChild.java:170)\n",
      "\n",
      "(from lines 39-62 of ssh://ec2-54-187-71-33.us-west-2.compute.amazonaws.com!172.31.2.128/mnt/var/log/hadoop/userlogs/application_1466356850433_0002/container_1466356850433_0002_01_000110/syslog)\n",
      "\n",
      "while reading input from lines 37384711-43462612 of hdfs://172.31.3.9:9000/tmp/mrjob/cosineSimJob.cloudera.20160619.171628.987411/step-output/0000/part-00005\n",
      "\n",
      "\n",
      "Step 2 of 2 failed\n",
      "Killing our SSH tunnel (pid 25728)\n",
      "Terminating cluster: j-3IJO8S0OHE0U8\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/cosineSimJob.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/CosineSim/ --recursive\n",
    "!python ~/w261/HW5/src/cosineSimJob.py -r emr s3://cerc-w261/HW5/5-2/output/Stripes/ \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/CosineSim \\\n",
    "        --no-output\n",
    "# !mkdir ~/w261/HW5/output/Stripes\n",
    "# !aws s3 cp s3://cerc-w261/HW5/5-2/output/Stripes/ output/Stripes --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 200,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/jaccardSimJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/jaccardSimJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "class jaccardSimJob(MRJob):\n",
    "    \n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.invertedIndexMapper,\n",
    "                reducer=self.invertedIndexReducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.JaccardSimMapper,\n",
    "                reducer=self.JaccardSimReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def invertedIndexMapper(self, _, line):\n",
    "        inverted_index = {}\n",
    "        line = line.split(\"\\t\")\n",
    "        key = line[0]\n",
    "        line = line[1]\n",
    "        doc = key\n",
    "        stripe = line\n",
    "        stripe = ast.literal_eval(stripe)\n",
    "        length = len(stripe)\n",
    "        norm_length = sqrt(length)\n",
    "        for k in stripe.keys():\n",
    "            stripe[k] = 1\n",
    "            stripe[k] = float(stripe[k]) / float(norm_length)\n",
    "            if k not in inverted_index.keys():\n",
    "                inverted_index[k] = {doc:{\"sim\":stripe[k]}}\n",
    "            else:\n",
    "                inverted_index[k][doc][\"sim\"] = stripe[k]\n",
    "        for k2, v2 in inverted_index.iteritems():\n",
    "            v2[doc][\"length\"] = length\n",
    "            yield k2, v2\n",
    "    def invertedIndexReducer(self, key, line):\n",
    "        posting_list = {}\n",
    "        doc = key\n",
    "        for stripe in line:\n",
    "        ## under the assumption that we have properly constructed\n",
    "        ## our inputs, we simply aggregate them as a single dict\n",
    "        ## in the value\n",
    "            for key,value in stripe.iteritems():\n",
    "                posting_list[key] = value\n",
    "        yield doc, posting_list\n",
    "\n",
    "    def JaccardSimMapper(self, key, value):\n",
    "        ## We are working with a try, except\n",
    "        ## framework to avoid artifacts that would\n",
    "        ## through off the entire EMR job\n",
    "        try:\n",
    "            JaccardSim = set()\n",
    "            for key1, value1 in value.iteritems():\n",
    "                for key2, value2 in value.iteritems():\n",
    "                    if key1 != key2:\n",
    "                        if frozenset([key1, key2]) not in JaccardSim:\n",
    "                            if key1 < key2:\n",
    "                                JaccardSim.add(frozenset([key1, key2]))\n",
    "                                #jaccard_similarity = float(value1[\"sim\"]) * float(value2[\"sim\"])\n",
    "                                yield (key1, key2), (1, value1[\"length\"], value2[\"length\"])\n",
    "                            else:\n",
    "                                #CosineSim[key2+key1] = value1[\"sim\"] * value2[\"sim\"]\n",
    "                                JaccardSim.add(frozenset([key1, key2]))\n",
    "                                #cosine_similarity = float(value1[\"sim\"]) * float(value2[\"sim\"])\n",
    "                                yield (key2, key1), (1, value2[\"length\"], value1[\"length\"]) \n",
    "        except:\n",
    "            pass\n",
    "\n",
    "    def JaccardSimReducer(self, key, value):\n",
    "        jaccard_similarity = 0\n",
    "        cardinality_key_intersection = 0\n",
    "        for v in value:\n",
    "            cardinality_key_1 = int(v[1])\n",
    "            cardinality_key_2 = int(v[2])\n",
    "            cardinality_key_intersection += int(v[0])\n",
    "        jaccard_similarity = float(cardinality_key_intersection) / (cardinality_key_1 + cardinality_key_2 - cardinality_key_intersection)\n",
    "        yield key, jaccard_similarity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    jaccardSimJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/jaccardSimJob.cloudera.20160619.182843.074529\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/jaccardSimJob.cloudera.20160619.182843.074529/output...\n",
      "Removing temp directory /tmp/jaccardSimJob.cloudera.20160619.182843.074529...\n",
      "[\"B\", \"C\"]\t0.2\n",
      "[\"A\", \"B\"]\t0.6666666666666666\n",
      "[\"A\", \"C\"]\t0.4\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/jaccardSimJob.py\n",
    "# # !ls data\n",
    "# !cat data/systems_test_HW54.txt\n",
    "!python ~/w261/HW5/src/jaccardSimJob.py -r local \\\n",
    "data/systems_test_HW54.txt > toy_jaccard_output\n",
    "!cat toy_jaccard_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/cosineSimJob.cloudera.20160619.190731.049643\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/cosineSimJob.cloudera.20160619.190731.049643/output...\n",
      "Removing temp directory /tmp/cosineSimJob.cloudera.20160619.190731.049643...\n",
      "[\"B\", \"C\"]\t0.35355339059327373\n",
      "[\"A\", \"B\"]\t0.816496580927726\n",
      "[\"A\", \"C\"]\t0.5773502691896258\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/cosineSimJob.py\n",
    "# # !ls data\n",
    "# !cat data/systems_test_HW54.txt\n",
    "!python ~/w261/HW5/src/cosineSimJob.py -r local \\\n",
    "data/systems_test_HW54.txt > toy_cosine_output\n",
    "!cat toy_cosine_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Sorting the word pairs by similarity__\n",
    "\n",
    "Next, I deliberately have a separate MapReduce job to sort the similarity scores computed for both Jaccard and cosine similarity. The reason that I have separated this job from the framework that computes the similarities is that I want the internal protocol for computing the similarities to be JSON, but for the purposes of sorting I want to use RawProtocol."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/SortSimilarityJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/SortSimilarityJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class SortSimilarityJob(MRJob):\n",
    "    \n",
    "    # The following three settings are your sorting best friends:\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 7,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def sortMapper(self, _, line):\n",
    "        key, value = line.split(\"\\t\")\n",
    "        #key = ngram\n",
    "        if float(value) >= 2:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 1:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.5:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.25:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.125:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.0625:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    SortSimilarityJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/SortSimilarityJob.cloudera.20160619.170304.412618\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.170304.412618/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6028730901998817595.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1466105531452_0039\n",
      "  Submitted application application_1466105531452_0039\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1466105531452_0039/\n",
      "  Running job: job_1466105531452_0039\n",
      "  Job job_1466105531452_0039 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 14%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 57%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466105531452_0039 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.170304.412618/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=90\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=63\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=204\n",
      "\t\tFILE: Number of bytes written=1114280\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=456\n",
      "\t\tHDFS: Number of bytes written=63\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=10046848\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=18863872\n",
      "\t\tTotal time spent by all map tasks (ms)=78491\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=10046848\n",
      "\t\tTotal time spent by all reduce tasks (ms)=147374\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=18863872\n",
      "\t\tTotal vcore-seconds taken by all map tasks=78491\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=147374\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7900\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1166\n",
      "\t\tInput split bytes=366\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=69\n",
      "\t\tMap output materialized bytes=288\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=14\n",
      "\t\tPhysical memory (bytes) snapshot=987852800\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=288\n",
      "\t\tShuffled Maps =14\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=456523776\n",
      "\t\tVirtual memory (bytes) snapshot=6335033344\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.170304.412618/output...\n",
      "[\"A\", \"B\"]\t0.6666666666666666\t\n",
      "[\"A\", \"C\"]\t0.4\t\n",
      "[\"B\", \"C\"]\t0.2\t\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.170304.412618...\n",
      "Removing temp directory /tmp/SortSimilarityJob.cloudera.20160619.170304.412618...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/SortSimilarityJob.py\n",
    "# # !ls data\n",
    "# !cat data/systems_test_HW54.txt\n",
    "!python ~/w261/HW5/src/SortSimilarityJob.py -r hadoop \\\n",
    "toy_jaccard_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/SortSimilarityJob.cloudera.20160619.165003.051812\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.165003.051812/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  mapred.text.key.partitioner.options is deprecated. Instead, use mapreduce.partition.keypartitioner.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2537738848794430089.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1466105531452_0038\n",
      "  Submitted application application_1466105531452_0038\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1466105531452_0038/\n",
      "  Running job: job_1466105531452_0038\n",
      "  Job job_1466105531452_0038 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 14%\n",
      "   map 100% reduce 29%\n",
      "   map 100% reduce 48%\n",
      "   map 100% reduce 52%\n",
      "   map 100% reduce 57%\n",
      "   map 100% reduce 86%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1466105531452_0038 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.165003.051812/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=135\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=93\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=237\n",
      "\t\tFILE: Number of bytes written=1114345\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=499\n",
      "\t\tHDFS: Number of bytes written=93\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=27\n",
      "\t\tHDFS: Number of write operations=14\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=7\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=8042752\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=19867904\n",
      "\t\tTotal time spent by all map tasks (ms)=62834\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=8042752\n",
      "\t\tTotal time spent by all reduce tasks (ms)=155218\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=19867904\n",
      "\t\tTotal vcore-seconds taken by all map tasks=62834\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=155218\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=7670\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=818\n",
      "\t\tInput split bytes=364\n",
      "\t\tMap input records=3\n",
      "\t\tMap output bytes=99\n",
      "\t\tMap output materialized bytes=329\n",
      "\t\tMap output records=3\n",
      "\t\tMerged Map outputs=14\n",
      "\t\tPhysical memory (bytes) snapshot=985608192\n",
      "\t\tReduce input groups=3\n",
      "\t\tReduce input records=3\n",
      "\t\tReduce output records=3\n",
      "\t\tReduce shuffle bytes=329\n",
      "\t\tShuffled Maps =14\n",
      "\t\tSpilled Records=6\n",
      "\t\tTotal committed heap usage (bytes)=456523776\n",
      "\t\tVirtual memory (bytes) snapshot=6334590976\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.165003.051812/output...\n",
      "[\"A\", \"B\"]\t0.816496580927726\t\n",
      "[\"A\", \"C\"]\t0.5773502691896258\t\n",
      "[\"B\", \"C\"]\t0.35355339059327373\t\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/SortSimilarityJob.cloudera.20160619.165003.051812...\n",
      "Removing temp directory /tmp/SortSimilarityJob.cloudera.20160619.165003.051812...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/SortSimilarityJob.py\n",
    "# # !ls data\n",
    "# !cat data/systems_test_HW54.txt\n",
    "!python ~/w261/HW5/src/SortSimilarityJob.py -r hadoop \\\n",
    "toy_cosine_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/createInvertedIndexJob.cloudera.20160619.163132.450956\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/createInvertedIndexJob.cloudera.20160619.163132.450956/output...\n",
      "[\"B\", \"C\"]\t0.35355339059327373\n",
      "[\"A\", \"B\"]\t0.816496580927726\n",
      "[\"A\", \"C\"]\t0.5773502691896258\n",
      "Removing temp directory /tmp/createInvertedIndexJob.cloudera.20160619.163132.450956...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createInvertedIndexJob.py\n",
    "# # !ls data\n",
    "# !cat data/systems_test_HW54.txt\n",
    "!python ~/w261/HW5/src/createInvertedIndexJob.py -r local \\\n",
    "data/systems_test_HW54.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/createInvertedIndexJob.cloudera.20160618.195830.925353\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/createInvertedIndexJob.cloudera.20160618.195830.925353/files/...\n",
      "Created new cluster j-14R032S7V7RWW\n",
      "Waiting for step 1 of 1 (s-S5ZN4GRLPVJX) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40310/cluster\n",
      "  RUNNING for 16.7s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 49.4s\n",
      "  RUNNING for 80.8s\n",
      "  RUNNING for 111.7s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-S5ZN4GRLPVJX on ec2-54-149-78-32.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-78-32.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-S5ZN4GRLPVJX/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=273824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=559323\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=148363\n",
      "\t\tFILE: Number of bytes written=7237820\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4700\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=47\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=273824\n",
      "\t\tS3: Number of bytes written=559323\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=47\n",
      "\t\tLaunched map tasks=47\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1541985120\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=750752640\n",
      "\t\tTotal time spent by all map tasks (ms)=1070823\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=48187035\n",
      "\t\tTotal time spent by all reduce tasks (ms)=260678\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23461020\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1070823\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=260678\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=70210\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=15999\n",
      "\t\tInput split bytes=4700\n",
      "\t\tMap input records=157\n",
      "\t\tMap output bytes=661312\n",
      "\t\tMap output materialized bytes=228404\n",
      "\t\tMap output records=9530\n",
      "\t\tMerged Map outputs=893\n",
      "\t\tPhysical memory (bytes) snapshot=28123054080\n",
      "\t\tReduce input groups=157\n",
      "\t\tReduce input records=9530\n",
      "\t\tReduce output records=157\n",
      "\t\tReduce shuffle bytes=228404\n",
      "\t\tShuffled Maps =893\n",
      "\t\tSpilled Records=19060\n",
      "\t\tTotal committed heap usage (bytes)=32407814144\n",
      "\t\tVirtual memory (bytes) snapshot=154000154624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/createInvertedIndexJob.cloudera.20160618.195830.925353/...\n",
      "Removing temp directory /tmp/createInvertedIndexJob.cloudera.20160618.195830.925353...\n",
      "Removing log files in s3://mrjob-4b78edd1b5baad75/tmp/logs/j-14R032S7V7RWW/...\n",
      "Killing our SSH tunnel (pid 23986)\n",
      "Terminating cluster: j-14R032S7V7RWW\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createInvertedIndexJob.py\n",
    "#!aws s3 rm s3://cerc-w261/HW5/5-2/output/Stripes/ --recursive\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/InvertedIndex/ --recursive\n",
    "!python ~/w261/HW5/src/createInvertedIndexJob.py -r emr s3://cerc-w261/HW5/5-2/output/Stripes \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/InvertedIndex \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/createStripesJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/createStripesJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "class createStripesJob(MRJob):\n",
    "\n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    ## ########################################\n",
    "    ## THE STEPS BELOW ARE PURELY FOR TESTING\n",
    "    ## I NEED TO WRITE THE OUTPUT OF THE STRIPE\n",
    "    ## JOB TO S3\n",
    "    ## ########################################\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.stripeMapper_init,\n",
    "                   mapper=self.stripeMapper,\n",
    "                   reducer=self.stripeReducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.invertedIndexMapper,\n",
    "                reducer=self.invertedIndexReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def stripeMapper_init(self):\n",
    "        with open('/home/cloudera/w261/HW5/data/stopwords.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                self.stopwords.add(line)\n",
    "\n",
    "        with open('/home/cloudera/w261/HW5/data/top10000words.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.top10000words.add(word)\n",
    "\n",
    "        with open('/home/cloudera/w261/HW5/data/ranked9001-10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "    def stripeMapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in self.top10000words and token1 not in self.stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in self.vocabulary and token2 not in self.stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    yield token1, stripe\n",
    "    def stripeReducer(self, key, value):\n",
    "        agg_stripe = {}\n",
    "        doc = key\n",
    "        for stripe in value:\n",
    "            ## LOOKS LIKE THE KEYS IN THE STRIPE ARE\n",
    "            ## UNICODE STRINGS WATCH OUT FOR THAT SHIT\n",
    "            ## ALSO\n",
    "            ## SEEMS LIKE THE STRIPES ARE ALREADY DICTS...\n",
    "            \n",
    "#             sys.stderr.write(\"THIS IS A RAW STRIPE DUDE: %s\\n\" %stripe)\n",
    "#             sys.stderr.write(\"THIS IS THE RAW STRIPE TYPE:\\n\")\n",
    "#             sys.stderr.write(type(stripe).__name__)\n",
    "            \n",
    "            #stripe = ast.literal_eval(stripe)\n",
    "            for key,value in stripe.iteritems():\n",
    "                if key in agg_stripe.keys():\n",
    "                    agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "                else:\n",
    "                    agg_stripe[key] = int(value)\n",
    "        #yield key, agg_stripe\n",
    "        yield doc, agg_stripe\n",
    "    \n",
    "    def invertedIndexMapper(self, key, line):\n",
    "        inverted_index = {}\n",
    "        sys.stderr.write(\"\\nTHIS IS THE SECOND MAPPER'S INPUT KEY\\n\")\n",
    "        sys.stderr.write(key)\n",
    "        sys.stderr.write(\"\\n#######################################\\n\")\n",
    "        sys.stderr.write(\"\\nTHIS IS THE SECOND MAPPER'S INPUT LINE\\n\")\n",
    "        ## we have figured out that the \"line\" input here is a dict\n",
    "        ## fascinating shit... BUT this is good cause we are getting closer\n",
    "        ## I think that the key is being sent as a string separately\n",
    "        ## LETS CHECK!!!!!\n",
    "        sys.stderr.write(str(line))\n",
    "        sys.stderr.write(\"\\n#######################################\\n\")\n",
    "        #line = line.split(\"\\t\")\n",
    "        #doc = line[0]\n",
    "        doc = key\n",
    "        #stripe = line[1]\n",
    "        #stripe = stripe\n",
    "        #stripe = ast.literal_eval(stripe)\n",
    "        stripe = line\n",
    "        length = len(stripe)\n",
    "        norm_length = sqrt(length)\n",
    "        for k in stripe.keys():\n",
    "            stripe[k] = 1\n",
    "            stripe[k] = float(stripe[k]) / float(norm_length)\n",
    "            if k not in inverted_index.keys():\n",
    "                inverted_index[k] = {doc:{\"sim\":stripe[k]}}\n",
    "            else:\n",
    "                inverted_index[k][doc][\"sim\"] = stripe[k]\n",
    "        for k2, v2 in inverted_index.iteritems():\n",
    "            v2[doc][\"length\"] = length\n",
    "            #print (\"\\t\".join([key, str(value)]) + \"\\n\")\n",
    "            yield k2, v2\n",
    "    def invertedIndexReducer(self, key, line):\n",
    "        posting_list = {}\n",
    "        doc = key\n",
    "        for stripe in line:\n",
    "        ## under the assumption that we have properly constructed\n",
    "        ## our inputs, we simply aggregate them as a single dict\n",
    "        ## in the value\n",
    "            for key,value in stripe.iteritems():\n",
    "                posting_list[key] = value\n",
    "#                 if key in agg_stripe.keys():\n",
    "#                     agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "#                 else:\n",
    "#                     agg_stripe[key] = int(value)\n",
    "        #yield key, agg_stripe\n",
    "        # doc is NOT ACTUALLY A DOCUMENT\n",
    "        # WE ARE ONLY USING IT TO GROUP THE WORDS THAT WE DO(!!)\n",
    "        # CARE ABOUT... hence we can simply ignore the below \n",
    "        # peice of code\n",
    "        \n",
    "#         posting_list[\"*\"+doc] = len(posting_list)\n",
    "        yield doc, posting_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    createStripesJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aspiration\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746, \"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"basin\"\t{\"midst\": 1.0, \"*midst\": 1}\r\n",
      "\"battle\"\t{\"ensued\": 1.0, \"fought\": 1.0, \"*fought\": 1, \"*ensued\": 1}\r\n",
      "\"belief\"\t{\"necessity\": 1.0, \"*necessity\": 1}\r\n",
      "\"bit\"\t{\"*farther\": 1, \"farther\": 1.0}\r\n",
      "\"body\"\t{\"*projected\": 1, \"projected\": 1.0}\r\n",
      "\"bone\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746, \"aspiration\": 0.70710678118654746, \"*aspiration\": 2}\r\n",
      "\"book\"\t{\"*designed\": 1, \"designed\": 1.0}\r\n",
      "\"branch\"\t{\"*established\": 1, \"established\": 1.0}\r\n",
      "\"designed\"\t{\"book\": 1.0, \"*book\": 1}\r\n",
      "\"ensued\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"established\"\t{\"*branch\": 1, \"branch\": 1.0}\r\n",
      "\"farther\"\t{\"bit\": 1.0, \"*bit\": 1}\r\n",
      "\"fought\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"marrow\"\t{\"*aspiration\": 2, \"aspiration\": 0.70710678118654746, \"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"midst\"\t{\"basin\": 1.0, \"*basin\": 1}\r\n",
      "\"necessity\"\t{\"*belief\": 1, \"belief\": 1.0}\r\n",
      "\"projected\"\t{\"body\": 1.0, \"*body\": 1}\r\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x src/createStripesJob.py\n",
    "# !./src/createStripesJob.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes_2\n",
    "!cat output_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"marrow\"\t{\"*aspiration\": 2, \"aspiration\": 0.70710678118654746}\r\n",
      "\"bone\"\t{\"*aspiration\": 2, \"aspiration\": 0.70710678118654746}\r\n",
      "\"midst\"\t{\"basin\": 1.0, \"*basin\": 1}\r\n",
      "\"fought\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"ensued\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"necessity\"\t{\"*belief\": 1, \"belief\": 1.0}\r\n",
      "\"farther\"\t{\"bit\": 1.0, \"*bit\": 1}\r\n",
      "\"projected\"\t{\"body\": 1.0, \"*body\": 1}\r\n",
      "\"marrow\"\t{\"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"aspiration\"\t{\"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"designed\"\t{\"book\": 1.0, \"*book\": 1}\r\n",
      "\"established\"\t{\"*branch\": 1, \"branch\": 1.0}\r\n",
      "\"book\"\t{\"*designed\": 1, \"designed\": 1.0}\r\n",
      "\"battle\"\t{\"*ensued\": 1, \"ensued\": 1.0}\r\n",
      "\"branch\"\t{\"*established\": 1, \"established\": 1.0}\r\n",
      "\"bit\"\t{\"*farther\": 1, \"farther\": 1.0}\r\n",
      "\"battle\"\t{\"fought\": 1.0, \"*fought\": 1}\r\n",
      "\"aspiration\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746}\r\n",
      "\"bone\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746}\r\n",
      "\"basin\"\t{\"midst\": 1.0, \"*midst\": 1}\r\n",
      "\"belief\"\t{\"necessity\": 1.0, \"*necessity\": 1}\r\n",
      "\"body\"\t{\"*projected\": 1, \"projected\": 1.0}\r\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x src/createStripesJob.py\n",
    "# !./src/createStripesJob.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes_1\n",
    "!cat output_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aspiration\"\t{\"marrow\": 94, \"bone\": 94}\r\n",
      "\"basin\"\t{\"midst\": 205}\r\n",
      "\"battle\"\t{\"fought\": 82, \"ensued\": 95}\r\n",
      "\"belief\"\t{\"necessity\": 102}\r\n",
      "\"bit\"\t{\"farther\": 52}\r\n",
      "\"body\"\t{\"projected\": 187}\r\n",
      "\"bone\"\t{\"marrow\": 94, \"aspiration\": 94}\r\n",
      "\"book\"\t{\"designed\": 77}\r\n",
      "\"branch\"\t{\"established\": 63}\r\n",
      "\"designed\"\t{\"book\": 77}\r\n",
      "\"ensued\"\t{\"battle\": 95}\r\n",
      "\"established\"\t{\"branch\": 63}\r\n",
      "\"farther\"\t{\"bit\": 52}\r\n",
      "\"fought\"\t{\"battle\": 82}\r\n",
      "\"marrow\"\t{\"aspiration\": 94, \"bone\": 94}\r\n",
      "\"midst\"\t{\"basin\": 205}\r\n",
      "\"necessity\"\t{\"belief\": 102}\r\n",
      "\"projected\"\t{\"body\": 187}\r\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x src/createStripesJob.py\n",
    "# !./src/createStripesJob.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes\n",
    "!cat output_stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basin {'midst': 205}\n",
      "midst {'basin': 205}\n",
      "battle {'ensued': 95}\n",
      "ensued {'battle': 95}\n",
      "battle {'fought': 82}\n",
      "fought {'battle': 82}\n",
      "belief {'necessity': 102}\n",
      "necessity {'belief': 102}\n",
      "bit {'farther': 52}\n",
      "farther {'bit': 52}\n",
      "body {'projected': 187}\n",
      "projected {'body': 187}\n",
      "bone {'marrow': 94, 'aspiration': 94}\n",
      "marrow {'aspiration': 94, 'bone': 94}\n",
      "aspiration {'marrow': 94, 'bone': 94}\n",
      "book {'designed': 77}\n",
      "designed {'book': 77}\n",
      "branch {'established': 63}\n",
      "established {'branch': 63}\n"
     ]
    }
   ],
   "source": [
    "# !head data/top10000words.txt\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "\n",
    "stopwords = set()\n",
    "top10000words = set()\n",
    "vocabulary = set()\n",
    "    \n",
    "\n",
    "with open('data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords.add(line)\n",
    "\n",
    "with open('data/top10000words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        word = line[0].strip('\"')\n",
    "        top10000words.add(word)\n",
    "\n",
    "with open('data/ranked9001-10000words.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        word = line[0].strip('\"')\n",
    "        vocabulary.add(word)\n",
    "\n",
    "# print vocabulary\n",
    "\n",
    "with open('data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in top10000words and token1 not in stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in vocabulary and token2 not in stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    print token1, stripe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.6 (Optional)\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.7 (Optional)\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.6 (Optional)\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hw 5.7 (Optional)\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
