{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale \n",
    "\n",
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 5**\n",
    "\n",
    "**Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.0\n",
    "- What is a data warehouse? What is a Star schema? When is it used?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.1\n",
    "- In the database world what is 3NF? Does machine learning use data in 3NF? If so why? \n",
    "- In what form does ML consume data?\n",
    "- Why would one use log files that are denormalized?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "* In the database world, 3NF is a format of ...\n",
    "* ML consumes data in a de-normalized format...\n",
    "* One would use log files that are denormalized..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.2\n",
    "Using MRJob, implement a hashside join (memory-backed map-side) for left, right and inner joins. Run your code on the  data used in HW 4.4: (Recall HW 4.4: Find the most frequent visitor of each page using mrjob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID.)\n",
    "\n",
    "Justify which table you chose as the Left table in this hashside join.\n",
    "\n",
    "Please report the number of rows resulting from:\n",
    "\n",
    "- (1) Left joining Table Left with Table Right\n",
    "- (2) Right joining Table Left with Table Right\n",
    "- (3) Inner joining Table Left with Table Right\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "I choose the table with the URLs (URL ID and URL name) as the Left table in the hashside join because it is considerably smaller than the URL ID and USER ID visits; by definition we wish to store the smallest of the tables being merged as the in-memory stored table.\n",
    "\n",
    "| |Inner Join|Left Outer Join|Right Outer Join|\n",
    "|---|---|---|---|\n",
    "|Number of rows|98654|98663|98654| |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#####################################\n",
    "## Preprocessing/housekeeping\n",
    "#####################################\n",
    "#!mkdir data\n",
    "#!mkdir src\n",
    "#!wget \"https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AADCHtrJ4CBCDO1po_OAWg0ia/anonymous-msweb.data?dl=0#\"\n",
    "#!ls\n",
    "#!mv \"anonymous-msweb.data?dl=0\" data/anonymous-msweb.data\n",
    "#!ls data\n",
    "\n",
    "!egrep \"^A,\" data/anonymous-msweb.data > data/URL_table.txt\n",
    "#!head data/URL_table.txt\n",
    "#!wc -l data/URL_table.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/preprocess.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Single node data preprocessing for HW4.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "url_dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    elements = line.split(\",\")\n",
    "    ## Given the design of the raw data file, \n",
    "    ## the script will first populate the url_dict\n",
    "    if elements[0] == 'C':\n",
    "        visitor_data = elements[0] + ',' + elements[2]\n",
    "        continue\n",
    "    elif elements[0] == 'V':\n",
    "        ## this is formatted as 'V,[URL_ID],[URL]\n",
    "        visit_data = elements[0] + ',' + elements[1]\n",
    "        ## this is formatted as 'V,[URL_ID],[URL],C,[USER_ID]\n",
    "        processed_line = visit_data + ',' + visitor_data\n",
    "        print processed_line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/preprocess.py\n",
    "!./src/preprocess.py < data/anonymous-msweb.data \\\n",
    "> data/preprocessed_anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/InnerJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/InnerJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class InnerJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    def mapper_init(self):\n",
    "#         print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = 'http://www.microsoft.com' + row[4].strip('\"')\n",
    "    \n",
    "    def identity_mapper(self, _, line):\n",
    "        yield _,line\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        url = self.in_memory_hash[url_id]\n",
    "        yield url_id, (url, user_id)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    InnerJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/InnerJoinMRjob.py\n",
    "!./src/InnerJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_InnerJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/RightOuterJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/RightOuterJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class RightOuterJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    def mapper_init(self):\n",
    "#         print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = 'http://www.microsoft.com' + row[4].strip('\"')\n",
    "    \n",
    "    def identity_mapper(self, _, line):\n",
    "        yield _,line\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        try:\n",
    "            url = self.in_memory_hash[url_id]\n",
    "        except KeyError:\n",
    "            sys.stderr.write(\"Did not find a URL for URL ID: %s\"%url_id)\n",
    "            url = \"NULL\"\n",
    "        yield url_id, (url, user_id)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    RightOuterJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808/output...\n",
      "Removing temp directory /tmp/RightOuterJoinMRjob.cloudera.20160615.211408.396808...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/RightOuterJoinMRjob.py\n",
    "!./src/RightOuterJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_RightOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/LeftOuterJoinMRjob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/LeftOuterJoinMRjob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class LeftOuterJoinMRjob(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k1,1 -k2,2r\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 1,\n",
    "            'mapreduce.job.maps': 1\n",
    "            \n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   mapper_final=self.mapper_final,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    COUNTER = 0\n",
    "    in_memory_hash = {}\n",
    "    visited_hash = {}\n",
    "    def mapper_init(self):\n",
    "        temp = [s.split('\\n')[0].split(',') for s in open(\"/home/cloudera/w261/HW5/data/URL_table.txt\", \"r\").readlines()]\n",
    "        for row in temp:\n",
    "            self.in_memory_hash[row[1]] = {\"url\":'http://www.microsoft.com' + row[4].strip('\"'),\"visited\":0}\n",
    "            \n",
    "    def identity_mapper(self, _, line):\n",
    "        yield _,line\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        elements = line.split(',')\n",
    "        url_id = elements[1]\n",
    "        user_id = elements[3]\n",
    "        self.in_memory_hash[url_id][\"visited\"] = 1\n",
    "        try:\n",
    "            url = self.in_memory_hash[url_id][\"url\"]\n",
    "        except KeyError:\n",
    "            sys.stderr.write(\"Did not find a URL for URL ID: %s\\n\"%url_id)\n",
    "            url = \"NULL\"\n",
    "        yield (1, (url_id, url, user_id))\n",
    "        \n",
    "    def mapper_final(self):\n",
    "        #non_visited_pages = []\n",
    "        for key in self.in_memory_hash.keys():\n",
    "            if self.in_memory_hash[key][\"visited\"] == 0:\n",
    "                #print (key, (key, self.in_memory_hash[key][\"url\"], \"NULL\"))\n",
    "                yield (99, (key, self.in_memory_hash[key][\"url\"], \"NULL\"))\n",
    "#         non_visited_pages = [print (key, (key, self.visited_hash[key][\"url\"], \"NULL\")) if self.visited_hash[key][\"visited\"] == 0 for key in self.visited_hash.keys()]\n",
    "#         for key in self.visited_hash.keys():\n",
    "#             if self.visited_hash[key][\"visited\"] == 0:\n",
    "#                 non_visited_pages.\n",
    "#                 yield key, (url_id, url, \"NULL\") \n",
    "#         yield non_visited_pages\n",
    "    def identity_reducer(self, visited, visit_tuples):\n",
    "        prev_url_id = \"\"\n",
    "        if int(visited) == 99:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                uid = vt[2]\n",
    "                yield visited, (url_id, url, uid)\n",
    "        \n",
    "    def reducer(self, visited, visit_tuples):\n",
    "        prev_url_id = \"\"\n",
    "        if int(visited) == 99:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                if prev_url_id != url_id:\n",
    "                    prev_url_id = url_id\n",
    "                    yield url_id, (url,\"NULL\")\n",
    "        else:\n",
    "            for vt in visit_tuples:\n",
    "                url_id = vt[0]\n",
    "                url = vt[1]\n",
    "                uid = vt[2]\n",
    "                yield url_id, (url, uid)\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    LeftOuterJoinMRjob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194/output...\n",
      "Removing temp directory /tmp/LeftOuterJoinMRjob.cloudera.20160615.211419.631194...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/LeftOuterJoinMRjob.py\n",
    "!./src/LeftOuterJoinMRjob.py data/preprocessed_anonymous-msweb.data > output_LeftOuterJoin\n",
    "# !wc -l output_LeftOuterJoin\n",
    "# !tail output_LeftOuterJoin\n",
    "#!grep \"99\\t\" output_LeftOuterJoin"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "############################\n",
      "##     INNER JOIN ROWS    ##\n",
      "############################\n",
      "98654 output_InnerJoin\n",
      "############################\n",
      "##  LEFT OUTER JOIN ROWS  ##\n",
      "############################\n",
      "98663 output_LeftOuterJoin\n",
      "############################\n",
      "## RIGHT OUTER JOIN ROWS  ##\n",
      "############################\n",
      "98654 output_RightOuterJoin\n"
     ]
    }
   ],
   "source": [
    "!echo \"############################\"\n",
    "!echo \"##     INNER JOIN ROWS    ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_InnerJoin\n",
    "!echo \"############################\"\n",
    "!echo \"##  LEFT OUTER JOIN ROWS  ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_LeftOuterJoin\n",
    "!echo \"############################\"\n",
    "!echo \"## RIGHT OUTER JOIN ROWS  ##\"\n",
    "!echo \"############################\"\n",
    "!wc -l output_RightOuterJoin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3  EDA of Google n-grams dataset\n",
    "A large subset of the Google n-grams dataset\n",
    "\n",
    "https://aws.amazon.com/datasets/google-books-ngrams/\n",
    "\n",
    "which we have placed in a bucket/folder on Dropbox on s3:\n",
    "\n",
    "https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACUifrl6wrMrlK6a3X3lZ9Ea?dl=0 \n",
    "\n",
    "s3://filtered-5grams/\n",
    "\n",
    "In particular, this bucket contains (~200) files (10Meg each) in the format:\n",
    "\n",
    "\t(ngram) \\t (count) \\t (pages_count) \\t (books_count)\n",
    "\n",
    "For HW 5.3-5.5, for the Google n-grams dataset unit test and regression test your code using the \n",
    "first 10 lines of the following file:\n",
    "\n",
    "googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "\n",
    "Once you are happy with your test results proceed to generating  your results on the Google n-grams dataset. \n",
    "\n",
    "Do some EDA on this dataset using mrjob, e.g., \n",
    "\n",
    "- Longest 5-gram (number of characters)\n",
    "- Top 10 most frequent words (please use the count information), i.e., unigrams\n",
    "- 20 Most/Least densely appearing words (count/pages_count) sorted in decreasing order of relative frequency \n",
    "- Distribution of 5-gram sizes (character length).  E.g., count (using the count field) up how many times a 5-gram of 50 characters shows up. Plot the data graphically using a histogram."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### AMSWER:\n",
    "- The longest 5-gram by number of characters is **AIOPJUMRXUYVASLYHYPSIBEMAPODIKR UFRYDIUUOLBIGASUAURUSREXLISNAYE RNOONDQSRUNSUBUNOUGRABBERYAIRTC UTAHRAPTOREDILEIPMILBDUMMYUVERI SYEVRAHVELOCYALLOSAURUSLINROTSR** with **159 characters**.\n",
    "- The top 10 most frequent words (unigrams) in decreasing order of frequency are:\n",
    "\n",
    "|Word|Frequency|\n",
    "|---|---|\n",
    "|the|5490815394|\n",
    "|of|3698583299|\n",
    "|to|2227866570|\n",
    "|in|1421312776|\n",
    "|a|1361123022|\n",
    "|and|1149577477|\n",
    "|that|802921147|\n",
    "|is|758328796|\n",
    "|be|688707130|\n",
    "|as|492170314|\n",
    "\n",
    "- The 20 most densely appearing words in decreasing order of relative frequency are:\n",
    "\n",
    "|Word|Relative Frequency|\n",
    "|---|---|\n",
    "|xxxx|11.5572916667|\n",
    "|blah|8.0741599073|\n",
    "|nnn|7.53333333333|\n",
    "|na|6.20174913142|\n",
    "|oooooooooooooooo|4.921875|\n",
    "|nd|4.85430572724|\n",
    "|llll|4.51162790698|\n",
    "|oooooo|4.16965001336|\n",
    "|ooooo|3.85863719347|\n",
    "|lillelu|3.76245210728|\n",
    "|madarassy|3.57692307692|\n",
    "|pfeffermann|3.57692307692|\n",
    "|meteoritical|3.56|\n",
    "|xxxxxxxx|3.5|\n",
    "|beep|3.22903885481|\n",
    "|latha|3.18867924528|\n",
    "|iyengar|2.91911764706|\n",
    "|counterfeiteth|2.825|\n",
    "|nonmorular|2.81981981982|\n",
    "|nonsquamous|2.81981981982|\n",
    "\n",
    "- The 20 least densely appearing words in decreasing order of relative frequency are:\n",
    "\n",
    "|Word|Relative Frequency|\n",
    "|---|---|\n",
    "|zwingst|1.0|\n",
    "|zwirnen|1.0|\n",
    "|zwischenstaatlicher|1.0|\n",
    "|zwitterionic|1.0|\n",
    "|zwt|1.0|\n",
    "|zwyn|1.0|\n",
    "|zx|1.0|\n",
    "|zxcvframeqasfuc|1.0|\n",
    "|zydeco|1.0|\n",
    "|zydom|1.0|\n",
    "|zygmunt|1.0|\n",
    "|zygomaticofacial|1.0|\n",
    "|zygomaticotemporal|1.0|\n",
    "|zygosity|1.0|\n",
    "|zylindrischen|1.0|\n",
    "|zymelman|1.0|\n",
    "|zymogens|1.0|\n",
    "|zymophore|1.0|\n",
    "|zymosan|1.0|\n",
    "|zymosis|1.0|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/sh/tmqpc4o0xswhkvz/AACr50woxiBWoaiiLmnwduX8a/googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0#\"\n",
    "#!ls data\n",
    "!mv \"googlebooks-eng-all-5gram-20090715-0-filtered.txt?dl=0\" data/googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "!head -n 10 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt > data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt\n",
    "!cat data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class Longest5ngramJob(MRJob):\n",
    "    \n",
    "    # The following three settings are your sorting best friends:\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        ngram = line.split(\"\\t\")[0]\n",
    "        key = ngram\n",
    "        value = len(ngram)\n",
    "        if int(value) >= 6000:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 3000:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 1500:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 750:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 375:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 186:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 93:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 45:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 25:\n",
    "            yield 'i', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'j', key+\"\\t\"+str(value)\n",
    "\n",
    "    def reducer(self,key,value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    Longest5ngramJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_1/ --recursive\n",
    "!python ~/w261/HW5/src/eda_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_1 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output\n",
    "!mkdir ~/w261/HW5/output/EDA_1\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_1/ output/EDA_1 --recursive\n",
    "!head -n 1  ~/w261/HW5/output/EDA_1/pa* | sort -k2,2nr | head -n 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/eda_1.py\n",
    "# !./src/eda_1.py -r hadoop \\\n",
    "#  data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_eda1\n",
    "# !head output_eda1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_2.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class UnigramCounterJob(MRJob):\n",
    "    \n",
    "    # The following three settings are your sorting best friends:\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 10,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        ngram_elements = line.split(\"\\t\")\n",
    "        ngram = ngram_elements[0]\n",
    "        count = int(ngram_elements[1])\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram.lower(), str(count)\n",
    "\n",
    "    def combiner(self, key, value):\n",
    "        temp_sum = 0\n",
    "        for v in value:\n",
    "            temp_sum += int(v)\n",
    "        yield key, str(temp_sum)\n",
    "    \n",
    "    def reducer(self, key, value):\n",
    "#         yield key, sum(value)\n",
    "        temp_sum = 0\n",
    "        for v in value:\n",
    "            temp_sum += int(v)\n",
    "        yield key, str(temp_sum)\n",
    "    \n",
    "    def sortMapper(self, key, value):\n",
    "        #ngram = line.split(\"\\t\")[0]\n",
    "        #key = ngram\n",
    "        #value = len(ngram)\n",
    "        if int(value) >= 6000:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 3000:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 1500:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 750:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 375:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 186:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 93:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 45:\n",
    "            yield 'h', key+\"\\t\"+str(value)\n",
    "        elif int(value) >= 25:\n",
    "            yield 'i', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'j', key+\"\\t\"+str(value)\n",
    "        \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    UnigramCounterJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_2.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_2/ --recursive\n",
    "!python ~/w261/HW5/src/eda_2.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_2 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output/EDA_2\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_2/ output/EDA_2 --recursive\n",
    "!head -n 10 ~/w261/HW5/output/EDA_2/pa* | sort -k2,2nr | head -n 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/eda_2.py\n",
    "# !python ~/w261/HW5/src/eda_2.py -r local data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/eda_3.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/eda_3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "\n",
    "class WordFrequencyJob(MRJob):\n",
    "    \n",
    "    # The following three settings are your sorting best friends:\n",
    "    MRJob.SORT_VALUES = True \n",
    "    INTERNAL_PROTOCOL = RawProtocol\n",
    "    OUTPUT_PROTOCOL = RawProtocol\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 3,\n",
    "            'stream.map.output.field.separator':'/t',\n",
    "            'mapreduce.partition.keypartitioner.options':'-k1,1',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k3,3nr -k2,2\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 7,\n",
    "            'partitioner':'org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner'\n",
    "        }\n",
    "        return [\n",
    "            MRStep(\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer\n",
    "            ),\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.sortMapper,\n",
    "                   reducer=self.sortReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        ngram_elements = line.split(\"\\t\")\n",
    "        ngram = ngram_elements[0]\n",
    "        count = int(ngram_elements[1])\n",
    "        page_count = int(ngram_elements[2])\n",
    "        unigrams = ngram.split()\n",
    "        for unigram in unigrams:\n",
    "            yield unigram.lower(), str(count)+\"\\t\"+str(page_count)\n",
    "    \n",
    "    def reducer (self, key, value):\n",
    "        total_count, total_page_count = 0, 0\n",
    "        for val in value:\n",
    "            val = val.split(\"\\t\")\n",
    "            total_count += int(val[0])\n",
    "            total_page_count += int(val[1])\n",
    "        yield key, str(float(total_count)/float(total_page_count))\n",
    "    \n",
    "    def sortMapper(self, key, value):\n",
    "        #ngram = line.split(\"\\t\")[0]\n",
    "        #key = ngram\n",
    "        if float(value) >= 2:\n",
    "            yield 'a', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 1:\n",
    "            yield 'b', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.5:\n",
    "            yield 'c', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.25:\n",
    "            yield 'd', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.125:\n",
    "            yield 'e', key+\"\\t\"+str(value)\n",
    "        elif float(value) >= 0.0625:\n",
    "            yield 'f', key+\"\\t\"+str(value)\n",
    "        else:\n",
    "            yield 'g', key+\"\\t\"+str(value)\n",
    "        \n",
    "    def sortReducer(self, key, value):\n",
    "        for v in value:\n",
    "            yield None, v\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    WordFrequencyJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x src/eda_3.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/EDA_3/ --recursive\n",
    "!python ~/w261/HW5/src/eda_3.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_3 \\\n",
    "        --no-output\n",
    "!mkdir ~/w261/HW5/output/EDA_3\n",
    "!aws s3 cp s3://cerc-w261/HW5/5-2/output/EDA_3/ output/EDA_3 --recursive\n",
    "!head -n 20 ~/w261/HW5/output/EDA_3/pa* | sort -k2,2nr | head -n 20\n",
    "!tail -n 20 ~/w261/HW5/output/EDA_3/pa* | sort -k2,2nr | tail -n 50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/eda_3.py\n",
    "# !aws s3 rm --recursive s3://cerc-w261/HW5/5-2/output/EDA_3\n",
    "# !python ~/w261/HW5/src/eda_3.py -r emr s3://filtered-5grams \\\n",
    "#     --output-dir=s3://cerc-w261/HW5/5-2/output/EDA_3 \\\n",
    "#         --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/eda_3.py\n",
    "# !./src/eda_3.py -r local \\\n",
    "#  data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "#     > output_eda3_1\n",
    "# !head -n 20 output_eda3_1\n",
    "# !tail -n 20 output_eda3_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0    33\n",
      "1    33\n",
      "2    29\n",
      "3    28\n",
      "4    27\n",
      "5    26\n",
      "6    24\n",
      "7    23\n",
      "8    22\n",
      "9    17\n",
      "Name: 1, dtype: int64\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7fd37a871850>"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEACAYAAABI5zaHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nAAAASUVORK5CYII=\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7fd37ae9e350>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"output_eda1\",\n",
    "                 sep='\\t',\n",
    "                header=None)\n",
    "\n",
    "counts = pd.Series(df[1])\n",
    "print counts\n",
    "counts.hist(bins=20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.3.1 OPTIONAL Question:\n",
    "Plot the log-log plot of the frequency distributuion of unigrams. Does it follow power law distribution?\n",
    "\n",
    "For more background see:\n",
    "- https://en.wikipedia.org/wiki/Log%E2%80%93log_plot\n",
    "- https://en.wikipedia.org/wiki/Power_law"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.4  Synonym detection over 2Gig of Data\n",
    "\n",
    "For the remainder of this assignment you will work with two datasets:\n",
    "\n",
    "### 1: unit/systems test data set: SYSTEMS TEST DATASET\n",
    "Three terms, A,B,C and their corresponding strip-docs of co-occurring terms\n",
    "\n",
    "- DocA {X:20, Y:30, Z:5}\n",
    "- DocB {X:100, Y:20}\n",
    "- DocC {M:5, N:20, Z:5}\n",
    "\n",
    "### 2: A large subset of the Google n-grams dataset as was described above\n",
    "\n",
    "For each HW 5.4 -5.5.1 Please unit test and system test your code with respect \n",
    "to SYSTEMS TEST DATASET and show the results. \n",
    "Please compute the expected answer by hand and show your hand calculations for the \n",
    "SYSTEMS TEST DATASET. Then show the results you get with your system.\n",
    "\n",
    "In this part of the assignment we will focus on developing methods\n",
    "for detecting synonyms, using the Google 5-grams dataset. To accomplish\n",
    "this you must script two main tasks using MRJob:\n",
    "\n",
    "(1) Build stripes for the most frequent 10,000 words using co-occurence information based on\n",
    "the words ranked from 9001,-10,000 as a basis/vocabulary (drop stopword-like terms),\n",
    "and output to a file in your bucket on s3 (bigram analysis, though the words are non-contiguous).\n",
    "\n",
    "\n",
    "(2) Using two (symmetric) comparison methods of your choice \n",
    "(e.g., correlations, distances, similarities), pairwise compare \n",
    "all stripes (vectors), and output to a file in your bucket on s3.\n",
    "\n",
    "==Design notes for (1)==\n",
    "For this task you will be able to modify the pattern we used in HW 3.2\n",
    "(feel free to use the solution as reference). To total the word counts \n",
    "across the 5-grams, output the support from the mappers using the total \n",
    "order inversion pattern:\n",
    "\n",
    "<*word,count>\n",
    "\n",
    "to ensure that the support arrives before the cooccurrences.\n",
    "\n",
    "In addition to ensuring the determination of the total word counts,\n",
    "the mapper must also output co-occurrence counts for the pairs of\n",
    "words inside of each 5-gram. Treat these words as a basket,\n",
    "as we have in HW 3, but count all stripes or pairs in both orders,\n",
    "i.e., count both orderings: (word1,word2), and (word2,word1), to preserve\n",
    "symmetry in our output for (2).\n",
    "\n",
    "==Design notes for (2)==\n",
    "For this task you will have to determine a method of comparison.\n",
    "Here are a few that you might consider:\n",
    "\n",
    "- Jaccard\n",
    "- Cosine similarity\n",
    "- Spearman correlation\n",
    "- Euclidean distance\n",
    "- Taxicab (Manhattan) distance\n",
    "- Shortest path graph distance (a graph, because our data is symmetric!)\n",
    "- Pearson correlation\n",
    "- Kendall correlation\n",
    "\n",
    "However, be cautioned that some comparison methods are more difficult to\n",
    "parallelize than others, and do not perform more associations than is necessary, \n",
    "since your choice of association will be symmetric.\n",
    "\n",
    "Please use the inverted index (discussed in live session #5) based pattern to compute the pairwise (term-by-term) similarity matrix. \n",
    "\n",
    "Please report the size of the cluster used and the amount of time it takes to run for the index construction task and for the synonym calculation task. How many pairs need to be processed (HINT: use the posting list length to calculate directly)? Report your  Cluster configuration!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!head -n 100 data/googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "> data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/createStripesJob_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/createStripesJob_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "import urllib\n",
    "\n",
    "\n",
    "class createStripesJob_1(MRJob):\n",
    "\n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    ## ########################################\n",
    "    ## THE STEPS BELOW ARE PURELY FOR TESTING\n",
    "    ## I NEED TO WRITE THE OUTPUT OF THE STRIPE\n",
    "    ## JOB TO S3\n",
    "    ## ########################################\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.stripeMapper_init,\n",
    "                   mapper=self.stripeMapper,\n",
    "                   reducer=self.stripeReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def stripeMapper_init(self):\n",
    "        #with open('/home/cloudera/w261/HW5/data/stopwords.txt', 'r') as f:\n",
    "        with open('stopwords.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                self.stopwords.add(line)\n",
    "\n",
    "        #with open('/home/cloudera/w261/HW5/data/top10000words.txt', 'r') as f:\n",
    "        with open('top10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.top10000words.add(word)\n",
    "\n",
    "        #with open('/home/cloudera/w261/HW5/data/ranked9001-10000words.txt','r') as f:\n",
    "        with open('ranked9001-10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "    def stripeMapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in self.top10000words and token1 not in self.stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in self.vocabulary and token2 not in self.stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    yield token1, stripe\n",
    "    def stripeReducer(self, key, value):\n",
    "        agg_stripe = {}\n",
    "        doc = key\n",
    "        for stripe in value:\n",
    "            for key,value in stripe.iteritems():\n",
    "                if key in agg_stripe.keys():\n",
    "                    agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "                else:\n",
    "                    agg_stripe[key] = int(value)\n",
    "        yield doc, agg_stripe\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    createStripesJob_1().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# !chmod a+x src/createStripesJob_1.py\n",
    "# !./src/createStripesJob_1.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes_31\n",
    "# !cat output_stripes_31"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/createStripesJob_1.cloudera.20160618.183937.349766\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/createStripesJob_1.cloudera.20160618.183937.349766/files/...\n",
      "Created new cluster j-M6JFM1QD6CQH\n",
      "Waiting for step 1 of 1 (s-KZQTN4GYH01J) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40833/cluster\n",
      "  RUNNING for 30.5s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 63.4s\n",
      "  RUNNING for 94.5s\n",
      "  RUNNING for 125.8s\n",
      "  RUNNING for 156.9s\n",
      "  RUNNING for 188.2s\n",
      "  RUNNING for 219.6s\n",
      "  RUNNING for 250.7s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-KZQTN4GYH01J on ec2-54-149-72-203.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-72-203.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-KZQTN4GYH01J/syslog\n",
      "Counters: 56\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2156069116\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=154769\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1634725\n",
      "\t\tFILE: Number of bytes written=26625581\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=23450\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=190\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2156069116\n",
      "\t\tS3: Number of bytes written=154769\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=189\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=191\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tOther local map tasks=2\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6505516800\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3810674880\n",
      "\t\tTotal time spent by all map tasks (ms)=4517720\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=203297400\n",
      "\t\tTotal time spent by all reduce tasks (ms)=1323151\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=119083590\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4517720\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=1323151\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=778980\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=51008\n",
      "\t\tInput split bytes=23450\n",
      "\t\tMap input records=58682266\n",
      "\t\tMap output bytes=5469662\n",
      "\t\tMap output materialized bytes=2995328\n",
      "\t\tMap output records=214937\n",
      "\t\tMerged Map outputs=3610\n",
      "\t\tPhysical memory (bytes) snapshot=100265967616\n",
      "\t\tReduce input groups=157\n",
      "\t\tReduce input records=214937\n",
      "\t\tReduce output records=157\n",
      "\t\tReduce shuffle bytes=2995328\n",
      "\t\tShuffled Maps =3610\n",
      "\t\tSpilled Records=429874\n",
      "\t\tTotal committed heap usage (bytes)=113174904832\n",
      "\t\tVirtual memory (bytes) snapshot=435395162112\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/createStripesJob_1.cloudera.20160618.183937.349766/...\n",
      "Removing temp directory /tmp/createStripesJob_1.cloudera.20160618.183937.349766...\n",
      "Removing log files in s3://mrjob-4b78edd1b5baad75/tmp/logs/j-M6JFM1QD6CQH/...\n",
      "Killing our SSH tunnel (pid 15765)\n",
      "Terminating cluster: j-M6JFM1QD6CQH\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createStripesJob_1.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/Stripes/ --recursive\n",
    "!python ~/w261/HW5/src/createStripesJob_1.py -r emr s3://filtered-5grams \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/Stripes \\\n",
    "        --file='/home/cloudera/w261/HW5/data/stopwords.txt#stopwords.txt' \\\n",
    "        --file='/home/cloudera/w261/HW5/data/top10000words.txt#top10000words.txt' \\\n",
    "        --file='/home/cloudera/w261/HW5/data/ranked9001-10000words.txt#ranked9001-10000words.txt' \\\n",
    "        --no-output\n",
    "# !mkdir ~/w261/HW5/output/Stripes\n",
    "# !aws s3 cp s3://cerc-w261/HW5/5-2/output/Stripes/ output/Stripes --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing src/cosineSimJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/cosineSimJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "class cosineSimJob(MRJob):\n",
    "    \n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "                mapper=self.invertedIndexMapper,\n",
    "                reducer=self.invertedIndexReducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.cosineSimMapper,\n",
    "                reducer=self.cosineSimReducer\n",
    "            )\n",
    "        ]\n",
    "    \n",
    "    def invertedIndexMapper(self, _, line):\n",
    "        inverted_index = {}\n",
    "        line = line.split(\"\\t\")\n",
    "        key = line[0]\n",
    "        line = line[1]\n",
    "        doc = key\n",
    "        stripe = line\n",
    "        stripe = ast.literal_eval(stripe)\n",
    "        length = len(stripe)\n",
    "        norm_length = sqrt(length)\n",
    "        for k in stripe.keys():\n",
    "            stripe[k] = 1\n",
    "            stripe[k] = float(stripe[k]) / float(norm_length)\n",
    "            if k not in inverted_index.keys():\n",
    "                inverted_index[k] = {doc:{\"sim\":stripe[k]}}\n",
    "            else:\n",
    "                inverted_index[k][doc][\"sim\"] = stripe[k]\n",
    "        for k2, v2 in inverted_index.iteritems():\n",
    "            v2[doc][\"length\"] = length\n",
    "            yield k2, v2\n",
    "    def invertedIndexReducer(self, key, line):\n",
    "        posting_list = {}\n",
    "        doc = key\n",
    "        for stripe in line:\n",
    "        ## under the assumption that we have properly constructed\n",
    "        ## our inputs, we simply aggregate them as a single dict\n",
    "        ## in the value\n",
    "            for key,value in stripe.iteritems():\n",
    "                posting_list[key] = value\n",
    "        yield doc, posting_list\n",
    "\n",
    "    def JaccardSimMapper(self, key, value):\n",
    "        yield 1,1\n",
    "\n",
    "    def JaccardSimReducer(self, key, value):\n",
    "        yield 1,1\n",
    "\n",
    "    def cosineSimMapper(self, key, value):\n",
    "        CosineSim = set()\n",
    "        for key1, value1 in value.iteritems():\n",
    "            for key2, value2 in value.iteritems():\n",
    "                if key1 != key2:\n",
    "                    if frozenset([key1, key2]) not in CosineSim:\n",
    "                        if key1 < key2:\n",
    "                            #CosineSim[key1+key2] = value1[\"sim\"] * value2[\"sim\"]\n",
    "                            CosineSim.add(frozenset([key1, key2]))\n",
    "                            cosine_similarity = float(value1[\"sim\"]) * float(value2[\"sim\"])\n",
    "                            yield (key1, key2), cosine_similarity\n",
    "                        else:\n",
    "                            #CosineSim[key2+key1] = value1[\"sim\"] * value2[\"sim\"]\n",
    "                            CosineSim.add(frozenset([key1, key2]))\n",
    "                            cosine_similarity = float(value1[\"sim\"]) * float(value2[\"sim\"])\n",
    "                            yield (key2, key1), cosine_similarity\n",
    "\n",
    "    def cosineSimReducer(self, key, value):\n",
    "        cosine_similarity = 0\n",
    "        for v in value:\n",
    "            cosine_similarity += float(v)\n",
    "        yield key, cosine_similarity\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cosineSimJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/cosineSimJob.cloudera.20160618.210710.539109\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/cosineSimJob.cloudera.20160618.210710.539109/files/...\n",
      "Created new cluster j-2V7W9L4EI449N\n",
      "Waiting for step 1 of 2 (s-2CWK7BKPMMM8D) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40625/cluster\n",
      "  RUNNING for 29.7s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 62.1s\n",
      "  RUNNING for 92.6s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2CWK7BKPMMM8D on ec2-54-213-47-248.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-213-47-248.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2CWK7BKPMMM8D/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=273824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=559323\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=148443\n",
      "\t\tFILE: Number of bytes written=7236992\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4700\n",
      "\t\tHDFS: Number of bytes written=559323\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=151\n",
      "\t\tHDFS: Number of write operations=38\n",
      "\t\tS3: Number of bytes read=273824\n",
      "\t\tS3: Number of bytes written=0\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=47\n",
      "\t\tLaunched map tasks=47\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1572734880\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=654517440\n",
      "\t\tTotal time spent by all map tasks (ms)=1092177\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=49147965\n",
      "\t\tTotal time spent by all reduce tasks (ms)=227263\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=20453670\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1092177\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=227263\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=283820\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=19202\n",
      "\t\tInput split bytes=4700\n",
      "\t\tMap input records=157\n",
      "\t\tMap output bytes=661312\n",
      "\t\tMap output materialized bytes=228404\n",
      "\t\tMap output records=9530\n",
      "\t\tMerged Map outputs=893\n",
      "\t\tPhysical memory (bytes) snapshot=27826511872\n",
      "\t\tReduce input groups=157\n",
      "\t\tReduce input records=9530\n",
      "\t\tReduce output records=157\n",
      "\t\tReduce shuffle bytes=228404\n",
      "\t\tShuffled Maps =893\n",
      "\t\tSpilled Records=19060\n",
      "\t\tTotal committed heap usage (bytes)=31276924928\n",
      "\t\tVirtual memory (bytes) snapshot=153859166208\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Waiting for step 2 of 2 (s-2AR651V7AVDRB) to complete...\n",
      "  RUNNING for 77.4s\n",
      "  RUNNING for 108.6s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2AR651V7AVDRB on ec2-54-213-47-248.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-213-47-248.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2AR651V7AVDRB/syslog\n",
      "Counters: 55\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=989708\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=614106\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1411500\n",
      "\t\tFILE: Number of bytes written=15737561\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=997228\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=94\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=0\n",
      "\t\tS3: Number of bytes written=614106\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=44\n",
      "\t\tLaunched map tasks=47\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tRack-local map tasks=3\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1794108960\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=849142080\n",
      "\t\tTotal time spent by all map tasks (ms)=1245909\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=56065905\n",
      "\t\tTotal time spent by all reduce tasks (ms)=294841\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=26535690\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1245909\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=294841\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=112060\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=21089\n",
      "\t\tInput split bytes=7520\n",
      "\t\tMap input records=157\n",
      "\t\tMap output bytes=19726778\n",
      "\t\tMap output materialized bytes=7467018\n",
      "\t\tMap output records=381978\n",
      "\t\tMerged Map outputs=893\n",
      "\t\tPhysical memory (bytes) snapshot=28739432448\n",
      "\t\tReduce input groups=12095\n",
      "\t\tReduce input records=381978\n",
      "\t\tReduce output records=12095\n",
      "\t\tReduce shuffle bytes=7467018\n",
      "\t\tShuffled Maps =893\n",
      "\t\tSpilled Records=763956\n",
      "\t\tTotal committed heap usage (bytes)=31518097408\n",
      "\t\tVirtual memory (bytes) snapshot=153856794624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/cosineSimJob.cloudera.20160618.210710.539109/...\n",
      "Removing temp directory /tmp/cosineSimJob.cloudera.20160618.210710.539109...\n",
      "Removing log files in s3://mrjob-4b78edd1b5baad75/tmp/logs/j-2V7W9L4EI449N/...\n",
      "Terminating cluster: j-2V7W9L4EI449N\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/cosineSimJob.py\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/CosineSim/ --recursive\n",
    "!python ~/w261/HW5/src/cosineSimJob.py -r emr s3://cerc-w261/HW5/5-2/output/Stripes/ \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/CosineSim \\\n",
    "        --no-output\n",
    "# !mkdir ~/w261/HW5/output/Stripes\n",
    "# !aws s3 cp s3://cerc-w261/HW5/5-2/output/Stripes/ output/Stripes --recursive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/createInvertedIndexJob.cloudera.20160618.205737.517667\n",
      "Running step 1 of 2...\n",
      "Running step 2 of 2...\n",
      "Streaming final output from /tmp/createInvertedIndexJob.cloudera.20160618.205737.517667/output...\n",
      "[\"B\", \"C\"]\t0.35355339059327373\n",
      "[\"A\", \"B\"]\t0.816496580927726\n",
      "[\"A\", \"C\"]\t0.5773502691896258\n",
      "Removing temp directory /tmp/createInvertedIndexJob.cloudera.20160618.205737.517667...\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createInvertedIndexJob.py\n",
    "# # !ls data\n",
    "# !cat data/systems_test_HW54.txt\n",
    "!python ~/w261/HW5/src/createInvertedIndexJob.py -r local \\\n",
    "data/systems_test_HW54.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/createInvertedIndexJob.cloudera.20160618.195830.925353\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/createInvertedIndexJob.cloudera.20160618.195830.925353/files/...\n",
      "Created new cluster j-14R032S7V7RWW\n",
      "Waiting for step 1 of 1 (s-S5ZN4GRLPVJX) to complete...\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING)\n",
      "  PENDING (cluster is STARTING: Configuring cluster software)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  PENDING (cluster is BOOTSTRAPPING: Running bootstrap actions)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40310/cluster\n",
      "  RUNNING for 16.7s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 49.4s\n",
      "  RUNNING for 80.8s\n",
      "  RUNNING for 111.7s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-S5ZN4GRLPVJX on ec2-54-149-78-32.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-78-32.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-S5ZN4GRLPVJX/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=273824\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=559323\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=148363\n",
      "\t\tFILE: Number of bytes written=7237820\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4700\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=47\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=273824\n",
      "\t\tS3: Number of bytes written=559323\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=47\n",
      "\t\tLaunched map tasks=47\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1541985120\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=750752640\n",
      "\t\tTotal time spent by all map tasks (ms)=1070823\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=48187035\n",
      "\t\tTotal time spent by all reduce tasks (ms)=260678\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=23461020\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1070823\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=260678\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=70210\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=15999\n",
      "\t\tInput split bytes=4700\n",
      "\t\tMap input records=157\n",
      "\t\tMap output bytes=661312\n",
      "\t\tMap output materialized bytes=228404\n",
      "\t\tMap output records=9530\n",
      "\t\tMerged Map outputs=893\n",
      "\t\tPhysical memory (bytes) snapshot=28123054080\n",
      "\t\tReduce input groups=157\n",
      "\t\tReduce input records=9530\n",
      "\t\tReduce output records=157\n",
      "\t\tReduce shuffle bytes=228404\n",
      "\t\tShuffled Maps =893\n",
      "\t\tSpilled Records=19060\n",
      "\t\tTotal committed heap usage (bytes)=32407814144\n",
      "\t\tVirtual memory (bytes) snapshot=154000154624\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/createInvertedIndexJob.cloudera.20160618.195830.925353/...\n",
      "Removing temp directory /tmp/createInvertedIndexJob.cloudera.20160618.195830.925353...\n",
      "Removing log files in s3://mrjob-4b78edd1b5baad75/tmp/logs/j-14R032S7V7RWW/...\n",
      "Killing our SSH tunnel (pid 23986)\n",
      "Terminating cluster: j-14R032S7V7RWW\n"
     ]
    }
   ],
   "source": [
    "!chmod a+x src/createInvertedIndexJob.py\n",
    "#!aws s3 rm s3://cerc-w261/HW5/5-2/output/Stripes/ --recursive\n",
    "!aws s3 rm s3://cerc-w261/HW5/5-2/output/InvertedIndex/ --recursive\n",
    "!python ~/w261/HW5/src/createInvertedIndexJob.py -r emr s3://cerc-w261/HW5/5-2/output/Stripes \\\n",
    "    --output-dir=s3://cerc-w261/HW5/5-2/output/InvertedIndex \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting src/createStripesJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile src/createStripesJob.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import mrjob\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "class createStripesJob(MRJob):\n",
    "\n",
    "    stopwords = set()\n",
    "    top10000words = set()\n",
    "    vocabulary = set()\n",
    "    \n",
    "    ## ########################################\n",
    "    ## THE STEPS BELOW ARE PURELY FOR TESTING\n",
    "    ## I NEED TO WRITE THE OUTPUT OF THE STRIPE\n",
    "    ## JOB TO S3\n",
    "    ## ########################################\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.stripeMapper_init,\n",
    "                   mapper=self.stripeMapper,\n",
    "                   reducer=self.stripeReducer\n",
    "            ),\n",
    "            MRStep(\n",
    "                mapper=self.invertedIndexMapper,\n",
    "                reducer=self.invertedIndexReducer\n",
    "            )\n",
    "        ]\n",
    "\n",
    "    def stripeMapper_init(self):\n",
    "        with open('/home/cloudera/w261/HW5/data/stopwords.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                self.stopwords.add(line)\n",
    "\n",
    "        with open('/home/cloudera/w261/HW5/data/top10000words.txt', 'r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.top10000words.add(word)\n",
    "\n",
    "        with open('/home/cloudera/w261/HW5/data/ranked9001-10000words.txt','r') as f:\n",
    "            for line in f:\n",
    "                line = line.strip()\n",
    "                line = line.split(\"\\t\")\n",
    "                word = line[0].strip('\"')\n",
    "                self.vocabulary.add(word)\n",
    "        \n",
    "    def stripeMapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in self.top10000words and token1 not in self.stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in self.vocabulary and token2 not in self.stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    yield token1, stripe\n",
    "    def stripeReducer(self, key, value):\n",
    "        agg_stripe = {}\n",
    "        doc = key\n",
    "        for stripe in value:\n",
    "            ## LOOKS LIKE THE KEYS IN THE STRIPE ARE\n",
    "            ## UNICODE STRINGS WATCH OUT FOR THAT SHIT\n",
    "            ## ALSO\n",
    "            ## SEEMS LIKE THE STRIPES ARE ALREADY DICTS...\n",
    "            \n",
    "#             sys.stderr.write(\"THIS IS A RAW STRIPE DUDE: %s\\n\" %stripe)\n",
    "#             sys.stderr.write(\"THIS IS THE RAW STRIPE TYPE:\\n\")\n",
    "#             sys.stderr.write(type(stripe).__name__)\n",
    "            \n",
    "            #stripe = ast.literal_eval(stripe)\n",
    "            for key,value in stripe.iteritems():\n",
    "                if key in agg_stripe.keys():\n",
    "                    agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "                else:\n",
    "                    agg_stripe[key] = int(value)\n",
    "        #yield key, agg_stripe\n",
    "        yield doc, agg_stripe\n",
    "    \n",
    "    def invertedIndexMapper(self, key, line):\n",
    "        inverted_index = {}\n",
    "        sys.stderr.write(\"\\nTHIS IS THE SECOND MAPPER'S INPUT KEY\\n\")\n",
    "        sys.stderr.write(key)\n",
    "        sys.stderr.write(\"\\n#######################################\\n\")\n",
    "        sys.stderr.write(\"\\nTHIS IS THE SECOND MAPPER'S INPUT LINE\\n\")\n",
    "        ## we have figured out that the \"line\" input here is a dict\n",
    "        ## fascinating shit... BUT this is good cause we are getting closer\n",
    "        ## I think that the key is being sent as a string separately\n",
    "        ## LETS CHECK!!!!!\n",
    "        sys.stderr.write(str(line))\n",
    "        sys.stderr.write(\"\\n#######################################\\n\")\n",
    "        #line = line.split(\"\\t\")\n",
    "        #doc = line[0]\n",
    "        doc = key\n",
    "        #stripe = line[1]\n",
    "        #stripe = stripe\n",
    "        #stripe = ast.literal_eval(stripe)\n",
    "        stripe = line\n",
    "        length = len(stripe)\n",
    "        norm_length = sqrt(length)\n",
    "        for k in stripe.keys():\n",
    "            stripe[k] = 1\n",
    "            stripe[k] = float(stripe[k]) / float(norm_length)\n",
    "            if k not in inverted_index.keys():\n",
    "                inverted_index[k] = {doc:{\"sim\":stripe[k]}}\n",
    "            else:\n",
    "                inverted_index[k][doc][\"sim\"] = stripe[k]\n",
    "        for k2, v2 in inverted_index.iteritems():\n",
    "            v2[doc][\"length\"] = length\n",
    "            #print (\"\\t\".join([key, str(value)]) + \"\\n\")\n",
    "            yield k2, v2\n",
    "    def invertedIndexReducer(self, key, line):\n",
    "        posting_list = {}\n",
    "        doc = key\n",
    "        for stripe in line:\n",
    "        ## under the assumption that we have properly constructed\n",
    "        ## our inputs, we simply aggregate them as a single dict\n",
    "        ## in the value\n",
    "            for key,value in stripe.iteritems():\n",
    "                posting_list[key] = value\n",
    "#                 if key in agg_stripe.keys():\n",
    "#                     agg_stripe[key] = agg_stripe[key] + int(value)\n",
    "#                 else:\n",
    "#                     agg_stripe[key] = int(value)\n",
    "        #yield key, agg_stripe\n",
    "        # doc is NOT ACTUALLY A DOCUMENT\n",
    "        # WE ARE ONLY USING IT TO GROUP THE WORDS THAT WE DO(!!)\n",
    "        # CARE ABOUT... hence we can simply ignore the below \n",
    "        # peice of code\n",
    "        \n",
    "#         posting_list[\"*\"+doc] = len(posting_list)\n",
    "        yield doc, posting_list\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    createStripesJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aspiration\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746, \"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"basin\"\t{\"midst\": 1.0, \"*midst\": 1}\r\n",
      "\"battle\"\t{\"ensued\": 1.0, \"fought\": 1.0, \"*fought\": 1, \"*ensued\": 1}\r\n",
      "\"belief\"\t{\"necessity\": 1.0, \"*necessity\": 1}\r\n",
      "\"bit\"\t{\"*farther\": 1, \"farther\": 1.0}\r\n",
      "\"body\"\t{\"*projected\": 1, \"projected\": 1.0}\r\n",
      "\"bone\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746, \"aspiration\": 0.70710678118654746, \"*aspiration\": 2}\r\n",
      "\"book\"\t{\"*designed\": 1, \"designed\": 1.0}\r\n",
      "\"branch\"\t{\"*established\": 1, \"established\": 1.0}\r\n",
      "\"designed\"\t{\"book\": 1.0, \"*book\": 1}\r\n",
      "\"ensued\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"established\"\t{\"*branch\": 1, \"branch\": 1.0}\r\n",
      "\"farther\"\t{\"bit\": 1.0, \"*bit\": 1}\r\n",
      "\"fought\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"marrow\"\t{\"*aspiration\": 2, \"aspiration\": 0.70710678118654746, \"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"midst\"\t{\"basin\": 1.0, \"*basin\": 1}\r\n",
      "\"necessity\"\t{\"*belief\": 1, \"belief\": 1.0}\r\n",
      "\"projected\"\t{\"body\": 1.0, \"*body\": 1}\r\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x src/createStripesJob.py\n",
    "# !./src/createStripesJob.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes_2\n",
    "!cat output_stripes_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"marrow\"\t{\"*aspiration\": 2, \"aspiration\": 0.70710678118654746}\r\n",
      "\"bone\"\t{\"*aspiration\": 2, \"aspiration\": 0.70710678118654746}\r\n",
      "\"midst\"\t{\"basin\": 1.0, \"*basin\": 1}\r\n",
      "\"fought\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"ensued\"\t{\"battle\": 0.70710678118654746, \"*battle\": 2}\r\n",
      "\"necessity\"\t{\"*belief\": 1, \"belief\": 1.0}\r\n",
      "\"farther\"\t{\"bit\": 1.0, \"*bit\": 1}\r\n",
      "\"projected\"\t{\"body\": 1.0, \"*body\": 1}\r\n",
      "\"marrow\"\t{\"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"aspiration\"\t{\"*bone\": 2, \"bone\": 0.70710678118654746}\r\n",
      "\"designed\"\t{\"book\": 1.0, \"*book\": 1}\r\n",
      "\"established\"\t{\"*branch\": 1, \"branch\": 1.0}\r\n",
      "\"book\"\t{\"*designed\": 1, \"designed\": 1.0}\r\n",
      "\"battle\"\t{\"*ensued\": 1, \"ensued\": 1.0}\r\n",
      "\"branch\"\t{\"*established\": 1, \"established\": 1.0}\r\n",
      "\"bit\"\t{\"*farther\": 1, \"farther\": 1.0}\r\n",
      "\"battle\"\t{\"fought\": 1.0, \"*fought\": 1}\r\n",
      "\"aspiration\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746}\r\n",
      "\"bone\"\t{\"*marrow\": 2, \"marrow\": 0.70710678118654746}\r\n",
      "\"basin\"\t{\"midst\": 1.0, \"*midst\": 1}\r\n",
      "\"belief\"\t{\"necessity\": 1.0, \"*necessity\": 1}\r\n",
      "\"body\"\t{\"*projected\": 1, \"projected\": 1.0}\r\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x src/createStripesJob.py\n",
    "# !./src/createStripesJob.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes_1\n",
    "!cat output_stripes_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"aspiration\"\t{\"marrow\": 94, \"bone\": 94}\r\n",
      "\"basin\"\t{\"midst\": 205}\r\n",
      "\"battle\"\t{\"fought\": 82, \"ensued\": 95}\r\n",
      "\"belief\"\t{\"necessity\": 102}\r\n",
      "\"bit\"\t{\"farther\": 52}\r\n",
      "\"body\"\t{\"projected\": 187}\r\n",
      "\"bone\"\t{\"marrow\": 94, \"aspiration\": 94}\r\n",
      "\"book\"\t{\"designed\": 77}\r\n",
      "\"branch\"\t{\"established\": 63}\r\n",
      "\"designed\"\t{\"book\": 77}\r\n",
      "\"ensued\"\t{\"battle\": 95}\r\n",
      "\"established\"\t{\"branch\": 63}\r\n",
      "\"farther\"\t{\"bit\": 52}\r\n",
      "\"fought\"\t{\"battle\": 82}\r\n",
      "\"marrow\"\t{\"aspiration\": 94, \"bone\": 94}\r\n",
      "\"midst\"\t{\"basin\": 205}\r\n",
      "\"necessity\"\t{\"belief\": 102}\r\n",
      "\"projected\"\t{\"body\": 187}\r\n"
     ]
    }
   ],
   "source": [
    "# !chmod a+x src/createStripesJob.py\n",
    "# !./src/createStripesJob.py -r hadoop \\\n",
    "# data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt \\\n",
    "# > output_stripes\n",
    "!cat output_stripes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "basin {'midst': 205}\n",
      "midst {'basin': 205}\n",
      "battle {'ensued': 95}\n",
      "ensued {'battle': 95}\n",
      "battle {'fought': 82}\n",
      "fought {'battle': 82}\n",
      "belief {'necessity': 102}\n",
      "necessity {'belief': 102}\n",
      "bit {'farther': 52}\n",
      "farther {'bit': 52}\n",
      "body {'projected': 187}\n",
      "projected {'body': 187}\n",
      "bone {'marrow': 94, 'aspiration': 94}\n",
      "marrow {'aspiration': 94, 'bone': 94}\n",
      "aspiration {'marrow': 94, 'bone': 94}\n",
      "book {'designed': 77}\n",
      "designed {'book': 77}\n",
      "branch {'established': 63}\n",
      "established {'branch': 63}\n"
     ]
    }
   ],
   "source": [
    "# !head data/top10000words.txt\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from math import *\n",
    "\n",
    "\n",
    "stopwords = set()\n",
    "top10000words = set()\n",
    "vocabulary = set()\n",
    "    \n",
    "\n",
    "with open('data/stopwords.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        stopwords.add(line)\n",
    "\n",
    "with open('data/top10000words.txt', 'r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        word = line[0].strip('\"')\n",
    "        top10000words.add(word)\n",
    "\n",
    "with open('data/ranked9001-10000words.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        word = line[0].strip('\"')\n",
    "        vocabulary.add(word)\n",
    "\n",
    "# print vocabulary\n",
    "\n",
    "with open('data/mini-googlebooks-eng-all-5gram-20090715-0-filtered.txt','r') as f:\n",
    "    for line in f:\n",
    "        line = line.strip()\n",
    "        line = line.split(\"\\t\")\n",
    "        fiveGram = line[0]\n",
    "        count = int(line[1])\n",
    "        unigrams = fiveGram.split()\n",
    "        for token1 in unigrams:\n",
    "            if token1 in top10000words and token1 not in stopwords:\n",
    "                stripe = {}\n",
    "                for token2 in unigrams:\n",
    "                    if token1 != token2 and token2 in vocabulary and token2 not in stopwords:\n",
    "                        stripe[token2] = count\n",
    "                if stripe:\n",
    "                    print token1, stripe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.5 Evaluation of synonyms that your discovered\n",
    "In this part of the assignment you will evaluate the success of you synonym detector (developed in response to HW5.4).\n",
    "Take the top 1,000 closest/most similar/correlative pairs of words as determined by your measure in HW5.4, and use the synonyms function in the accompanying python code:\n",
    "\n",
    "nltk_synonyms.py\n",
    "\n",
    "Note: This will require installing the python nltk package:\n",
    "\n",
    "http://www.nltk.org/install.html\n",
    "\n",
    "and downloading its data with nltk.download().\n",
    "\n",
    "For each (word1,word2) pair, check to see if word1 is in the list, \n",
    "synonyms(word2), and vice-versa. If one of the two is a synonym of the other, \n",
    "then consider this pair a 'hit', and then report the precision, recall, and F1 measure  of \n",
    "your detector across your 1,000 best guesses. Report the macro averages of these measures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW5.6 (Optional)\n",
    "\n",
    "Repeat HW5 using vocabulary words ranked from 8001,-10,000;  7001,-10,000; 6001,-10,000; 5001,-10,000; 3001,-10,000; and 1001,-10,000;\n",
    "Dont forget to report you Cluster configuration.\n",
    "\n",
    "Generate the following graphs:\n",
    "-- vocabulary size (X-Axis) versus CPU time for indexing\n",
    "-- vocabulary size (X-Axis) versus number of pairs processed\n",
    "-- vocabulary size (X-Axis) versus F1 measure, Precision, Recall"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.7 (Optional)\n",
    "There is also a corpus of stopwords, that is, high-frequency words like \"the\", \"to\" and \"also\" that we sometimes want to filter out of a document before further processing. Stopwords usually have little lexical content, and their presence in a text fails to distinguish it from other texts. Python's nltk comes with a prebuilt list of stopwords (see below). Using this stopword list filter out these tokens from your analysis and rerun the experiments in 5.5 and disucuss the results of using a stopword list and without using a stopword list.\n",
    "\n",
    "> from nltk.corpus import stopwords\n",
    ">> stopwords.words('english')\n",
    "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', 'your', 'yours',\n",
    "'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', 'her', 'hers',\n",
    "'herself', 'it', 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves',\n",
    "'what', 'which', 'who', 'whom', 'this', 'that', 'these', 'those', 'am', 'is', 'are',\n",
    "'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does',\n",
    "'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until',\n",
    "'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into',\n",
    "'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down',\n",
    "'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here',\n",
    "'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more',\n",
    "'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so',\n",
    "'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', 'should', 'now']\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## HW 5.6 (Optional)\n",
    "There are many good ways to build our synonym detectors, so for optional homework, \n",
    "measure co-occurrence by (left/right/all) consecutive words only, \n",
    "or make stripes according to word co-occurrences with the accompanying \n",
    "2-, 3-, or 4-grams (note here that your output will no longer \n",
    "be interpretable as a network) inside of the 5-grams."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hw 5.7 (Optional)\n",
    "Once again, benchmark your top 10,000 associations (as in 5.5), this time for your\n",
    "results from 5.6. Has your detector improved?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
