{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 3**\n",
    "\n",
    "**Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook attempts to solve the exercises for homework assignment three.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__\n",
    "\n",
    "Two sorted lists of records of the form [key, value] can be merged into a single, sorted list by:\n",
    "\n",
    "- iterating through the larger of the two sorted lists.\n",
    "    - comparing the first (assumed smallest) element of the smaller list to the first (assumed smallest) element of the larger list\n",
    "    - popping the smallest of the elements from the above comparison and placing it at the bottom of a new list to hold the single merged list\n",
    "    - the above is repeated until one of the two lists is empty\n",
    "    - finally, the remainding list is appended to the bottom of the single merged list (because itself is sorted)\n",
    "\n",
    "This sorting technique is used in the shuffle phase of Hadoop MapReduce: the shuffle occurs after the mappers have output the intermediate key value records, but before these intermediate records are passed to the reducers. Specifically, after the mapper tasks have been executed, the intermediate records are written to memory (and later to disk if there is enough data), the records are then partitioned based on their keys, and then mergesorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is  a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__\n",
    "\n",
    "In the context of Hadoop, a combiner function is a function that aggregates intermediate results from mapper tasks before these are fed to reducer tasks downstream in the MapReduce framework of Hadoop. The Hadoop MapReduce framework reserves the right to use combiners (or not) at its discretion regardless of the instructions presented by the programmer to the framework. As such, it is critical that the combiner functions not only ingest intermediate records in the same format as those fed to the reducers downstream, but they must also produce aggregated intermediate records that are identical in format to the un-aggregated records output by the mapper tasks upstream. As a sort of 'mini reducers', combiners are meant to accomplish the important tasks of (1) minimizing the number of key-value pairs that are  shuffled across the network from the mappers to the reducers and (2) reducing the risk of reducer tasks 'lagging' due to aggregating across keys that have a very large list of values.\n",
    "\n",
    "An example of when combiners can (and should) be used is the simple word count exercise! Combiners should be used in word count to achieve tasks (1) and (2) mentioned above. Particularly, the frequency of words varies widely in a corpus; in an English corpus, the token \"the\" is likely to be encountered a very large number of times and considerably more times than many other tokens in the corpus. As such the reducer assigned to aggregate the counts for the token will likely lag behind other reducers processing rarer tokens; furthermore, there will be a very large number of \"the\\t1\" intermediate records that will be shuffled across the network. Combiner tasks will critically reduce the amount of work that the reducers assigned to process common tokens (such as \"the\") will need to do while also minimizing the amount of data that is shuffled across the network (e.g. shuffling \"the\\tN\" intermediate records as opposed to \"the\\t1\").\n",
    "\n",
    "Finally, it is important to note that the word count example (and any larger task that uses combiners for that matter) can be resolved (abeit consuming more resources) without the use of combiners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "The Hadoop MapReduce shuffle is a critical step in a Hadoop MapReduce job; on a high level, it is a step that transfers intermediate records that are output in the map phase of a job by mappers to reducers such that they may be used as inputs to the reduce phase. During the shuffle the intermediate key value pairs are merge sorted and passed through the network from the mapper nodes to the reducer nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.1 Consumer complaints dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use Counters to do EDA (exploratory data analysis and to monitor progress)__\n",
    "\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "|Complaint ID|Product|Sub-product|Issue|Sub-issue|State|ZIP code|Submitted via|Date received|Date sent to company|Company|Company response|Timely response?|Consumer disputed?|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|1114245|Debt collection|Medical|Disclosure verification of debt|Not given enough info to verify debt|FL|32219|Web|11/13/2014|11/13/2014|\"Choice Recovery, Inc.\"|Closed with explanation|Yes|\n",
    "|1114488|Debt collection|Medical|Disclosure verification of debt|Right to dispute notice not received|TX|75006|Web|11/13/2014|11/13/2014|\"Expert Global Solutions, Inc.\"|In progress|Yes|\n",
    "|1114255|Bank account or service|Checking account|Deposits and withdrawals| |NY|11102|Web|11/13/2014|11/13/2014|\"FNIS (Fidelity National Information Services, Inc.)\"|In progress|Yes|\n",
    "|1115106|Debt collection|\"Other (phone, health club, etc.)\"|Communication tactics|Frequent or repeated calls|GA|31721|Web|11/13/2014|11/13/2014|\"Expert Global Solutions, Inc.\"|In progress|Yes|\n",
    "\n",
    "__User-defined Counters__\n",
    "\n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.\n",
    "\n",
    "__ANSWER:__\n",
    "\n",
    "Using Hadoop Counters, we identified that the number of complaints pertaining to __debt collection is 44372__, the number of complaints pertaining to __mortgage is 125752__, and the number of complaints pertaining to __other issues is 142789__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing counterMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counterMapper.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for counterMapper.py for W261 HW3.0\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def map_function(record):\n",
    "    record = record.strip()\n",
    "    record_parameters = record.split(\",\")\n",
    "    product = record_parameters[1]\n",
    "    return (product, 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for line in sys.stdin:\n",
    "        product_type, count = map_function(line)\n",
    "        print \"%s\\t%d\" % (product_type, int(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x counterMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting counterReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counterReducer.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for counterReducer.py for W261 HW3.0\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def reduce_function(record):\n",
    "    record = record.strip()\n",
    "    record_parameters = record.split(\"\\t\")\n",
    "    product_type = record_parameters[0]\n",
    "    count = int(record_parameters[1])\n",
    "    if product_type == \"Mortgage\":\n",
    "        sys.stderr.write(\"reporter:counter:Product,Mortgage,1\\n\")\n",
    "    elif product_type == \"Debt collection\":\n",
    "        sys.stderr.write(\"reporter:counter:Product,Debt collection,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Product,Other,1\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    for line in sys.stdin:\n",
    "        reduce_function(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x counterReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/HW3/data/* \\\n",
    "/user/cloudera/w261/HW3/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/01 10:33:50 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_0' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_0\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2418945648414801191.jar tmpDir=null\n",
      "16/06/01 10:33:54 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/01 10:33:55 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/01 10:33:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/01 10:33:55 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/01 10:33:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/06/01 10:33:56 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/01 10:33:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0039\n",
      "16/06/01 10:33:56 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0039\n",
      "16/06/01 10:33:56 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0039/\n",
      "16/06/01 10:33:56 INFO mapreduce.Job: Running job: job_1464465911163_0039\n",
      "16/06/01 10:34:06 INFO mapreduce.Job: Job job_1464465911163_0039 running in uber mode : false\n",
      "16/06/01 10:34:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/01 10:34:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/01 10:34:39 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/06/01 10:34:43 INFO mapreduce.Job:  map 100% reduce 34%\n",
      "16/06/01 10:34:45 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "16/06/01 10:34:46 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/01 10:35:56 INFO mapreduce.Job:  map 100% reduce 61%\n",
      "16/06/01 10:35:59 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/06/01 10:36:02 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/06/01 10:36:06 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/06/01 10:36:16 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/06/01 11:15:18 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/06/01 11:15:25 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/01 11:15:25 INFO mapreduce.Job: Job job_1464465911163_0039 completed successfully\n",
      "16/06/01 11:15:26 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=261990\n",
      "\t\tFILE: Number of bytes written=1120696\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50906621\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1139968\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=628716032\n",
      "\t\tTotal time spent by all map tasks (ms)=8906\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4911844\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8906\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4911844\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1139968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=628716032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=261974\n",
      "\t\tInput split bytes=135\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=261974\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=3160\n",
      "\t\tCPU time spent (ms)=25340\n",
      "\t\tPhysical memory (bytes) snapshot=550035456\n",
      "\t\tVirtual memory (bytes) snapshot=3520311296\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tProduct\n",
      "\t\tDebt collection=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/06/01 11:15:26 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_0\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/counterMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/counterReducer.py \\\n",
    "-input /user/cloudera/w261/HW3/data/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this brief study the Input file will be one record (the next line only):\n",
    "\n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperCounter3_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperCounter3_2.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for mapper counter for W261 HW3.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def map_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Calls,1\\n\")\n",
    "\n",
    "def map_function(record):\n",
    "    record = record.strip()\n",
    "    terms = record.split(\" \")\n",
    "    for term in terms:\n",
    "        print \"%s\\t1\" % term\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_initialize()\n",
    "    for line in sys.stdin:\n",
    "        intermediate_input = map_function(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperCounter3_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerCounter3_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerCounter3_2.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for reducer counter for W261 HW3.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def reduce_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def reduce_function(record,current_word,current_count):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "    return (current_word, current_count)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    word = reduce_initialize()\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, current_count = reduce_function(line, word, current_count)\n",
    "    print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerCounter3_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo foo quux labs foo bar quux\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/document3_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper,Calls,1\r\n",
      "reporter:counter:Reducer,Calls,1\r\n",
      "reporter:counter:Reducer,Calls,1\r\n",
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/document3_2.txt | ./mapperCounter3_2.py | sort -k1,1 | ./reducerCounter3_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/HW3/data/document3_2.txt \\\n",
    "/user/cloudera/w261/HW3/data3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/cloudera/w261/HW3/data3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/cloudera/w261/HW3/output-3_2': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5284007959467736255.jar tmpDir=null\n",
      "16/06/02 19:55:42 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 19:55:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 19:55:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 19:55:44 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 19:55:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0043\n",
      "16/06/02 19:55:45 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0043\n",
      "16/06/02 19:55:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0043/\n",
      "16/06/02 19:55:45 INFO mapreduce.Job: Running job: job_1464465911163_0043\n",
      "16/06/02 19:55:55 INFO mapreduce.Job: Job job_1464465911163_0043 running in uber mode : false\n",
      "16/06/02 19:55:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 19:56:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 19:56:24 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 19:56:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 19:56:40 INFO mapreduce.Job: Job job_1464465911163_0043 completed successfully\n",
      "16/06/02 19:56:41 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=127\n",
      "\t\tFILE: Number of bytes written=597005\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=161\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=990080\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7910912\n",
      "\t\tTotal time spent by all map tasks (ms)=7735\n",
      "\t\tTotal time spent by all reduce tasks (ms)=61804\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7735\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=61804\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=990080\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7910912\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=111\n",
      "\t\tInput split bytes=130\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=111\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=444\n",
      "\t\tCPU time spent (ms)=5810\n",
      "\t\tPhysical memory (bytes) snapshot=637849600\n",
      "\t\tVirtual memory (bytes) snapshot=3520536576\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tMapper\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/06/02 19:56:41 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapperCounter3_2.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducerCounter3_2.py \\\n",
    "-input /user/cloudera/w261/HW3/data3_2/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quux\t2\r\n",
      "foo\t3\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3_2_b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3_2_b.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for mapper for W261 HW3.0.b\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def map_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Calls,1\\n\")\n",
    "\n",
    "def map_function(record):\n",
    "    record = record.strip()\n",
    "    record_parameters = record.split(\",\")\n",
    "    issue = record_parameters[3]\n",
    "    issue_words = issue.split()\n",
    "    for word in issue_words:\n",
    "        count = 1\n",
    "        print \"%s\\t%d\" % (word, int(count))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_initialize()\n",
    "    for line in sys.stdin:\n",
    "        map_function(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3_2_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat ./mapper3_2_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer3_2_b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3_2_b.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for reducer for W261 HW3.2.b\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def reduce_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def reduce_function(record,current_word,current_count):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "    return (current_word, current_count)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #reduce_initialize()\n",
    "    word = reduce_initialize()\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, current_count = reduce_function(line, word, current_count)\n",
    "    print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer3_2_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/cloudera/w261/HW3/output-3_2_b': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8582929186455445908.jar tmpDir=null\n",
      "16/06/02 20:02:46 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:02:46 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:02:47 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 20:02:47 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 20:02:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0044\n",
      "16/06/02 20:02:48 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0044\n",
      "16/06/02 20:02:48 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0044/\n",
      "16/06/02 20:02:48 INFO mapreduce.Job: Running job: job_1464465911163_0044\n",
      "16/06/02 20:02:58 INFO mapreduce.Job: Job job_1464465911163_0044 running in uber mode : false\n",
      "16/06/02 20:02:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 20:03:12 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/06/02 20:03:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 20:03:31 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/06/02 20:03:32 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 20:03:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 20:03:54 INFO mapreduce.Job: Job job_1464465911163_0044 completed successfully\n",
      "16/06/02 20:03:54 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1231531\n",
      "\t\tFILE: Number of bytes written=2223549\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50906621\n",
      "\t\tHDFS: Number of bytes written=2295\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1608960\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9226112\n",
      "\t\tTotal time spent by all map tasks (ms)=12570\n",
      "\t\tTotal time spent by all reduce tasks (ms)=72079\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12570\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=72079\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1608960\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9226112\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=9428893\n",
      "\t\tMap output materialized bytes=541620\n",
      "\t\tInput split bytes=135\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=541620\n",
      "\t\tReduce input records=978634\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=2935902\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=338\n",
      "\t\tCPU time spent (ms)=11350\n",
      "\t\tPhysical memory (bytes) snapshot=638361600\n",
      "\t\tVirtual memory (bytes) snapshot=3522072576\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tMapper\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2295\n",
      "16/06/02 20:03:54 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_b\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_b\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapper3_2_b.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_b.py \\\n",
    "-input /user/cloudera/w261/HW3/data/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   1 cloudera cloudera          0 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/_SUCCESS\n",
      "-rw-r--r--   1 cloudera cloudera        491 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00000\n",
      "-rw-r--r--   1 cloudera cloudera        661 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00001\n",
      "-rw-r--r--   1 cloudera cloudera        548 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00002\n",
      "-rw-r--r--   1 cloudera cloudera        595 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00003\n",
      "\"Account\t16205\n",
      "Account\t350\n",
      "Applied\t139\n",
      "Can't\t1999\n",
      "Cash\t240\n",
      "Cont'd\t11848\n",
      "Debt\t1343\n",
      "Delinquent\t1061\n",
      "I\t925\n",
      "Incorrect\t29069\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "\"Account\t16205\n",
      "Account\t350\n",
      "Applied\t139\n",
      "Can't\t1999\n",
      "Cash\t240\n",
      "Cont'd\t11848\n",
      "Debt\t1343\n",
      "Delinquent\t1061\n",
      "I\t925\n",
      "Incorrect\t29069\n",
      "\"Loan\t107254\n",
      "\"Making/receiving\t3226\n",
      "ATM\t2422\n",
      "Communication\t6920\n",
      "Dealing\t1944\n",
      "Improper\t4309\n",
      "Payment\t92\n",
      "Problems\t9484\n",
      "Shopping\t672\n",
      "Taking\t1242\n",
      "/\t12386\n",
      "APR\t3431\n",
      "Arbitration\t168\n",
      "Bankruptcy\t222\n",
      "Billing\t8158\n",
      "Convenience\t75\n",
      "Credit\t14768\n",
      "Deposits\t10555\n",
      "Disclosure\t5214\n",
      "False\t2508\n",
      "\"Application\t8625\n",
      "Advertising\t1193\n",
      "Application\t243\n",
      "Balance\t597\n",
      "Charged\t878\n",
      "Closing/Cancelling\t2795\n",
      "Collection\t1907\n",
      "Customer\t2734\n",
      "Embezzlement\t3276\n",
      "Forbearance\t350\n",
      "low\t5663\n",
      "money\t139\n",
      "opening\t16205\n",
      "pay\t3821\n",
      "practices\t1003\n",
      "report/credit\t4357\n",
      "sharing\t2832\n",
      "stop\t131\n",
      "when\t4095\n",
      "with\t1944\n",
      "repay\t1647\n",
      "service\t1518\n",
      "the\t6248\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t3821\n",
      "verification\t5214\n",
      "was\t274\n",
      "wrong\t71\n",
      "your\t3844\n",
      "plans\t350\n",
      "rate\t3431\n",
      "report\t30546\n",
      "reporting\t6559\n",
      "scam\t566\n",
      "score\t4357\n",
      "servicing\t36767\n",
      "statements\t2508\n",
      "transaction\t387\n",
      "withdrawals\t10555\n",
      "received\t98\n",
      "relations\t1367\n",
      "representation\t2508\n",
      "servicer\t1944\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "theft\t3276\n",
      "use\t1477\n",
      "you\t3821\n",
      "low\t5663\n",
      "money\t139\n",
      "opening\t16205\n",
      "pay\t3821\n",
      "practices\t1003\n",
      "report/credit\t4357\n",
      "sharing\t2832\n",
      "stop\t131\n",
      "when\t4095\n",
      "with\t1944\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/cloudera/w261/HW3/output-3_2_b\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_b/* | head\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_b/* | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__\n",
    "\n",
    "*Note that I am using the mapper and reducer code from 3.2.b as it does not change for this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner3_2_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner3_2_c.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "This is the combiner code for HW3.2.c\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def combine_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Combiner,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def combine_function(record,current_word,current_count):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "    return (current_word, current_count)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    word = combine_initialize()\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, current_count = combine_function(line, word, current_count)\n",
    "    print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner3_2_c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/02 20:35:38 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_2_c' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_2_c\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1137443623450988204.jar tmpDir=null\n",
      "16/06/02 20:35:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:35:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:35:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 20:35:44 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 20:35:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0046\n",
      "16/06/02 20:35:45 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0046\n",
      "16/06/02 20:35:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0046/\n",
      "16/06/02 20:35:45 INFO mapreduce.Job: Running job: job_1464465911163_0046\n",
      "16/06/02 20:35:53 INFO mapreduce.Job: Job job_1464465911163_0046 running in uber mode : false\n",
      "16/06/02 20:35:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 20:36:07 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/02 20:36:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 20:36:24 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/06/02 20:36:25 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 20:36:38 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/06/02 20:36:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 20:36:39 INFO mapreduce.Job: Job job_1464465911163_0046 completed successfully\n",
      "16/06/02 20:36:39 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15720\n",
      "\t\tFILE: Number of bytes written=610617\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50906621\n",
      "\t\tHDFS: Number of bytes written=2295\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1651200\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6374272\n",
      "\t\tTotal time spent by all map tasks (ms)=12900\n",
      "\t\tTotal time spent by all reduce tasks (ms)=49799\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12900\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=49799\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1651200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6374272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=9428893\n",
      "\t\tMap output materialized bytes=3661\n",
      "\t\tInput split bytes=135\n",
      "\t\tCombine input records=978634\n",
      "\t\tCombine output records=334\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=3661\n",
      "\t\tReduce input records=334\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=1002\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=278\n",
      "\t\tCPU time spent (ms)=6970\n",
      "\t\tPhysical memory (bytes) snapshot=632274944\n",
      "\t\tVirtual memory (bytes) snapshot=3523854336\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tCombiner\n",
      "\t\tCalls=8\n",
      "\tMapper\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2295\n",
      "16/06/02 20:36:39 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_c\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_c\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapper3_2_b.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_b.py \\\n",
    "-combiner /home/cloudera/w261/HW3/src/combiner3_2_c.py \\\n",
    "-input /user/cloudera/w261/HW3/data/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using a single reducer:__ What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).\n",
    "\n",
    "__ANSWER:__\n",
    "\n",
    "_Note: I am taking as input to this exercise the results from the word count analysis in the previous exercise._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting identityMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile identityMapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#need to specify an identity mapper in order to trigger the sort in Hadoop\n",
    "#\n",
    "#Can also use the identtiy mapper:  -mapper /bin/cat \\  \n",
    "#\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # minimal feature engineering: setting tokens to all be lower case\n",
    "    # for the purposes of having case insensitive secondary sort\n",
    "    line = line.lower()\n",
    "    print line.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x identityMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3_2_d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3_2_d.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for reducer for W261 HW3.2.b\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "def reduce_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def reduce_function(record, current_total_terms):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    current_total_terms = current_total_terms + count\n",
    "    return (word, count, current_total_terms)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    output_array = []\n",
    "    word = reduce_initialize()\n",
    "    total_terms = 0\n",
    "    for line in sys.stdin:\n",
    "        word, word_count, total_terms = reduce_function(line, total_terms)\n",
    "        output_array.extend([{\"word\":word,\"count\":word_count}])\n",
    "\n",
    "    print \"\\n########################################################\"\n",
    "    print \"### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "    print \"########################################################\\n\"\n",
    "    for i in range(0,50):\n",
    "        print \"%s\\t%d\\t%.10f\" % (output_array[i][\"word\"], output_array[i][\"count\"], float(output_array[i][\"count\"]/float(total_terms)))\n",
    "\n",
    "    print \"\\n###########################################################\"\n",
    "    print \"### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "    print \"###########################################################\\n\"\n",
    "    for i in range(1,11):\n",
    "        i = -1*i\n",
    "        print \"%s\\t%d\\t%.10f\" % (output_array[i][\"word\"], output_array[i][\"count\"], float(output_array[i][\"count\"]/float(total_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer3_2_d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer,Calls,1\n",
      "\n",
      "########################################################\n",
      "### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\n",
      "########################################################\n",
      "\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "on\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "incorrect\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n",
      "and\t16448\t0.0168071005\n",
      "opening\t16205\t0.0165587952\n",
      "\"account\t16205\t0.0165587952\n",
      "credit\t14768\t0.0150904220\n",
      "health\t12545\t0.0128188884\n",
      "club\t12545\t0.0128188884\n",
      "/\t12386\t0.0126564170\n",
      "not\t12353\t0.0126226965\n",
      "loan\t12237\t0.0125041640\n",
      "owed\t11848\t0.0121066711\n",
      "cont'd\t11848\t0.0121066711\n",
      "collect\t11848\t0.0121066711\n",
      "attempts\t11848\t0.0121066711\n",
      "of\t10885\t0.0111226465\n",
      "my\t10731\t0.0109652843\n",
      "withdrawals\t10555\t0.0107854417\n",
      "deposits\t10555\t0.0107854417\n",
      "problems\t9484\t0.0096910592\n",
      "\"application\t8625\t0.0088133051\n",
      "to\t8401\t0.0085844146\n",
      "billing\t8158\t0.0083361093\n",
      "other\t7886\t0.0080581709\n",
      "disputes\t6938\t0.0070894737\n",
      "tactics\t6920\t0.0070710807\n",
      "communication\t6920\t0.0070710807\n",
      "reporting\t6559\t0.0067021992\n",
      "lease\t6337\t0.0064753524\n",
      "the\t6248\t0.0063844093\n",
      "low\t5663\t0.0057866373\n",
      "funds\t5663\t0.0057866373\n",
      "caused\t5663\t0.0057866373\n",
      "by\t5663\t0.0057866373\n",
      "being\t5663\t0.0057866373\n",
      "process\t5505\t0.0056251878\n",
      "verification\t5214\t0.0053278345\n",
      "disclosure\t5214\t0.0053278345\n",
      "managing\t5006\t0.0051152934\n",
      "investigation\t4858\t0.0049640622\n",
      "company's\t4858\t0.0049640622\n",
      "card\t4405\t0.0045011720\n",
      "\n",
      "###########################################################\n",
      "### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\n",
      "###########################################################\n",
      "\n",
      "issue\t1\t0.0000010218\n",
      "disclosures\t64\t0.0000653973\n",
      "incorrect/missing\t64\t0.0000653973\n",
      "amt\t71\t0.0000725501\n",
      "day\t71\t0.0000725501\n",
      "wrong\t71\t0.0000725501\n",
      "checks\t75\t0.0000766374\n",
      "convenience\t75\t0.0000766374\n",
      "credited\t92\t0.0000940086\n",
      "payment\t92\t0.0000940086\n"
     ]
    }
   ],
   "source": [
    "# Testing code for HW3.2.d\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./identityMapper.py \\\n",
    "| sort -k2,2nr -k1,1r | ./reducer3_2_d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 06:45:41 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_2_d' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_2_d1465134341181\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4013988832558737341.jar tmpDir=null\n",
      "16/06/05 06:45:44 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 06:45:44 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 06:45:45 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/06/05 06:45:45 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/06/05 06:45:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0010\n",
      "16/06/05 06:45:46 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0010\n",
      "16/06/05 06:45:46 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0010/\n",
      "16/06/05 06:45:46 INFO mapreduce.Job: Running job: job_1465099569246_0010\n",
      "16/06/05 06:45:55 INFO mapreduce.Job: Job job_1465099569246_0010 running in uber mode : false\n",
      "16/06/05 06:45:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 06:46:09 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/06/05 06:46:10 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/05 06:46:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 06:46:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 06:46:34 INFO mapreduce.Job: Job job_1465099569246_0010 completed successfully\n",
      "16/06/05 06:46:34 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2218\n",
      "\t\tFILE: Number of bytes written=605083\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2815\n",
      "\t\tHDFS: Number of bytes written=1907\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6652544\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=878208\n",
      "\t\tTotal time spent by all map tasks (ms)=51973\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6861\n",
      "\t\tTotal vcore-seconds taken by all map tasks=51973\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6861\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6652544\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=878208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=182\n",
      "\t\tMap output records=182\n",
      "\t\tMap output bytes=2477\n",
      "\t\tMap output materialized bytes=2550\n",
      "\t\tInput split bytes=520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=2550\n",
      "\t\tReduce input records=182\n",
      "\t\tReduce output records=70\n",
      "\t\tSpilled Records=364\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=305\n",
      "\t\tCPU time spent (ms)=3000\n",
      "\t\tPhysical memory (bytes) snapshot=664748032\n",
      "\t\tVirtual memory (bytes) snapshot=3491516416\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tReducer\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2295\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1907\n",
      "16/06/05 06:46:34 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_d\n",
      "\t\n",
      "########################################################\t\n",
      "### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\t\n",
      "########################################################\t\n",
      "\t\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "incorrect\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "on\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n",
      "and\t16448\t0.0168071005\n",
      "\"account\t16205\t0.0165587952\n",
      "opening\t16205\t0.0165587952\n",
      "credit\t14768\t0.0150904220\n",
      "club\t12545\t0.0128188884\n",
      "health\t12545\t0.0128188884\n",
      "/\t12386\t0.0126564170\n",
      "not\t12353\t0.0126226965\n",
      "loan\t12237\t0.0125041640\n",
      "attempts\t11848\t0.0121066711\n",
      "collect\t11848\t0.0121066711\n",
      "cont'd\t11848\t0.0121066711\n",
      "owed\t11848\t0.0121066711\n",
      "of\t10885\t0.0111226465\n",
      "my\t10731\t0.0109652843\n",
      "deposits\t10555\t0.0107854417\n",
      "withdrawals\t10555\t0.0107854417\n",
      "problems\t9484\t0.0096910592\n",
      "\"application\t8625\t0.0088133051\n",
      "to\t8401\t0.0085844146\n",
      "billing\t8158\t0.0083361093\n",
      "other\t7886\t0.0080581709\n",
      "disputes\t6938\t0.0070894737\n",
      "communication\t6920\t0.0070710807\n",
      "tactics\t6920\t0.0070710807\n",
      "reporting\t6559\t0.0067021992\n",
      "lease\t6337\t0.0064753524\n",
      "the\t6248\t0.0063844093\n",
      "being\t5663\t0.0057866373\n",
      "by\t5663\t0.0057866373\n",
      "caused\t5663\t0.0057866373\n",
      "funds\t5663\t0.0057866373\n",
      "low\t5663\t0.0057866373\n",
      "process\t5505\t0.0056251878\n",
      "disclosure\t5214\t0.0053278345\n",
      "verification\t5214\t0.0053278345\n",
      "managing\t5006\t0.0051152934\n",
      "company's\t4858\t0.0049640622\n",
      "investigation\t4858\t0.0049640622\n",
      "card\t4405\t0.0045011720\n",
      "\t\n",
      "###########################################################\t\n",
      "### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\t\n",
      "###########################################################\t\n",
      "\t\n",
      "issue\t1\t0.0000010218\n",
      "incorrect/missing\t64\t0.0000653973\n",
      "disclosures\t64\t0.0000653973\n",
      "wrong\t71\t0.0000725501\n",
      "day\t71\t0.0000725501\n",
      "amt\t71\t0.0000725501\n",
      "convenience\t75\t0.0000766374\n",
      "checks\t75\t0.0000766374\n",
      "payment\t92\t0.0000940086\n",
      "credited\t92\t0.0000940086\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_d\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-numReduceTasks 1 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/identityMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_d.py \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input /user/cloudera/w261/HW3/output-3_2_c/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_d\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_d/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.2.1 Using two reducers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using two reducers: What are the top 50 most frequent terms in your word count analysis?\n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_NOTE: I used a combiner in the previous exercise to produce the word count analysis that serves as the input for the sorting MapReduce job._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperWithPartitionTable.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "total_term_count = 0\n",
    "values = []\n",
    "# using the word count output to determine the partitions\n",
    "f = open('word_count_output','r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    values.extend([int(value)])\n",
    "\n",
    "values = sorted(values)\n",
    "median = int(values[(len(values) / 2)])\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    # minimal feature engineering: setting tokens to all be lower case\n",
    "    # for the purposes of having case insensitive secondary sort\n",
    "    line = line.lower()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    value = int(value)\n",
    "    if value <= median:\n",
    "        print \"group1\\t%s\" % (line)\n",
    "    else:\n",
    "        print \"group2\\t%s\" % (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperWithPartitionTable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group2\t\"account\t16205\r\n",
      "group1\taccount\t350\r\n",
      "group1\tapplied\t139\r\n",
      "group1\tcan't\t1999\r\n",
      "group1\tcash\t240\r\n",
      "group2\tcont'd\t11848\r\n",
      "group1\tdebt\t1343\r\n",
      "group1\tdelinquent\t1061\r\n",
      "group1\ti\t925\r\n",
      "group2\tincorrect\t29069\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./mapperWithPartitionTable.py | head\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combineWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combineWithPartitionTable.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "This is the combiner code for HW3.2.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Combiner,Calls,1\\n\")\n",
    "    output_array = []\n",
    "    cur_count = 0\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        print line"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combineWithPartitionTable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Combiner,Calls,1\n",
      "group2\t\"loan\t107254\n",
      "group2\tmodification\t70487\n",
      "group2\tservicing\t36767\n",
      "group2\tcredit\t36126\n",
      "group2\treport\t30546\n",
      "group2\tincorrect\t29069\n",
      "group2\tinformation\t29069\n",
      "group2\ton\t29069\n",
      "group2\tor\t22533\n",
      "group2\tdebt\t17966\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./mapperWithPartitionTable.py \\\n",
    "| ./combineWithPartitionTable.py | sort -k3,3nr -k2,2 | head\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerWithPartitionKey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerWithPartitionKey.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "total_tokens = 0\n",
    "output_array = []\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# using the word count output to determine the total tokens\n",
    "# for relative frequency\n",
    "f = open('word_count_output','r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    total_tokens += int(value)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    group, key, value = line.split(\"\\t\") #one minor modification to process the parition key. I.e., drop it\n",
    "    value = int(value)\n",
    "    print \"%s\\t%d\\t%.10f\" %(key,value,(float(value)/float(total_tokens)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerWithPartitionKey.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\n",
      "reporter:counter:Combiner,Calls,1\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "incorrect\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "on\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./mapperWithPartitionTable.py \\\n",
    "| ./combineWithPartitionTable.py \\\n",
    "| sort -k3,3nr -k2,2 | ./reducerWithPartitionKey.py | head\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 18:46:11 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_2_2' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_2_21465177571073\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8778152038763889165.jar tmpDir=null\n",
      "16/06/05 18:46:14 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 18:46:15 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 18:46:16 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/06/05 18:46:16 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/06/05 18:46:16 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0042\n",
      "16/06/05 18:46:17 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0042\n",
      "16/06/05 18:46:17 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0042/\n",
      "16/06/05 18:46:17 INFO mapreduce.Job: Running job: job_1465099569246_0042\n",
      "16/06/05 18:46:28 INFO mapreduce.Job: Job job_1465099569246_0042 running in uber mode : false\n",
      "16/06/05 18:46:28 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 18:46:44 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/05 18:46:58 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 18:47:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 18:47:14 INFO mapreduce.Job: Job job_1465099569246_0042 completed successfully\n",
      "16/06/05 18:47:14 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2440\n",
      "\t\tFILE: Number of bytes written=734351\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2815\n",
      "\t\tHDFS: Number of bytes written=4661\n",
      "\t\tHDFS: Number of read operations=18\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6531072\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3399296\n",
      "\t\tTotal time spent by all map tasks (ms)=51024\n",
      "\t\tTotal time spent by all reduce tasks (ms)=26557\n",
      "\t\tTotal vcore-seconds taken by all map tasks=51024\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=26557\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6531072\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3399296\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=182\n",
      "\t\tMap output records=182\n",
      "\t\tMap output bytes=3751\n",
      "\t\tMap output materialized bytes=2985\n",
      "\t\tInput split bytes=520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=2985\n",
      "\t\tReduce input records=182\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=364\n",
      "\t\tShuffled Maps =8\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=8\n",
      "\t\tGC time elapsed (ms)=347\n",
      "\t\tCPU time spent (ms)=3690\n",
      "\t\tPhysical memory (bytes) snapshot=792379392\n",
      "\t\tVirtual memory (bytes) snapshot=4197769216\n",
      "\t\tTotal committed heap usage (bytes)=304349184\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2295\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=4661\n",
      "16/06/05 18:47:14 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_2\n",
      "\\n########################################################\n",
      "### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\n",
      "########################################################\\n\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "incorrect\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "on\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n",
      "\\n###########################################################\n",
      "### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\n",
      "###########################################################\\n\n",
      "credited\t92\t0.0000940086\n",
      "payment\t92\t0.0000940086\n",
      "checks\t75\t0.0000766374\n",
      "convenience\t75\t0.0000766374\n",
      "amt\t71\t0.0000725501\n",
      "day\t71\t0.0000725501\n",
      "wrong\t71\t0.0000725501\n",
      "disclosures\t64\t0.0000653973\n",
      "incorrect/missing\t64\t0.0000653973\n",
      "issue\t1\t0.0000010218\n",
      "16/06/05 18:47:28 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_2_2' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_2_21465177648005\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_2\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-files word_count_output \\\n",
    "-input /user/cloudera/w261/HW3/output-3_2_c/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_2 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapperWithPartitionTable.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducerWithPartitionKey.py \\\n",
    "-numReduceTasks 2 \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "!echo \"########################################################\"\n",
    "!echo \"### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "!echo \"########################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_2/part-00001 | head -n 50\n",
    "!echo \"###########################################################\"\n",
    "!echo \"### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "!echo \"###########################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_2/part-00000 | tail\n",
    "!rm word_count_output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.3 Shopping Cart Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Product Recommendations__: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the Product Purchase Data \n",
    "\n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222\n",
    "\n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192\n",
    "\n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643\n",
    "\n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465\n",
    "\n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "__Using a single reducer__: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "I found that the number of unique products in the Product Purchase Data is __12592__. The top 50 most frequently purchased items,  their frequency,  and their relative frequency are output below after\n",
    "\n",
    "########################################################\n",
    "\n",
    " TOP 50 PRODUCTS BY FREQUENCY WITH RELATIVE FREQUENCY \n",
    " \n",
    "########################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-06-05 18:48:38--  https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
      "Resolving www.dropbox.com... 162.125.4.1\n",
      "Connecting to www.dropbox.com|162.125.4.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/Vr2nRoxO62LRVcOMfkP2tIAvkhtSaLYtVA5oNlOhPOe92q2E5LhBbGJXBElhA1NF/file [following]\n",
      "--2016-06-05 18:48:39--  https://dl.dropboxusercontent.com/content_link/Vr2nRoxO62LRVcOMfkP2tIAvkhtSaLYtVA5oNlOhPOe92q2E5LhBbGJXBElhA1NF/file\n",
      "Resolving dl.dropboxusercontent.com... 108.160.173.165\n",
      "Connecting to dl.dropboxusercontent.com|108.160.173.165|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 3458517 (3.3M) [text/plain]\n",
      "Saving to: â€œProductPurchaseData.txt?dl=0â€\n",
      "\n",
      "100%[======================================>] 3,458,517   2.09M/s   in 1.6s    \n",
      "\n",
      "2016-06-05 18:48:47 (2.09 MB/s) - â€œProductPurchaseData.txt?dl=0â€ saved [3458517/3458517]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\"\n",
    "!mv \"ProductPurchaseData.txt?dl=0\" ~/w261/HW3/data/ProductPurchaseData.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing productPurchaseDataMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile productPurchaseDataMapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split()\n",
    "    for product in products:\n",
    "        print \"%s\\t1\" % product"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x productPurchaseDataMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DAI22177\t1\r\n",
      "DAI62779\t1\r\n",
      "DAI92600\t1\r\n",
      "DAI45339\t1\r\n",
      "SNA59903\t1\r\n",
      "DAI62779\t1\r\n",
      "DAI92600\t1\r\n",
      "DAI42083\t1\r\n",
      "GRO59710\t1\r\n",
      "SNA63881\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/ProductPurchaseData.txt | ./productPurchaseDataMapper.py | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Combiner,Calls,1\n",
      "reporter:counter:Reducer,Calls,1\n",
      "SNA99791\t2\n",
      "SNA99814\t1\n",
      "SNA99861\t3\n",
      "SNA99870\t19\n",
      "SNA99873\t2083\n",
      "SNA99886\t44\n",
      "SNA99895\t2\n",
      "SNA99918\t3\n",
      "SNA99924\t16\n",
      "SNA99941\t1\n"
     ]
    }
   ],
   "source": [
    "# we can recycle the reducer from HW3.2.b and\n",
    "# the combiner from HW3.2.c to compute the\n",
    "# number of unique products\n",
    "!cat /home/cloudera/w261/HW3/data/ProductPurchaseData.txt \\\n",
    "| ./productPurchaseDataMapper.py | sort -k1,1 \\\n",
    "| ./combiner3_2_c.py \\\n",
    "| ./reducer3_2_b.py | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/cloudera/w261/HW3/data_3_3\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/HW3/data/ProductPurchaseData.txt /user/cloudera/w261/HW3/data_3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 19:48:49 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_3_0' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_3_01465181329506\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2793694761706164184.jar tmpDir=null\n",
      "16/06/05 19:48:53 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 19:48:53 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 19:48:54 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 19:48:54 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/05 19:48:54 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0054\n",
      "16/06/05 19:48:55 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0054\n",
      "16/06/05 19:48:55 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0054/\n",
      "16/06/05 19:48:55 INFO mapreduce.Job: Running job: job_1465099569246_0054\n",
      "16/06/05 19:49:03 INFO mapreduce.Job: Job job_1465099569246_0054 running in uber mode : false\n",
      "16/06/05 19:49:03 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 19:49:14 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 19:49:24 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 19:49:24 INFO mapreduce.Job: Job job_1465099569246_0054 completed successfully\n",
      "16/06/05 19:49:24 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=88134\n",
      "\t\tFILE: Number of bytes written=415703\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3458656\n",
      "\t\tHDFS: Number of bytes written=142658\n",
      "\t\tHDFS: Number of read operations=6\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1053184\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=910592\n",
      "\t\tTotal time spent by all map tasks (ms)=8228\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7114\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8228\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7114\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1053184\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=910592\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=4189064\n",
      "\t\tMap output materialized bytes=88130\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=380824\n",
      "\t\tCombine output records=12592\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=88130\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =1\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=1\n",
      "\t\tGC time elapsed (ms)=87\n",
      "\t\tCPU time spent (ms)=3450\n",
      "\t\tPhysical memory (bytes) snapshot=261435392\n",
      "\t\tVirtual memory (bytes) snapshot=1402298368\n",
      "\t\tTotal committed heap usage (bytes)=101449728\n",
      "\tCombiner\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142658\n",
      "16/06/05 19:49:24 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_3_0\n",
      "########################################################\n",
      "################ NUMBER OF UNIQUE PRODUCTS #############\n",
      "########################################################\n",
      "12592\n",
      "rm: `/user/cloudera/w261/HW3/output-3_3_0_b': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1061625465001892398.jar tmpDir=null\n",
      "16/06/05 19:49:39 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 19:49:39 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 19:49:40 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 19:49:41 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 19:49:41 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0055\n",
      "16/06/05 19:49:41 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0055\n",
      "16/06/05 19:49:41 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0055/\n",
      "16/06/05 19:49:41 INFO mapreduce.Job: Running job: job_1465099569246_0055\n",
      "16/06/05 19:49:50 INFO mapreduce.Job: Job job_1465099569246_0055 running in uber mode : false\n",
      "16/06/05 19:49:50 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 19:50:04 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 19:50:14 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 19:50:14 INFO mapreduce.Job: Job job_1465099569246_0055 completed successfully\n",
      "16/06/05 19:50:14 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=93746\n",
      "\t\tFILE: Number of bytes written=552436\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=202661\n",
      "\t\tHDFS: Number of bytes written=306354\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3104256\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=929408\n",
      "\t\tTotal time spent by all map tasks (ms)=24252\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7261\n",
      "\t\tTotal vcore-seconds taken by all map tasks=24252\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7261\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3104256\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=929408\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12592\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=243394\n",
      "\t\tMap output materialized bytes=94173\n",
      "\t\tInput split bytes=260\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=94173\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=123\n",
      "\t\tCPU time spent (ms)=3660\n",
      "\t\tPhysical memory (bytes) snapshot=398618624\n",
      "\t\tVirtual memory (bytes) snapshot=2098704384\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=202401\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=306354\n",
      "16/06/05 19:50:14 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_3_0_b\n",
      "########################################################\n",
      "# TOP 50 PRODUCTS BY FREQUENCY WITH RELATIVE FREQUENCY #\n",
      "########################################################\n",
      "dai62779\t6667\t0.0175067748\n",
      "fro40251\t3881\t0.0101910594\n",
      "ele17451\t3875\t0.0101753041\n",
      "gro73461\t3602\t0.0094584375\n",
      "sna80324\t3044\t0.0079931937\n",
      "ele32164\t2851\t0.0074863979\n",
      "dai75645\t2736\t0.0071844211\n",
      "sna45677\t2455\t0.0064465475\n",
      "fro31317\t2330\t0.0061183119\n",
      "dai85309\t2293\t0.0060211541\n",
      "ele26917\t2292\t0.0060185282\n",
      "fro80039\t2233\t0.0058636010\n",
      "gro21487\t2115\t0.0055537466\n",
      "sna99873\t2083\t0.0054697183\n",
      "gro59710\t2004\t0.0052622734\n",
      "gro71621\t1920\t0.0050416991\n",
      "fro85978\t1918\t0.0050364473\n",
      "gro30386\t1840\t0.0048316283\n",
      "ele74009\t1816\t0.0047686070\n",
      "gro56726\t1784\t0.0046845787\n",
      "dai63921\t1773\t0.0046556940\n",
      "gro46854\t1756\t0.0046110539\n",
      "ele66600\t1713\t0.0044981409\n",
      "dai83733\t1712\t0.0044955150\n",
      "fro32293\t1702\t0.0044692561\n",
      "ele66810\t1697\t0.0044561267\n",
      "sna55762\t1646\t0.0043222066\n",
      "dai22177\t1627\t0.0042723148\n",
      "fro78087\t1531\t0.0040202298\n",
      "ele99737\t1516\t0.0039808415\n",
      "ele34057\t1489\t0.0039099427\n",
      "gro94758\t1489\t0.0039099427\n",
      "fro35904\t1436\t0.0037707707\n",
      "fro53271\t1420\t0.0037287566\n",
      "sna93860\t1407\t0.0036946201\n",
      "sna90094\t1390\t0.0036499800\n",
      "gro38814\t1352\t0.0035501964\n",
      "ele56788\t1345\t0.0035318152\n",
      "gro61133\t1321\t0.0034687940\n",
      "dai88807\t1316\t0.0034556646\n",
      "ele74482\t1316\t0.0034556646\n",
      "ele59935\t1311\t0.0034425351\n",
      "sna96271\t1295\t0.0034005210\n",
      "dai43223\t1290\t0.0033873916\n",
      "ele91337\t1289\t0.0033847657\n",
      "gro15017\t1275\t0.0033480033\n",
      "dai31081\t1261\t0.0033112409\n",
      "gro81087\t1220\t0.0032035796\n",
      "dai22896\t1219\t0.0032009537\n",
      "gro85051\t1214\t0.0031878243\n",
      "cat: Unable to write to output stream.\n",
      "16/06/05 19:50:22 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_3_0_b' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_3_0_b1465181422291\n"
     ]
    }
   ],
   "source": [
    "# I first run a MapReduce job (a flavor of the classic WordCount)\n",
    "# to determine the number of times that the Products appear.\n",
    "# The number of lines in the output from this job is the \n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_3_0\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 1 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/productPurchaseDataMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_b.py \\\n",
    "-combiner /home/cloudera/w261/HW3/src/combiner3_2_c.py \\\n",
    "-input /user/cloudera/w261/HW3/data_3_3/ \\\n",
    "-output /user/cloudera/w261/HW3/output-3_3_0\n",
    "!echo \"########################################################\"\n",
    "!echo \"################ NUMBER OF UNIQUE PRODUCTS #############\"\n",
    "!echo \"########################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0/part-00000 | wc -l\n",
    "\n",
    "# Next I run a second MapReduce job to sort the output from the first,\n",
    "# previous job, using the code I wrote for HW3.2.2. This permits me\n",
    "# to determine the product with the largest number of appearances\n",
    "# as well as the top 50 most frequently purchased items,  \n",
    "# their frequency,  and their relative frequency\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0/* > product_purchase_output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_3_0_b\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-files \"product_purchase_output#word_count_output\" \\\n",
    "-input /user/cloudera/w261/HW3/output-3_3_0/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_3_0_b \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapperWithPartitionTable.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducerWithPartitionKey.py \\\n",
    "-numReduceTasks 1 \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "!echo \"########################################################\"\n",
    "!echo \"# TOP 50 PRODUCTS BY FREQUENCY WITH RELATIVE FREQUENCY #\"\n",
    "!echo \"########################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0_b/part-00000 | head -n 50\n",
    "!rm product_purchase_output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_3_0_b"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.3.1 [OPTIONAL] Using 2 reducers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using 2 reducers__:  Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 20:11:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_3_0_opt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_3_0_opt1465182663643\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3460450876668152919.jar tmpDir=null\n",
      "16/06/05 20:11:08 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 20:11:08 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 20:11:10 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 20:11:10 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/05 20:11:10 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0060\n",
      "16/06/05 20:11:10 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0060\n",
      "16/06/05 20:11:10 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0060/\n",
      "16/06/05 20:11:10 INFO mapreduce.Job: Running job: job_1465099569246_0060\n",
      "16/06/05 20:11:20 INFO mapreduce.Job: Job job_1465099569246_0060 running in uber mode : false\n",
      "16/06/05 20:11:20 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 20:11:38 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 20:11:53 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/05 20:11:54 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 20:11:54 INFO mapreduce.Job: Job job_1465099569246_0060 completed successfully\n",
      "16/06/05 20:11:54 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=91621\n",
      "\t\tFILE: Number of bytes written=542401\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3458656\n",
      "\t\tHDFS: Number of bytes written=142658\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1865728\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3540480\n",
      "\t\tTotal time spent by all map tasks (ms)=14576\n",
      "\t\tTotal time spent by all reduce tasks (ms)=27660\n",
      "\t\tTotal vcore-seconds taken by all map tasks=14576\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=27660\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1865728\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3540480\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=380824\n",
      "\t\tMap output bytes=4189064\n",
      "\t\tMap output materialized bytes=91613\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=380824\n",
      "\t\tCombine output records=12592\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=91613\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=187\n",
      "\t\tCPU time spent (ms)=4880\n",
      "\t\tPhysical memory (bytes) snapshot=384978944\n",
      "\t\tVirtual memory (bytes) snapshot=2108637184\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\tCombiner\n",
      "\t\tCalls=2\n",
      "\tReducer\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=142658\n",
      "16/06/05 20:11:54 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_3_0_opt\n",
      "########################################################\n",
      "################ NUMBER OF UNIQUE PRODUCTS #############\n",
      "########################################################\n",
      "12592\n",
      "rm: `/user/cloudera/w261/HW3/output-3_3_0_b_opt': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob6348427461903746299.jar tmpDir=null\n",
      "16/06/05 20:12:09 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 20:12:10 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 20:12:11 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/06/05 20:12:11 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 20:12:11 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0061\n",
      "16/06/05 20:12:11 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0061\n",
      "16/06/05 20:12:12 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0061/\n",
      "16/06/05 20:12:12 INFO mapreduce.Job: Running job: job_1465099569246_0061\n",
      "16/06/05 20:12:21 INFO mapreduce.Job: Job job_1465099569246_0061 running in uber mode : false\n",
      "16/06/05 20:12:21 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 20:12:36 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 20:12:53 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 20:12:53 INFO mapreduce.Job: Job job_1465099569246_0061 completed successfully\n",
      "16/06/05 20:12:54 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=93407\n",
      "\t\tFILE: Number of bytes written=677470\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=142926\n",
      "\t\tHDFS: Number of bytes written=306354\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3273216\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3555968\n",
      "\t\tTotal time spent by all map tasks (ms)=25572\n",
      "\t\tTotal time spent by all reduce tasks (ms)=27781\n",
      "\t\tTotal vcore-seconds taken by all map tasks=25572\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=27781\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3273216\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3555968\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=12592\n",
      "\t\tMap output records=12592\n",
      "\t\tMap output bytes=243394\n",
      "\t\tMap output materialized bytes=97965\n",
      "\t\tInput split bytes=268\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=12592\n",
      "\t\tReduce shuffle bytes=97965\n",
      "\t\tReduce input records=12592\n",
      "\t\tReduce output records=12592\n",
      "\t\tSpilled Records=25184\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=276\n",
      "\t\tCPU time spent (ms)=5180\n",
      "\t\tPhysical memory (bytes) snapshot=514117632\n",
      "\t\tVirtual memory (bytes) snapshot=2804768768\n",
      "\t\tTotal committed heap usage (bytes)=202899456\n",
      "\tReducer Counters\n",
      "\t\tCalls=2\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=142658\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=306354\n",
      "16/06/05 20:12:54 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_3_0_b_opt\n",
      "########################################################\n",
      "# TOP 50 PRODUCTS BY FREQUENCY WITH RELATIVE FREQUENCY #\n",
      "########################################################\n",
      "dai62779\t6667\t0.0175067748\n",
      "fro40251\t3881\t0.0101910594\n",
      "ele17451\t3875\t0.0101753041\n",
      "gro73461\t3602\t0.0094584375\n",
      "sna80324\t3044\t0.0079931937\n",
      "ele32164\t2851\t0.0074863979\n",
      "dai75645\t2736\t0.0071844211\n",
      "sna45677\t2455\t0.0064465475\n",
      "fro31317\t2330\t0.0061183119\n",
      "dai85309\t2293\t0.0060211541\n",
      "ele26917\t2292\t0.0060185282\n",
      "fro80039\t2233\t0.0058636010\n",
      "gro21487\t2115\t0.0055537466\n",
      "sna99873\t2083\t0.0054697183\n",
      "gro59710\t2004\t0.0052622734\n",
      "gro71621\t1920\t0.0050416991\n",
      "fro85978\t1918\t0.0050364473\n",
      "gro30386\t1840\t0.0048316283\n",
      "ele74009\t1816\t0.0047686070\n",
      "gro56726\t1784\t0.0046845787\n",
      "dai63921\t1773\t0.0046556940\n",
      "gro46854\t1756\t0.0046110539\n",
      "ele66600\t1713\t0.0044981409\n",
      "dai83733\t1712\t0.0044955150\n",
      "fro32293\t1702\t0.0044692561\n",
      "ele66810\t1697\t0.0044561267\n",
      "sna55762\t1646\t0.0043222066\n",
      "dai22177\t1627\t0.0042723148\n",
      "fro78087\t1531\t0.0040202298\n",
      "ele99737\t1516\t0.0039808415\n",
      "ele34057\t1489\t0.0039099427\n",
      "gro94758\t1489\t0.0039099427\n",
      "fro35904\t1436\t0.0037707707\n",
      "fro53271\t1420\t0.0037287566\n",
      "sna93860\t1407\t0.0036946201\n",
      "sna90094\t1390\t0.0036499800\n",
      "gro38814\t1352\t0.0035501964\n",
      "ele56788\t1345\t0.0035318152\n",
      "gro61133\t1321\t0.0034687940\n",
      "dai88807\t1316\t0.0034556646\n",
      "ele74482\t1316\t0.0034556646\n",
      "ele59935\t1311\t0.0034425351\n",
      "sna96271\t1295\t0.0034005210\n",
      "dai43223\t1290\t0.0033873916\n",
      "ele91337\t1289\t0.0033847657\n",
      "gro15017\t1275\t0.0033480033\n",
      "dai31081\t1261\t0.0033112409\n",
      "gro81087\t1220\t0.0032035796\n",
      "dai22896\t1219\t0.0032009537\n",
      "gro85051\t1214\t0.0031878243\n",
      "cat: Unable to write to output stream.\n",
      "16/06/05 20:13:03 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_3_0_b_opt' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_3_0_b_opt1465182783259\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "### Notice that the below code runs identically for the case with\n",
    "### 1 and two reducers in both MapReduce jobs\n",
    "#################################################################\n",
    "# I first run a MapReduce job (a flavor of the classic WordCount)\n",
    "# to determine the number of times that the Products appear.\n",
    "# The number of lines in the output from this job is the \n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_3_0_opt\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 2 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/productPurchaseDataMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_b.py \\\n",
    "-combiner /home/cloudera/w261/HW3/src/combiner3_2_c.py \\\n",
    "-input /user/cloudera/w261/HW3/data_3_3/ \\\n",
    "-output /user/cloudera/w261/HW3/output-3_3_0_opt\n",
    "!echo \"########################################################\"\n",
    "!echo \"################ NUMBER OF UNIQUE PRODUCTS #############\"\n",
    "!echo \"########################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0_opt/* | wc -l\n",
    "\n",
    "# Next I run a second MapReduce job to sort the output from the first,\n",
    "# previous job, using the code I wrote for HW3.2.2. This permits me\n",
    "# to determine the product with the largest number of appearances\n",
    "# as well as the top 50 most frequently purchased items,  \n",
    "# their frequency,  and their relative frequency\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0_opt/* > product_purchase_output_opt\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_3_0_b_opt\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-files \"product_purchase_output_opt#word_count_output\" \\\n",
    "-input /user/cloudera/w261/HW3/output-3_3_0_opt/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_3_0_b_opt \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapperWithPartitionTable.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducerWithPartitionKey.py \\\n",
    "-numReduceTasks 2 \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "!echo \"########################################################\"\n",
    "!echo \"# TOP 50 PRODUCTS BY FREQUENCY WITH RELATIVE FREQUENCY #\"\n",
    "!echo \"########################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0_b_opt/part-00001 | head -n 50\n",
    "!rm product_purchase_output_opt\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_3_0_b_opt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.4 Pairs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From a data mining perspective (aPriori), Support and confidence are defined as follows:\n",
    "\n",
    "       In data mining, the support value of X with respect to T is defined as the proportion of transactions in the \n",
    "       database which contains the item-set X. (a relative frequency of sorts)\n",
    "       The confidence value of a rule, X ==>  Y , with respect to a set of transactions T, is the \n",
    "       proportion of the transactions that contains X which also contains Y. \n",
    "       The pairs/stripes algorithm returns cooccurrence information that can be used directly to  calculate the confidence and support. \n",
    "       Note that confidence for pair X ==>  Y will  differ from the relative frequency that results from stripes when X occurs by itself in transactions.\n",
    "\n",
    "\n",
    "Suppose we want to recommend new products to the customer based on the products they\n",
    "have already browsed on the online website. Write a map-reduce program \n",
    "to find products which are frequently browsed together. Fix the support count (cooccurence count) to s = 100 \n",
    "(i.e. product pairs need to occur together at least 100 times to be considered frequent) \n",
    "and find pairs of items (sometimes referred to itemsets of size 2 in association rule mining) that have a support count of 100 or more.\n",
    "\n",
    "List the top 50 product pairs with corresponding support count (aka frequency), and relative frequency or support (number of records where they coccur, the number of records where they coccur/the number of baskets in the dataset)  in decreasing order of support  for frequent (100>count) itemsets of size 2. \n",
    "\n",
    "Use the Pairs pattern (lecture 3)  to  extract these frequent itemsets of size 2. Free free to use combiners if they bring value. Instrument your code with counters for count the number of times your mapper, combiner and reducers are called.  \n",
    "\n",
    "Please output records of the following form for the top 50 pairs (itemsets of size 2): \n",
    "\n",
    "      item1, item2, support count, support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsMapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    products = line.split()\n",
    "    line_products = set()\n",
    "    for product in products:\n",
    "        for p in products:\n",
    "            if p != product:\n",
    "                temp = product + \".\" + p\n",
    "                reverse_temp = p + \".\" + product\n",
    "                if not set([temp]).issubset(line_products) and not set([reverse_temp]).issubset(line_products):\n",
    "                    print \"%s.%s\\t1\" % (product,p)\n",
    "                    line_products.add(temp)\n",
    "                    line_products.add(reverse_temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pairsMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GRO59710.DAI62779\t1\r\n",
      "GRO59710.DAI92600\t1\r\n",
      "GRO59710.DAI42083\t1\r\n",
      "GRO59710.SNA63881\t1\r\n",
      "SNA63881.DAI45339\t1\r\n",
      "SNA63881.SNA59903\t1\r\n",
      "SNA63881.DAI62779\t1\r\n",
      "SNA63881.DAI92600\t1\r\n",
      "SNA63881.DAI42083\t1\r\n",
      "SNA63881.GRO59710\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/ProductPurchaseData.txt \\\n",
    "| ./pairsMapper.py | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsCombiner.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsCombiner.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "current_count = 0\n",
    "current_word = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    record = line.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "print \"%s\\t%d\" % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pairsCombiner.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNA99941.FRO66216\t1\r\n",
      "SNA99941.FRO77633\t1\r\n",
      "SNA99941.GRO31753\t1\r\n",
      "SNA99941.GRO38983\t1\r\n",
      "SNA99941.GRO53618\t1\r\n",
      "SNA99941.GRO59710\t1\r\n",
      "SNA99941.GRO93224\t1\r\n",
      "SNA99941.SNA18093\t1\r\n",
      "SNA99941.SNA47306\t1\r\n",
      "SNA99941.SNA99873\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/ProductPurchaseData.txt \\\n",
    "| ./pairsMapper.py | sort -k1,1 | ./pairsCombiner.py | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing pairsReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsReducer.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "current_count = 0\n",
    "current_word = \"\"\n",
    "\n",
    "for line in sys.stdin:\n",
    "    record = line.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "print \"%s\\t%d\" % (current_word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pairsReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SNA99941.FRO66216\t1\r\n",
      "SNA99941.FRO77633\t1\r\n",
      "SNA99941.GRO31753\t1\r\n",
      "SNA99941.GRO38983\t1\r\n",
      "SNA99941.GRO53618\t1\r\n",
      "SNA99941.GRO59710\t1\r\n",
      "SNA99941.GRO93224\t1\r\n",
      "SNA99941.SNA18093\t1\r\n",
      "SNA99941.SNA47306\t1\r\n",
      "SNA99941.SNA99873\t1\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/ProductPurchaseData.txt \\\n",
    "| ./pairsMapper.py | sort -k1,1 | ./pairsCombiner.py \\\n",
    "| ./pairsReducer.py | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting pairsMapperWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile pairsMapperWithPartitionTable.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "total_term_count = 0\n",
    "values = []\n",
    "FREQUENCY_FLOOR = 100\n",
    "# using the word count output to determine the partitions\n",
    "f = open('word_count_output','r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    values.extend([int(value)])\n",
    "\n",
    "values = sorted(values)\n",
    "median = int(values[(len(values) / 2)])\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    # minimal feature engineering: setting tokens to all be lower case\n",
    "    # for the purposes of having case insensitive secondary sort\n",
    "    line = line.lower()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    value = int(value)\n",
    "    if value <= median:\n",
    "        if value >= FREQUENCY_FLOOR:\n",
    "            print \"group1\\t%s\" % (line)\n",
    "    else:\n",
    "        if value >= FREQUENCY_FLOOR:\n",
    "            print \"group2\\t%s\" % (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x pairsMapperWithPartitionTable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group2\tsna99873.gro59710\t124\r\n",
      "group2\tsna99873.gro61133\t134\r\n",
      "group2\tsna99873.gro71621\t172\r\n",
      "group2\tsna99873.gro73461\t296\r\n",
      "group2\tsna99873.gro94758\t139\r\n",
      "group2\tsna99873.sna40784\t129\r\n",
      "group2\tsna99873.sna45677\t215\r\n",
      "group2\tsna99873.sna55762\t143\r\n",
      "group2\tsna99873.sna80324\t163\r\n",
      "group2\tsna99873.sna93860\t105\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/ProductPurchaseData.txt \\\n",
    "| ./pairsMapper.py | sort -k1,1 | ./pairsCombiner.py \\\n",
    "| ./pairsReducer.py > word_count_output\n",
    "!cat word_count_output | ./pairsMapperWithPartitionTable.py \\\n",
    "| tail\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/cloudera/w261/HW3/data_3_4\n",
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/HW3/data/ProductPurchaseData.txt /user/cloudera/w261/HW3/data_3_3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 21:27:26 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_4_0' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_4_01465187246346\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3495248505987087217.jar tmpDir=null\n",
      "16/06/05 21:27:32 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 21:27:33 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 21:27:34 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/05 21:27:34 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/05 21:27:34 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0068\n",
      "16/06/05 21:27:35 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0068\n",
      "16/06/05 21:27:35 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0068/\n",
      "16/06/05 21:27:35 INFO mapreduce.Job: Running job: job_1465099569246_0068\n",
      "16/06/05 21:27:45 INFO mapreduce.Job: Job job_1465099569246_0068 running in uber mode : false\n",
      "16/06/05 21:27:45 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 21:27:59 INFO mapreduce.Job:  map 15% reduce 0%\n",
      "16/06/05 21:28:02 INFO mapreduce.Job:  map 23% reduce 0%\n",
      "16/06/05 21:28:05 INFO mapreduce.Job:  map 30% reduce 0%\n",
      "16/06/05 21:28:08 INFO mapreduce.Job:  map 33% reduce 0%\n",
      "16/06/05 21:28:11 INFO mapreduce.Job:  map 43% reduce 0%\n",
      "16/06/05 21:28:14 INFO mapreduce.Job:  map 45% reduce 0%\n",
      "16/06/05 21:28:17 INFO mapreduce.Job:  map 53% reduce 0%\n",
      "16/06/05 21:28:20 INFO mapreduce.Job:  map 63% reduce 0%\n",
      "16/06/05 21:28:23 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/06/05 21:28:26 INFO mapreduce.Job:  map 99% reduce 0%\n",
      "16/06/05 21:28:29 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 21:28:48 INFO mapreduce.Job:  map 100% reduce 80%\n",
      "16/06/05 21:28:51 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 21:28:52 INFO mapreduce.Job: Job job_1465099569246_0068 completed successfully\n",
      "16/06/05 21:28:52 INFO mapreduce.Job: Counters: 49\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=20845569\n",
      "\t\tFILE: Number of bytes written=29574898\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3458656\n",
      "\t\tHDFS: Number of bytes written=21239610\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5310080\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=5073664\n",
      "\t\tTotal time spent by all map tasks (ms)=41485\n",
      "\t\tTotal time spent by all reduce tasks (ms)=39638\n",
      "\t\tTotal vcore-seconds taken by all map tasks=41485\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=39638\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5310080\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=5073664\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=31101\n",
      "\t\tMap output records=2534014\n",
      "\t\tMap output bytes=50680280\n",
      "\t\tMap output materialized bytes=8522392\n",
      "\t\tInput split bytes=139\n",
      "\t\tCombine input records=4046966\n",
      "\t\tCombine output records=2573177\n",
      "\t\tReduce input groups=1060225\n",
      "\t\tReduce shuffle bytes=8522392\n",
      "\t\tReduce input records=1060225\n",
      "\t\tReduce output records=1060225\n",
      "\t\tSpilled Records=3633402\n",
      "\t\tShuffled Maps =2\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tGC time elapsed (ms)=220\n",
      "\t\tCPU time spent (ms)=29060\n",
      "\t\tPhysical memory (bytes) snapshot=417185792\n",
      "\t\tVirtual memory (bytes) snapshot=2116366336\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=3458517\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=21239610\n",
      "16/06/05 21:28:52 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_4_0\n",
      "/bin/sh: -c: line 0: syntax error near unexpected token `10'\n",
      "/bin/sh: -c: line 0: `sleep(10)'\n",
      "rm: `/user/cloudera/w261/HW3/output-3_4_0_b': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4531423955382894607.jar tmpDir=null\n",
      "16/06/05 21:29:03 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 21:29:04 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 21:29:05 INFO mapred.FileInputFormat: Total input paths to process : 2\n",
      "16/06/05 21:29:05 INFO mapreduce.JobSubmitter: number of splits:2\n",
      "16/06/05 21:29:05 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0069\n",
      "16/06/05 21:29:06 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0069\n",
      "16/06/05 21:29:06 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0069/\n",
      "16/06/05 21:29:06 INFO mapreduce.Job: Running job: job_1465099569246_0069\n",
      "16/06/05 21:29:15 INFO mapreduce.Job: Job job_1465099569246_0069 running in uber mode : false\n",
      "16/06/05 21:29:15 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 21:29:33 INFO mapreduce.Job:  map 46% reduce 0%\n",
      "16/06/05 21:29:34 INFO mapreduce.Job:  map 73% reduce 0%\n",
      "16/06/05 21:29:35 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 21:29:51 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/05 21:29:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 21:29:52 INFO mapreduce.Job: Job job_1465099569246_0069 completed successfully\n",
      "16/06/05 21:29:52 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=8258\n",
      "\t\tFILE: Number of bytes written=503191\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=21239870\n",
      "\t\tHDFS: Number of bytes written=26602\n",
      "\t\tHDFS: Number of read operations=12\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=4\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=2\n",
      "\t\tData-local map tasks=2\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4557696\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=3565312\n",
      "\t\tTotal time spent by all map tasks (ms)=35607\n",
      "\t\tTotal time spent by all reduce tasks (ms)=27854\n",
      "\t\tTotal vcore-seconds taken by all map tasks=35607\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=27854\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4557696\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=3565312\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1060225\n",
      "\t\tMap output records=760\n",
      "\t\tMap output bytes=22802\n",
      "\t\tMap output materialized bytes=8959\n",
      "\t\tInput split bytes=260\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=760\n",
      "\t\tReduce shuffle bytes=8959\n",
      "\t\tReduce input records=760\n",
      "\t\tReduce output records=760\n",
      "\t\tSpilled Records=1520\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=268\n",
      "\t\tCPU time spent (ms)=7480\n",
      "\t\tPhysical memory (bytes) snapshot=510689280\n",
      "\t\tVirtual memory (bytes) snapshot=2804953088\n",
      "\t\tTotal committed heap usage (bytes)=202899456\n",
      "\tReducer Counters\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=21239610\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26602\n",
      "16/06/05 21:29:52 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_4_0_b\n",
      "#############################################################\n",
      "# TOP 50 PRODUCT PAIRS BY FREQUENCY WITH RELATIVE FREQUENCY #\n",
      "#############################################################\n",
      "dai62779.ele17451\t1370\t0.0005406442\n",
      "fro40251.gro85051\t1048\t0.0004135731\n",
      "fro40251.sna80324\t963\t0.0003800295\n",
      "gro73461.fro40251\t829\t0.0003271489\n",
      "ele92920.dai62779\t804\t0.0003172832\n",
      "gro73461.dai62779\t750\t0.0002959731\n",
      "fro40251.fro92469\t704\t0.0002778201\n",
      "gro73461.dai75645\t687\t0.0002711114\n",
      "dai62779.sna80324\t662\t0.0002612456\n",
      "fro40251.dai75645\t662\t0.0002612456\n",
      "dai62779.dai75645\t618\t0.0002438818\n",
      "ele99737.dai85309\t614\t0.0002423033\n",
      "dai75645.fro40251\t592\t0.0002336214\n",
      "sna80324.dai75645\t589\t0.0002324375\n",
      "dai62779.dai85309\t579\t0.0002284912\n",
      "dai62779.gro30386\t576\t0.0002273073\n",
      "fro40251.dai62779\t550\t0.0002170469\n",
      "dai75645.sna80324\t541\t0.0002134953\n",
      "gro73461.sna80324\t539\t0.0002127060\n",
      "ele32164.dai62779\t531\t0.0002095490\n",
      "dai62779.fro40251\t520\t0.0002052080\n",
      "gro21487.gro73461\t520\t0.0002052080\n",
      "ele32164.dai43223\t506\t0.0001996832\n",
      "ele32164.gro59710\t465\t0.0001835033\n",
      "sna18336.dai62779\t462\t0.0001823194\n",
      "fro85978.sna95666\t459\t0.0001811355\n",
      "fro40251.ele17451\t454\t0.0001791624\n",
      "gro73461.ele17451\t451\t0.0001779785\n",
      "sna18336.ele92920\t451\t0.0001779785\n",
      "sna80324.fro40251\t449\t0.0001771892\n",
      "gro59710.ele32164\t446\t0.0001760053\n",
      "gro73461.ele32164\t432\t0.0001704805\n",
      "dai55148.dai62779\t425\t0.0001677181\n",
      "dai62779.ele26917\t422\t0.0001665342\n",
      "dai62779.sna45677\t412\t0.0001625879\n",
      "sna80324.ele17451\t409\t0.0001614040\n",
      "gro73461.gro38814\t408\t0.0001610094\n",
      "dai62779.gro73461\t389\t0.0001535114\n",
      "gro21487.dai62779\t376\t0.0001483812\n",
      "dai62779.sna93860\t371\t0.0001464080\n",
      "sna55762.dai62779\t353\t0.0001393047\n",
      "dai62779.gro71621\t351\t0.0001385154\n",
      "ele17451.dai75645\t351\t0.0001385154\n",
      "ele92920.ele17451\t351\t0.0001385154\n",
      "dai85309.dai62779\t339\t0.0001337798\n",
      "fro40251.dai88079\t338\t0.0001333852\n",
      "ele32164.ele17451\t336\t0.0001325960\n",
      "ele17451.gro30386\t332\t0.0001310174\n",
      "dai62779.sna96271\t330\t0.0001302282\n",
      "sna99873.ele74482\t330\t0.0001302282\n",
      "16/06/05 21:30:01 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_4_0_b' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_4_0_b1465187400998\n"
     ]
    }
   ],
   "source": [
    "#################################################################\n",
    "### Notice that the below code runs identically for the case with\n",
    "### 1 and two reducers in both MapReduce jobs\n",
    "#################################################################\n",
    "# I first run a MapReduce job (a flavor of the classic WordCount)\n",
    "# to determine the number of times that the Products appear.\n",
    "# The number of lines in the output from this job is the \n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_4_0\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 2 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/pairsMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/pairsReducer.py \\\n",
    "-combiner /home/cloudera/w261/HW3/src/pairsCombiner.py \\\n",
    "-input /user/cloudera/w261/HW3/data_3_3/ \\\n",
    "-output /user/cloudera/w261/HW3/output-3_4_0\n",
    "# !echo \"########################################################\"\n",
    "# !echo \"################ NUMBER OF UNIQUE PRODUCTS #############\"\n",
    "# !echo \"########################################################\"\n",
    "# !hdfs dfs -cat /user/cloudera/w261/HW3/output-3_3_0_opt/* | wc -l\n",
    "\n",
    "# Next I run a second MapReduce job to sort the output from the first,\n",
    "# previous job, using the code I wrote for HW3.2.2. This permits me\n",
    "# to determine the product with the largest number of appearances\n",
    "# as well as the top 50 most frequently purchased items,  \n",
    "# their frequency,  and their relative frequency\n",
    "\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_4_0/* > pairs_output\n",
    "!sleep 10\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_4_0_b\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D stream.num.map.output.key.fields=3 \\\n",
    "-D stream.map.output.field.separator=\"\\t\" \\\n",
    "-D mapreduce.partition.keypartitioner.options=\"-k1,1\" \\\n",
    "-D mapreduce.job.output.key.comparator.class=org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k3,3nr -k2,2\" \\\n",
    "-files \"pairs_output#word_count_output\" \\\n",
    "-input /user/cloudera/w261/HW3/output-3_4_0/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_4_0_b \\\n",
    "-mapper /home/cloudera/w261/HW3/src/pairsMapperWithPartitionTable.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducerWithPartitionKey.py \\\n",
    "-numReduceTasks 2 \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner\n",
    "!echo \"#############################################################\"\n",
    "!echo \"# TOP 50 PRODUCT PAIRS BY FREQUENCY WITH RELATIVE FREQUENCY #\"\n",
    "!echo \"#############################################################\"\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_4_0_b/part-00001 | head -n 50\n",
    "!rm pairs_output\n",
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_4_0_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
