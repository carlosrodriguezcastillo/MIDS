{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 3**\n",
    "\n",
    "**Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__This notebook attempts to solve the exercises for homework assignment three.__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW3.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you merge  two sorted  lists/arrays of records of the form [key, value]? Where is this  used in Hadoop MapReduce? [Hint within the shuffle]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__\n",
    "\n",
    "Two sorted lists of records of the form [key, value] can be merged into a single, sorted list by:\n",
    "\n",
    "- iterating through the larger of the two sorted lists.\n",
    "    - comparing the first (assumed smallest) element of the smaller list to the first (assumed smallest) element of the larger list\n",
    "    - popping the smallest of the elements from the above comparison and placing it at the bottom of a new list to hold the single merged list\n",
    "    - the above is repeated until one of the two lists is empty\n",
    "    - finally, the remainding list is appended to the bottom of the single merged list (because itself is sorted)\n",
    "\n",
    "This sorting technique is used in the shuffle phase of Hadoop MapReduce: the shuffle occurs after the mappers have output the intermediate key value records, but before these intermediate records are passed to the reducers. Specifically, after the mapper tasks have been executed, the intermediate records are written to memory (and later to disk if there is enough data), the records are then partitioned based on their keys, and then mergesorted."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is  a combiner function in the context of Hadoop? Give an example where it can be used and justify why it should be used in the context of this problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__\n",
    "\n",
    "In the context of Hadoop, a combiner function is a function that aggregates intermediate results from mapper tasks before these are fed to reducer tasks downstream in the MapReduce framework of Hadoop. The Hadoop MapReduce framework reserves the right to use combiners (or not) at its discretion regardless of the instructions presented by the programmer to the framework. As such, it is critical that the combiner functions not only ingest intermediate records in the same format as those fed to the reducers downstream, but they must also produce aggregated intermediate records that are identical in format to the un-aggregated records output by the mapper tasks upstream. As a sort of 'mini reducers', combiners are meant to accomplish the important tasks of (1) minimizing the number of key-value pairs that are  shuffled across the network from the mappers to the reducers and (2) reducing the risk of reducer tasks 'lagging' due to aggregating across keys that have a very large list of values.\n",
    "\n",
    "An example of when combiners can (and should) be used is the simple word count exercise! Combiners should be used in word count to achieve tasks (1) and (2) mentioned above. Particularly, the frequency of words varies widely in a corpus; in an English corpus, the token \"the\" is likely to be encountered a very large number of times and considerably more times than many other tokens in the corpus. As such the reducer assigned to aggregate the counts for the token will likely lag behind other reducers processing rarer tokens; furthermore, there will be a very large number of \"the\\t1\" intermediate records that will be shuffled across the network. Combiner tasks will critically reduce the amount of work that the reducers assigned to process common tokens (such as \"the\") will need to do while also minimizing the amount of data that is shuffled across the network (e.g. shuffling \"the\\tN\" intermediate records as opposed to \"the\\t1\").\n",
    "\n",
    "Finally, it is important to note that the word count example (and any larger task that uses combiners for that matter) can be resolved (abeit consuming more resources) without the use of combiners."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the Hadoop shuffle?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "The Hadoop MapReduce shuffle is a critical step in a Hadoop MapReduce job; on a high level, it is a step that transfers intermediate records that are output in the map phase of a job by mappers to reducers such that they may be used as inputs to the reduce phase. During the shuffle the intermediate key value pairs are merge sorted and passed through the network from the mapper nodes to the reducer nodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.1 Consumer complaints dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Use Counters to do EDA (exploratory data analysis and to monitor progress)__\n",
    "\n",
    "Counters are lightweight objects in Hadoop that allow you to keep track of system progress in both the map and reduce stages of processing. By default, Hadoop defines a number of standard counters in \"groups\"; these show up in the jobtracker webapp, giving you information such as \"Map input records\", \"Map output records\", etc. \n",
    "\n",
    "While processing information/data using MapReduce job, it is a challenge to monitor the progress of parallel threads running across nodes of distributed clusters. Moreover, it is also complicated to distinguish between the data that has been processed and the data which is yet to be processed. The MapReduce Framework offers a provision of user-defined Counters, which can be effectively utilized to monitor the progress of data across nodes of distributed clusters.\n",
    "\n",
    "Use the Consumer Complaints  Dataset provide here to complete this question:\n",
    "\n",
    "     https://www.dropbox.com/s/vbalm3yva2rr86m/Consumer_Complaints.csv?dl=0\n",
    "\n",
    "The consumer complaints dataset consists of diverse consumer complaints, which have been reported across the United States regarding various types of loans. The dataset consists of records of the form:\n",
    "\n",
    "|Complaint ID|Product|Sub-product|Issue|Sub-issue|State|ZIP code|Submitted via|Date received|Date sent to company|Company|Company response|Timely response?|Consumer disputed?|\n",
    "|---|---|---|---|---|---|---|---|---|---|---|---|---|---|\n",
    "|1114245|Debt collection|Medical|Disclosure verification of debt|Not given enough info to verify debt|FL|32219|Web|11/13/2014|11/13/2014|\"Choice Recovery, Inc.\"|Closed with explanation|Yes|\n",
    "|1114488|Debt collection|Medical|Disclosure verification of debt|Right to dispute notice not received|TX|75006|Web|11/13/2014|11/13/2014|\"Expert Global Solutions, Inc.\"|In progress|Yes|\n",
    "|1114255|Bank account or service|Checking account|Deposits and withdrawals| |NY|11102|Web|11/13/2014|11/13/2014|\"FNIS (Fidelity National Information Services, Inc.)\"|In progress|Yes|\n",
    "|1115106|Debt collection|\"Other (phone, health club, etc.)\"|Communication tactics|Frequent or repeated calls|GA|31721|Web|11/13/2014|11/13/2014|\"Expert Global Solutions, Inc.\"|In progress|Yes|\n",
    "\n",
    "__User-defined Counters__\n",
    "\n",
    "Now, letâ€™s use Hadoop Counters to identify the number of complaints pertaining to debt collection, mortgage and other categories (all other categories get lumped into this one) in the consumer complaints dataset. Basically produce the distribution of the Product column in this dataset using counters (limited to 3 counters here).\n",
    "\n",
    "Hadoop offers Job Tracker, an UI tool to determine the status and statistics of all jobs. Using the job tracker UI, developers can view the Counters that have been created. Screenshot your  job tracker UI as your job completes and include it here. Make sure that your user defined counters are visible.\n",
    "\n",
    "__ANSWER:__\n",
    "\n",
    "Using Hadoop Counters, we identified that the number of complaints pertaining to __debt collection is 44372__, the number of complaints pertaining to __mortgage is 125752__, and the number of complaints pertaining to __other issues is 142789__."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing counterMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counterMapper.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for counterMapper.py for W261 HW3.0\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def map_function(record):\n",
    "    record = record.strip()\n",
    "    record_parameters = record.split(\",\")\n",
    "    product = record_parameters[1]\n",
    "    return (product, 1)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    for line in sys.stdin:\n",
    "        product_type, count = map_function(line)\n",
    "        print \"%s\\t%d\" % (product_type, int(count))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x counterMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting counterReducer.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile counterReducer.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for counterReducer.py for W261 HW3.0\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def reduce_function(record):\n",
    "    record = record.strip()\n",
    "    record_parameters = record.split(\"\\t\")\n",
    "    product_type = record_parameters[0]\n",
    "    count = int(record_parameters[1])\n",
    "    if product_type == \"Mortgage\":\n",
    "        sys.stderr.write(\"reporter:counter:Product,Mortgage,1\\n\")\n",
    "    elif product_type == \"Debt collection\":\n",
    "        sys.stderr.write(\"reporter:counter:Product,Debt collection,1\\n\")\n",
    "    else:\n",
    "        sys.stderr.write(\"reporter:counter:Product,Other,1\\n\")\n",
    "if __name__ == \"__main__\":\n",
    "    for line in sys.stdin:\n",
    "        reduce_function(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x counterReducer.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/HW3/data/* \\\n",
    "/user/cloudera/w261/HW3/data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/01 10:33:50 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_0' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_0\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob2418945648414801191.jar tmpDir=null\n",
      "16/06/01 10:33:54 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/01 10:33:55 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/01 10:33:55 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/01 10:33:55 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/01 10:33:56 INFO Configuration.deprecation: mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "16/06/01 10:33:56 INFO Configuration.deprecation: mapred.map.tasks is deprecated. Instead, use mapreduce.job.maps\n",
      "16/06/01 10:33:56 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0039\n",
      "16/06/01 10:33:56 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0039\n",
      "16/06/01 10:33:56 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0039/\n",
      "16/06/01 10:33:56 INFO mapreduce.Job: Running job: job_1464465911163_0039\n",
      "16/06/01 10:34:06 INFO mapreduce.Job: Job job_1464465911163_0039 running in uber mode : false\n",
      "16/06/01 10:34:06 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/01 10:34:17 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/01 10:34:39 INFO mapreduce.Job:  map 100% reduce 33%\n",
      "16/06/01 10:34:43 INFO mapreduce.Job:  map 100% reduce 34%\n",
      "16/06/01 10:34:45 INFO mapreduce.Job:  map 100% reduce 42%\n",
      "16/06/01 10:34:46 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/01 10:35:56 INFO mapreduce.Job:  map 100% reduce 61%\n",
      "16/06/01 10:35:59 INFO mapreduce.Job:  map 100% reduce 70%\n",
      "16/06/01 10:36:02 INFO mapreduce.Job:  map 100% reduce 76%\n",
      "16/06/01 10:36:06 INFO mapreduce.Job:  map 100% reduce 83%\n",
      "16/06/01 10:36:16 INFO mapreduce.Job:  map 100% reduce 92%\n",
      "16/06/01 11:15:18 INFO mapreduce.Job:  map 100% reduce 95%\n",
      "16/06/01 11:15:25 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/01 11:15:25 INFO mapreduce.Job: Job job_1464465911163_0039 completed successfully\n",
      "16/06/01 11:15:26 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=261990\n",
      "\t\tFILE: Number of bytes written=1120696\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50906621\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1139968\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=628716032\n",
      "\t\tTotal time spent by all map tasks (ms)=8906\n",
      "\t\tTotal time spent by all reduce tasks (ms)=4911844\n",
      "\t\tTotal vcore-seconds taken by all map tasks=8906\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=4911844\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1139968\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=628716032\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=312913\n",
      "\t\tMap output bytes=4878322\n",
      "\t\tMap output materialized bytes=261974\n",
      "\t\tInput split bytes=135\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce shuffle bytes=261974\n",
      "\t\tReduce input records=312913\n",
      "\t\tReduce output records=0\n",
      "\t\tSpilled Records=625826\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=3160\n",
      "\t\tCPU time spent (ms)=25340\n",
      "\t\tPhysical memory (bytes) snapshot=550035456\n",
      "\t\tVirtual memory (bytes) snapshot=3520311296\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tProduct\n",
      "\t\tDebt collection=44372\n",
      "\t\tMortgage=125752\n",
      "\t\tOther=142789\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "16/06/01 11:15:26 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_0\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_0\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapred.map.tasks=1 \\\n",
    "-D mapred.reduce.tasks=4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/counterMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/counterReducer.py \\\n",
    "-input /user/cloudera/w261/HW3/data/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.2 Analyze the performance of your Mappers, Combiners and Reducers using Counters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this brief study the Input file will be one record (the next line only):\n",
    "\n",
    "foo foo quux labs foo bar quux\n",
    "\n",
    "Perform a word count analysis of this single record dataset using a Mapper and Reducer based WordCount (i.e., no combiners are used here) using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing this word count job. The answer  should be 1 and 4 respectively. Please explain.\n",
    "\n",
    "Please use multiple mappers and reducers for these jobs (at least 2 mappers and 2 reducers)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperCounter3_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperCounter3_2.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for mapper counter for W261 HW3.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def map_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Calls,1\\n\")\n",
    "\n",
    "def map_function(record):\n",
    "    record = record.strip()\n",
    "    #sys.stderr.write(\"mapper record:%s\\n\" % record)\n",
    "    terms = record.split(\" \")\n",
    "    for term in terms:\n",
    "        #sys.stderr.write(\"mapper term:%s\\n\" % term)\n",
    "        print \"%s\\t1\" % term\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_initialize()\n",
    "    for line in sys.stdin:\n",
    "        intermediate_input = map_function(line)\n",
    "        #print intermediate_input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperCounter3_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerCounter3_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerCounter3_2.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for reducer counter for W261 HW3.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def reduce_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def reduce_function(record,current_word,current_count):\n",
    "    record = record.strip()\n",
    "    #sys.stderr.write(\"reducer record:%s\\n\" % record)\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "#         return (current_word, current_sum)\n",
    "#     elif current_word == word:\n",
    "#         current_word = word\n",
    "#         current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "    return (current_word, current_count)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #reduce_initialize()\n",
    "    word = reduce_initialize()\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, current_count = reduce_function(line, word, current_count)\n",
    "    print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerCounter3_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "foo foo quux labs foo bar quux\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/document3_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Mapper,Calls,1\r\n",
      "reporter:counter:Reducer,Calls,1\r\n",
      "reporter:counter:Reducer,Calls,1\r\n",
      "bar\t1\r\n",
      "foo\t3\r\n",
      "labs\t1\r\n",
      "quux\t2\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW3/data/document3_2.txt | ./mapperCounter3_2.py | sort -k1,1 | ./reducerCounter3_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -copyFromLocal /home/cloudera/w261/HW3/data/document3_2.txt \\\n",
    "/user/cloudera/w261/HW3/data3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!hdfs dfs -mkdir /user/cloudera/w261/HW3/data3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/cloudera/w261/HW3/output-3_2': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5284007959467736255.jar tmpDir=null\n",
      "16/06/02 19:55:42 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 19:55:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 19:55:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 19:55:44 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 19:55:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0043\n",
      "16/06/02 19:55:45 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0043\n",
      "16/06/02 19:55:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0043/\n",
      "16/06/02 19:55:45 INFO mapreduce.Job: Running job: job_1464465911163_0043\n",
      "16/06/02 19:55:55 INFO mapreduce.Job: Job job_1464465911163_0043 running in uber mode : false\n",
      "16/06/02 19:55:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 19:56:05 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 19:56:24 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 19:56:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 19:56:40 INFO mapreduce.Job: Job job_1464465911163_0043 completed successfully\n",
      "16/06/02 19:56:41 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=127\n",
      "\t\tFILE: Number of bytes written=597005\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=161\n",
      "\t\tHDFS: Number of bytes written=26\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=990080\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=7910912\n",
      "\t\tTotal time spent by all map tasks (ms)=7735\n",
      "\t\tTotal time spent by all reduce tasks (ms)=61804\n",
      "\t\tTotal vcore-seconds taken by all map tasks=7735\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=61804\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=990080\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=7910912\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=1\n",
      "\t\tMap output records=7\n",
      "\t\tMap output bytes=45\n",
      "\t\tMap output materialized bytes=111\n",
      "\t\tInput split bytes=130\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=4\n",
      "\t\tReduce shuffle bytes=111\n",
      "\t\tReduce input records=7\n",
      "\t\tReduce output records=4\n",
      "\t\tSpilled Records=14\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=444\n",
      "\t\tCPU time spent (ms)=5810\n",
      "\t\tPhysical memory (bytes) snapshot=637849600\n",
      "\t\tVirtual memory (bytes) snapshot=3520536576\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tMapper\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=31\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=26\n",
      "16/06/02 19:56:41 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapperCounter3_2.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducerCounter3_2.py \\\n",
    "-input /user/cloudera/w261/HW3/data3_2/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "quux\t2\r\n",
      "foo\t3\r\n",
      "bar\t1\r\n",
      "labs\t1\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper and Reducer based WordCount (i.e., no combiners used anywhere)  using user defined Counters to count up how many time the mapper and reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapper3_2_b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapper3_2_b.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for mapper for W261 HW3.0.b\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def map_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Mapper,Calls,1\\n\")\n",
    "\n",
    "def map_function(record):\n",
    "    record = record.strip()\n",
    "    record_parameters = record.split(\",\")\n",
    "    issue = record_parameters[3]\n",
    "    issue_words = issue.split()\n",
    "    for word in issue_words:\n",
    "        count = 1\n",
    "        print \"%s\\t%d\" % (word, int(count))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    map_initialize()\n",
    "    for line in sys.stdin:\n",
    "        map_function(line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapper3_2_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!cat ./mapper3_2_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing reducer3_2_b.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3_2_b.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for reducer for W261 HW3.2.b\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def reduce_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def reduce_function(record,current_word,current_count):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "    return (current_word, current_count)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #reduce_initialize()\n",
    "    word = reduce_initialize()\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, current_count = reduce_function(line, word, current_count)\n",
    "    print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer3_2_b.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: `/user/cloudera/w261/HW3/output-3_2_b': No such file or directory\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8582929186455445908.jar tmpDir=null\n",
      "16/06/02 20:02:46 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:02:46 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:02:47 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 20:02:47 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 20:02:48 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0044\n",
      "16/06/02 20:02:48 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0044\n",
      "16/06/02 20:02:48 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0044/\n",
      "16/06/02 20:02:48 INFO mapreduce.Job: Running job: job_1464465911163_0044\n",
      "16/06/02 20:02:58 INFO mapreduce.Job: Job job_1464465911163_0044 running in uber mode : false\n",
      "16/06/02 20:02:58 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 20:03:12 INFO mapreduce.Job:  map 75% reduce 0%\n",
      "16/06/02 20:03:13 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 20:03:31 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/06/02 20:03:32 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 20:03:52 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 20:03:54 INFO mapreduce.Job: Job job_1464465911163_0044 completed successfully\n",
      "16/06/02 20:03:54 INFO mapreduce.Job: Counters: 51\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1231531\n",
      "\t\tFILE: Number of bytes written=2223549\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50906621\n",
      "\t\tHDFS: Number of bytes written=2295\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1608960\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=9226112\n",
      "\t\tTotal time spent by all map tasks (ms)=12570\n",
      "\t\tTotal time spent by all reduce tasks (ms)=72079\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12570\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=72079\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1608960\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=9226112\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=9428893\n",
      "\t\tMap output materialized bytes=541620\n",
      "\t\tInput split bytes=135\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=541620\n",
      "\t\tReduce input records=978634\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=2935902\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=338\n",
      "\t\tCPU time spent (ms)=11350\n",
      "\t\tPhysical memory (bytes) snapshot=638361600\n",
      "\t\tVirtual memory (bytes) snapshot=3522072576\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tMapper\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2295\n",
      "16/06/02 20:03:54 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_b\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_b\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapper3_2_b.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_b.py \\\n",
    "-input /user/cloudera/w261/HW3/data/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 5 items\n",
      "-rw-r--r--   1 cloudera cloudera          0 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/_SUCCESS\n",
      "-rw-r--r--   1 cloudera cloudera        491 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00000\n",
      "-rw-r--r--   1 cloudera cloudera        661 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00001\n",
      "-rw-r--r--   1 cloudera cloudera        548 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00002\n",
      "-rw-r--r--   1 cloudera cloudera        595 2016-06-02 20:03 /user/cloudera/w261/HW3/output-3_2_b/part-00003\n",
      "\"Account\t16205\n",
      "Account\t350\n",
      "Applied\t139\n",
      "Can't\t1999\n",
      "Cash\t240\n",
      "Cont'd\t11848\n",
      "Debt\t1343\n",
      "Delinquent\t1061\n",
      "I\t925\n",
      "Incorrect\t29069\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "cat: Unable to write to output stream.\n",
      "\"Account\t16205\n",
      "Account\t350\n",
      "Applied\t139\n",
      "Can't\t1999\n",
      "Cash\t240\n",
      "Cont'd\t11848\n",
      "Debt\t1343\n",
      "Delinquent\t1061\n",
      "I\t925\n",
      "Incorrect\t29069\n",
      "\"Loan\t107254\n",
      "\"Making/receiving\t3226\n",
      "ATM\t2422\n",
      "Communication\t6920\n",
      "Dealing\t1944\n",
      "Improper\t4309\n",
      "Payment\t92\n",
      "Problems\t9484\n",
      "Shopping\t672\n",
      "Taking\t1242\n",
      "/\t12386\n",
      "APR\t3431\n",
      "Arbitration\t168\n",
      "Bankruptcy\t222\n",
      "Billing\t8158\n",
      "Convenience\t75\n",
      "Credit\t14768\n",
      "Deposits\t10555\n",
      "Disclosure\t5214\n",
      "False\t2508\n",
      "\"Application\t8625\n",
      "Advertising\t1193\n",
      "Application\t243\n",
      "Balance\t597\n",
      "Charged\t878\n",
      "Closing/Cancelling\t2795\n",
      "Collection\t1907\n",
      "Customer\t2734\n",
      "Embezzlement\t3276\n",
      "Forbearance\t350\n",
      "low\t5663\n",
      "money\t139\n",
      "opening\t16205\n",
      "pay\t3821\n",
      "practices\t1003\n",
      "report/credit\t4357\n",
      "sharing\t2832\n",
      "stop\t131\n",
      "when\t4095\n",
      "with\t1944\n",
      "repay\t1647\n",
      "service\t1518\n",
      "the\t6248\n",
      "to\t8401\n",
      "transfer\t597\n",
      "unable\t3821\n",
      "verification\t5214\n",
      "was\t274\n",
      "wrong\t71\n",
      "your\t3844\n",
      "plans\t350\n",
      "rate\t3431\n",
      "report\t30546\n",
      "reporting\t6559\n",
      "scam\t566\n",
      "score\t4357\n",
      "servicing\t36767\n",
      "statements\t2508\n",
      "transaction\t387\n",
      "withdrawals\t10555\n",
      "received\t98\n",
      "relations\t1367\n",
      "representation\t2508\n",
      "servicer\t1944\n",
      "statement\t1220\n",
      "tactics\t6920\n",
      "terms\t350\n",
      "theft\t3276\n",
      "use\t1477\n",
      "you\t3821\n",
      "low\t5663\n",
      "money\t139\n",
      "opening\t16205\n",
      "pay\t3821\n",
      "practices\t1003\n",
      "report/credit\t4357\n",
      "sharing\t2832\n",
      "stop\t131\n",
      "when\t4095\n",
      "with\t1944\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /user/cloudera/w261/HW3/output-3_2_b\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_b/* | head\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_b/* | tail"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform a word count analysis of the Issue column of the Consumer Complaints  Dataset using a Mapper, Reducer, and standalone combiner (i.e., not an in-memory combiner) based WordCount using user defined Counters to count up how many time the mapper, combiner, reducer are called. What is the value of your user defined Mapper Counter, and Reducer Counter after completing your word count job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER:__\n",
    "\n",
    "*Note that I am using the mapper and reducer code from 3.2.b as it does not change for this exercise.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combiner3_2_c.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combiner3_2_c.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "This is the combiner code for HW3.2.c\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "def combine_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Combiner,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def combine_function(record,current_word,current_count):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "    if current_word == \"\" or current_word == word:\n",
    "        current_word = word\n",
    "        current_count = current_count + count\n",
    "    else:\n",
    "        print \"%s\\t%d\" % (current_word, current_count)\n",
    "        current_word = word\n",
    "        current_count = count\n",
    "    return (current_word, current_count)\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    word = combine_initialize()\n",
    "    current_count = 0\n",
    "    for line in sys.stdin:\n",
    "        word, current_count = combine_function(line, word, current_count)\n",
    "    print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combiner3_2_c.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/02 20:35:38 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_2_c' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_2_c\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob1137443623450988204.jar tmpDir=null\n",
      "16/06/02 20:35:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:35:43 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/02 20:35:44 INFO mapred.FileInputFormat: Total input paths to process : 1\n",
      "16/06/02 20:35:44 INFO mapreduce.JobSubmitter: number of splits:1\n",
      "16/06/02 20:35:44 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1464465911163_0046\n",
      "16/06/02 20:35:45 INFO impl.YarnClientImpl: Submitted application application_1464465911163_0046\n",
      "16/06/02 20:35:45 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1464465911163_0046/\n",
      "16/06/02 20:35:45 INFO mapreduce.Job: Running job: job_1464465911163_0046\n",
      "16/06/02 20:35:53 INFO mapreduce.Job: Job job_1464465911163_0046 running in uber mode : false\n",
      "16/06/02 20:35:53 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/02 20:36:07 INFO mapreduce.Job:  map 67% reduce 0%\n",
      "16/06/02 20:36:09 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/02 20:36:24 INFO mapreduce.Job:  map 100% reduce 25%\n",
      "16/06/02 20:36:25 INFO mapreduce.Job:  map 100% reduce 50%\n",
      "16/06/02 20:36:38 INFO mapreduce.Job:  map 100% reduce 75%\n",
      "16/06/02 20:36:39 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/02 20:36:39 INFO mapreduce.Job: Job job_1464465911163_0046 completed successfully\n",
      "16/06/02 20:36:39 INFO mapreduce.Job: Counters: 52\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=15720\n",
      "\t\tFILE: Number of bytes written=610617\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=50906621\n",
      "\t\tHDFS: Number of bytes written=2295\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=8\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tLaunched reduce tasks=4\n",
      "\t\tData-local map tasks=1\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=1651200\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=6374272\n",
      "\t\tTotal time spent by all map tasks (ms)=12900\n",
      "\t\tTotal time spent by all reduce tasks (ms)=49799\n",
      "\t\tTotal vcore-seconds taken by all map tasks=12900\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=49799\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1651200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=6374272\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=312913\n",
      "\t\tMap output records=978634\n",
      "\t\tMap output bytes=9428893\n",
      "\t\tMap output materialized bytes=3661\n",
      "\t\tInput split bytes=135\n",
      "\t\tCombine input records=978634\n",
      "\t\tCombine output records=334\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=3661\n",
      "\t\tReduce input records=334\n",
      "\t\tReduce output records=182\n",
      "\t\tSpilled Records=1002\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=278\n",
      "\t\tCPU time spent (ms)=6970\n",
      "\t\tPhysical memory (bytes) snapshot=632274944\n",
      "\t\tVirtual memory (bytes) snapshot=3523854336\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tCombiner\n",
      "\t\tCalls=8\n",
      "\tMapper\n",
      "\t\tCalls=1\n",
      "\tReducer\n",
      "\t\tCalls=4\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=50906486\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2295\n",
      "16/06/02 20:36:39 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_c\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_c\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-numReduceTasks 4 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/mapper3_2_b.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_b.py \\\n",
    "-combiner /home/cloudera/w261/HW3/src/combiner3_2_c.py \\\n",
    "-input /user/cloudera/w261/HW3/data/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_c"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__Using a single reducer:__ What are the top 50 most frequent terms in your word count analysis? Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items).\n",
    "\n",
    "__ANSWER:__\n",
    "\n",
    "_Note: I am taking as input to this exercise the results from the word count analysis in the previous exercise._"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting identityMapper.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile identityMapper.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "#need to specify an identity mapper in order to trigger the sort in Hadoop\n",
    "#\n",
    "#Can also use the identtiy mapper:  -mapper /bin/cat \\  \n",
    "#\n",
    "import sys\n",
    "\n",
    "# input comes from STDIN\n",
    "for line in sys.stdin:\n",
    "    # minimal feature engineering: setting tokens to all be lower case\n",
    "    # for the purposes of having case insensitive secondary sort\n",
    "    line = line.lower()\n",
    "    print line.strip()\n",
    "    #print output to stadout/screen \n",
    "    #sys.stderr.write(\"reporter:status:processing line [%s]\" % (line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x identityMapper.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducer3_2_d.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducer3_2_d.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Code for reducer for W261 HW3.2.b\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "from operator import itemgetter\n",
    "\n",
    "def reduce_initialize():\n",
    "    sys.stderr.write(\"reporter:counter:Reducer,Calls,1\\n\")\n",
    "    init_word = \"\"\n",
    "    return init_word\n",
    "\n",
    "def reduce_function(record, current_total_terms):\n",
    "    record = record.strip()\n",
    "    word_count_pair = record.split(\"\\t\")\n",
    "    word = word_count_pair[0]\n",
    "    count = int(word_count_pair[1])\n",
    "#     if current_word == \"\" or current_word == word:\n",
    "#         current_word = word\n",
    "#         current_count = current_count + count\n",
    "#     else:\n",
    "#         #print \"%s\\t%d\" % (current_word, current_count)\n",
    "#         output_array.extend([{\"count\":current_count,\"word\":word}])\n",
    "#         current_word = word\n",
    "#         current_count = count\n",
    "    current_total_terms = current_total_terms + count\n",
    "    return (word, count, current_total_terms)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    #reduce_initialize()\n",
    "    output_array = []\n",
    "    word = reduce_initialize()\n",
    "#     current_count = 0\n",
    "#     total_words = 0\n",
    "    total_terms = 0\n",
    "    for line in sys.stdin:\n",
    "        word, word_count, total_terms = reduce_function(line, total_terms)\n",
    "        output_array.extend([{\"word\":word,\"count\":word_count}])\n",
    "#         total_words = total_words + int(current_count)\n",
    "    #print \"%s\\t%d\" % (word, current_count)\n",
    "#     output_array = sorted(output_array, key=lambda k: (k[\"count\"],k[\"word\"]), reverse=True)\n",
    "    #output_array = sorted(output_array, key= itemgetter(\"word\"))\n",
    "    #output_array = sorted(output_array, key=itemgetter(\"count\"), reverse=True)\n",
    "#     print output_array\n",
    "    print \"\\n########################################################\"\n",
    "    print \"### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "    print \"########################################################\\n\"\n",
    "    for i in range(0,50):\n",
    "        print \"%s\\t%d\\t%.10f\" % (output_array[i][\"word\"], output_array[i][\"count\"], float(output_array[i][\"count\"]/float(total_terms)))\n",
    "    #output_array = sorted(output_array, key=itemgetter(\"word\"))\n",
    "    #output_array = sorted(output_array, key=itemgetter(\"count\"))\n",
    "#     output_array = sorted(output_array, key=lambda k: (k[\"count\"],k[\"word\"]))\n",
    "    print \"\\n###########################################################\"\n",
    "    print \"### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "    print \"###########################################################\\n\"\n",
    "    for i in range(1,11):\n",
    "        i = -1*i\n",
    "        print \"%s\\t%d\\t%.10f\" % (output_array[i][\"word\"], output_array[i][\"count\"], float(output_array[i][\"count\"]/float(total_terms)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducer3_2_d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer,Calls,1\n",
      "\n",
      "########################################################\n",
      "### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\n",
      "########################################################\n",
      "\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "on\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "incorrect\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n",
      "and\t16448\t0.0168071005\n",
      "opening\t16205\t0.0165587952\n",
      "\"account\t16205\t0.0165587952\n",
      "credit\t14768\t0.0150904220\n",
      "health\t12545\t0.0128188884\n",
      "club\t12545\t0.0128188884\n",
      "/\t12386\t0.0126564170\n",
      "not\t12353\t0.0126226965\n",
      "loan\t12237\t0.0125041640\n",
      "owed\t11848\t0.0121066711\n",
      "cont'd\t11848\t0.0121066711\n",
      "collect\t11848\t0.0121066711\n",
      "attempts\t11848\t0.0121066711\n",
      "of\t10885\t0.0111226465\n",
      "my\t10731\t0.0109652843\n",
      "withdrawals\t10555\t0.0107854417\n",
      "deposits\t10555\t0.0107854417\n",
      "problems\t9484\t0.0096910592\n",
      "\"application\t8625\t0.0088133051\n",
      "to\t8401\t0.0085844146\n",
      "billing\t8158\t0.0083361093\n",
      "other\t7886\t0.0080581709\n",
      "disputes\t6938\t0.0070894737\n",
      "tactics\t6920\t0.0070710807\n",
      "communication\t6920\t0.0070710807\n",
      "reporting\t6559\t0.0067021992\n",
      "lease\t6337\t0.0064753524\n",
      "the\t6248\t0.0063844093\n",
      "low\t5663\t0.0057866373\n",
      "funds\t5663\t0.0057866373\n",
      "caused\t5663\t0.0057866373\n",
      "by\t5663\t0.0057866373\n",
      "being\t5663\t0.0057866373\n",
      "process\t5505\t0.0056251878\n",
      "verification\t5214\t0.0053278345\n",
      "disclosure\t5214\t0.0053278345\n",
      "managing\t5006\t0.0051152934\n",
      "investigation\t4858\t0.0049640622\n",
      "company's\t4858\t0.0049640622\n",
      "card\t4405\t0.0045011720\n",
      "\n",
      "###########################################################\n",
      "### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\n",
      "###########################################################\n",
      "\n",
      "issue\t1\t0.0000010218\n",
      "disclosures\t64\t0.0000653973\n",
      "incorrect/missing\t64\t0.0000653973\n",
      "amt\t71\t0.0000725501\n",
      "day\t71\t0.0000725501\n",
      "wrong\t71\t0.0000725501\n",
      "checks\t75\t0.0000766374\n",
      "convenience\t75\t0.0000766374\n",
      "credited\t92\t0.0000940086\n",
      "payment\t92\t0.0000940086\n"
     ]
    }
   ],
   "source": [
    "# Testing code for HW3.2.d\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./identityMapper.py \\\n",
    "| sort -k2,2nr -k1,1r | ./reducer3_2_d.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "16/06/05 06:45:41 INFO fs.TrashPolicyDefault: Moved: 'hdfs://quickstart.cloudera:8020/user/cloudera/w261/HW3/output-3_2_d' to trash at: hdfs://quickstart.cloudera:8020/user/cloudera/.Trash/Current/user/cloudera/w261/HW3/output-3_2_d1465134341181\n",
      "packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4013988832558737341.jar tmpDir=null\n",
      "16/06/05 06:45:44 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 06:45:44 INFO client.RMProxy: Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "16/06/05 06:45:45 INFO mapred.FileInputFormat: Total input paths to process : 4\n",
      "16/06/05 06:45:45 INFO mapreduce.JobSubmitter: number of splits:4\n",
      "16/06/05 06:45:46 INFO mapreduce.JobSubmitter: Submitting tokens for job: job_1465099569246_0010\n",
      "16/06/05 06:45:46 INFO impl.YarnClientImpl: Submitted application application_1465099569246_0010\n",
      "16/06/05 06:45:46 INFO mapreduce.Job: The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465099569246_0010/\n",
      "16/06/05 06:45:46 INFO mapreduce.Job: Running job: job_1465099569246_0010\n",
      "16/06/05 06:45:55 INFO mapreduce.Job: Job job_1465099569246_0010 running in uber mode : false\n",
      "16/06/05 06:45:55 INFO mapreduce.Job:  map 0% reduce 0%\n",
      "16/06/05 06:46:09 INFO mapreduce.Job:  map 25% reduce 0%\n",
      "16/06/05 06:46:10 INFO mapreduce.Job:  map 50% reduce 0%\n",
      "16/06/05 06:46:24 INFO mapreduce.Job:  map 100% reduce 0%\n",
      "16/06/05 06:46:34 INFO mapreduce.Job:  map 100% reduce 100%\n",
      "16/06/05 06:46:34 INFO mapreduce.Job: Job job_1465099569246_0010 completed successfully\n",
      "16/06/05 06:46:34 INFO mapreduce.Job: Counters: 50\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=2218\n",
      "\t\tFILE: Number of bytes written=605083\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=2815\n",
      "\t\tHDFS: Number of bytes written=1907\n",
      "\t\tHDFS: Number of read operations=15\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=4\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tData-local map tasks=4\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6652544\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=878208\n",
      "\t\tTotal time spent by all map tasks (ms)=51973\n",
      "\t\tTotal time spent by all reduce tasks (ms)=6861\n",
      "\t\tTotal vcore-seconds taken by all map tasks=51973\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=6861\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6652544\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=878208\n",
      "\tMap-Reduce Framework\n",
      "\t\tMap input records=182\n",
      "\t\tMap output records=182\n",
      "\t\tMap output bytes=2477\n",
      "\t\tMap output materialized bytes=2550\n",
      "\t\tInput split bytes=520\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tReduce input groups=182\n",
      "\t\tReduce shuffle bytes=2550\n",
      "\t\tReduce input records=182\n",
      "\t\tReduce output records=70\n",
      "\t\tSpilled Records=364\n",
      "\t\tShuffled Maps =4\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tMerged Map outputs=4\n",
      "\t\tGC time elapsed (ms)=305\n",
      "\t\tCPU time spent (ms)=3000\n",
      "\t\tPhysical memory (bytes) snapshot=664748032\n",
      "\t\tVirtual memory (bytes) snapshot=3491516416\n",
      "\t\tTotal committed heap usage (bytes)=253624320\n",
      "\tReducer\n",
      "\t\tCalls=1\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2295\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1907\n",
      "16/06/05 06:46:34 INFO streaming.StreamJob: Output directory: /user/cloudera/w261/HW3/output-3_2_d\n",
      "\t\n",
      "########################################################\t\n",
      "### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\t\n",
      "########################################################\t\n",
      "\t\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "incorrect\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "on\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n",
      "and\t16448\t0.0168071005\n",
      "\"account\t16205\t0.0165587952\n",
      "opening\t16205\t0.0165587952\n",
      "credit\t14768\t0.0150904220\n",
      "club\t12545\t0.0128188884\n",
      "health\t12545\t0.0128188884\n",
      "/\t12386\t0.0126564170\n",
      "not\t12353\t0.0126226965\n",
      "loan\t12237\t0.0125041640\n",
      "attempts\t11848\t0.0121066711\n",
      "collect\t11848\t0.0121066711\n",
      "cont'd\t11848\t0.0121066711\n",
      "owed\t11848\t0.0121066711\n",
      "of\t10885\t0.0111226465\n",
      "my\t10731\t0.0109652843\n",
      "deposits\t10555\t0.0107854417\n",
      "withdrawals\t10555\t0.0107854417\n",
      "problems\t9484\t0.0096910592\n",
      "\"application\t8625\t0.0088133051\n",
      "to\t8401\t0.0085844146\n",
      "billing\t8158\t0.0083361093\n",
      "other\t7886\t0.0080581709\n",
      "disputes\t6938\t0.0070894737\n",
      "communication\t6920\t0.0070710807\n",
      "tactics\t6920\t0.0070710807\n",
      "reporting\t6559\t0.0067021992\n",
      "lease\t6337\t0.0064753524\n",
      "the\t6248\t0.0063844093\n",
      "being\t5663\t0.0057866373\n",
      "by\t5663\t0.0057866373\n",
      "caused\t5663\t0.0057866373\n",
      "funds\t5663\t0.0057866373\n",
      "low\t5663\t0.0057866373\n",
      "process\t5505\t0.0056251878\n",
      "disclosure\t5214\t0.0053278345\n",
      "verification\t5214\t0.0053278345\n",
      "managing\t5006\t0.0051152934\n",
      "company's\t4858\t0.0049640622\n",
      "investigation\t4858\t0.0049640622\n",
      "card\t4405\t0.0045011720\n",
      "\t\n",
      "###########################################################\t\n",
      "### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\t\n",
      "###########################################################\t\n",
      "\t\n",
      "issue\t1\t0.0000010218\n",
      "incorrect/missing\t64\t0.0000653973\n",
      "disclosures\t64\t0.0000653973\n",
      "wrong\t71\t0.0000725501\n",
      "day\t71\t0.0000725501\n",
      "amt\t71\t0.0000725501\n",
      "convenience\t75\t0.0000766374\n",
      "checks\t75\t0.0000766374\n",
      "payment\t92\t0.0000940086\n",
      "credited\t92\t0.0000940086\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -rm -r /user/cloudera/w261/HW3/output-3_2_d\n",
    "!hadoop jar /usr/lib/hadoop-0.20-mapreduce/contrib/streaming/hadoop-streaming-mr1.jar \\\n",
    "-D stream.num.map.output.key.fields=2 \\\n",
    "-D mapreduce.job.maps=1 \\\n",
    "-D mapreduce.job.output.key.comparator.class=\\\n",
    "org.apache.hadoop.mapred.lib.KeyFieldBasedComparator \\\n",
    "-D mapreduce.partition.keycomparator.options=\"-k2,2nr -k1,1\" \\\n",
    "-numReduceTasks 1 \\\n",
    "-mapper /home/cloudera/w261/HW3/src/identityMapper.py \\\n",
    "-reducer /home/cloudera/w261/HW3/src/reducer3_2_d.py \\\n",
    "-partitioner org.apache.hadoop.mapred.lib.KeyFieldBasedPartitioner \\\n",
    "-input /user/cloudera/w261/HW3/output-3_2_c/* \\\n",
    "-output /user/cloudera/w261/HW3/output-3_2_d\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_d/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.2.1 Using two reducers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using two reducers: What are the top 50 most frequent terms in your word count analysis?\n",
    "\n",
    "Present the top 50 terms and their frequency and their relative frequency. Present the top 50 terms and their frequency and their relative frequency. If there are ties please sort the tokens in alphanumeric/string order. Present bottom 10 tokens (least frequent items). Please use a combiner."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting mapperWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile mapperWithPartitionTable.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "total_term_count = 0\n",
    "values = []\n",
    "# using the word count output to determine the partitions\n",
    "f = open('word_count_output','r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    values.extend([int(value)])\n",
    "\n",
    "values = sorted(values)\n",
    "median = int(values[(len(values) / 2)])\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    # minimal feature engineering: setting tokens to all be lower case\n",
    "    # for the purposes of having case insensitive secondary sort\n",
    "    line = line.lower()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    value = int(value)\n",
    "    if value <= median:\n",
    "        print \"group1\\t%s\" % (line)\n",
    "    else:\n",
    "        print \"group2\\t%s\" % (line)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x mapperWithPartitionTable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group2\t\"account\t16205\r\n",
      "group1\taccount\t350\r\n",
      "group1\tapplied\t139\r\n",
      "group1\tcan't\t1999\r\n",
      "group1\tcash\t240\r\n",
      "group2\tcont'd\t11848\r\n",
      "group1\tdebt\t1343\r\n",
      "group1\tdelinquent\t1061\r\n",
      "group1\ti\t925\r\n",
      "group2\tincorrect\t29069\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./mapperWithPartitionTable.py | head\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting combineWithPartitionTable.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile combineWithPartitionTable.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "This is the combiner code for HW3.2.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "# NOTE:\n",
    "# The exercise mentions that a combiner should be used, but for the\n",
    "# purposes of sorting the output of the word count analysis\n",
    "# combining is not required...\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    sys.stderr.write(\"reporter:counter:Combiner,Calls,1\\n\")\n",
    "    output_array = []\n",
    "    cur_count = 0\n",
    "    for line in sys.stdin:\n",
    "        line = line.strip()\n",
    "        print line\n",
    "#         group, key, value = line.split(\"\\t\")\n",
    "#         output_array.extend([{\"group\":group,\"word\":key,\"count\":int(value)}])\n",
    "#     for line in output_array:\n",
    "#         print \"%s\\t%s\\t%d\\t%d\" % (line[\"group\"], line[\"word\"], line[\"count\"], cur_count)\n",
    "#         word, current_count = combine_function(line, word, current_count)\n",
    "#     print \"%s\\t%d\" % (word, current_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x combineWithPartitionTable.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Combiner,Calls,1\n",
      "group2\t\"loan\t107254\n",
      "group2\tmodification\t70487\n",
      "group2\tservicing\t36767\n",
      "group2\tcredit\t36126\n",
      "group2\treport\t30546\n",
      "group2\tincorrect\t29069\n",
      "group2\tinformation\t29069\n",
      "group2\ton\t29069\n",
      "group2\tor\t22533\n",
      "group2\tdebt\t17966\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./mapperWithPartitionTable.py \\\n",
    "| ./combineWithPartitionTable.py | sort -k3,3nr -k2,2 | head\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting reducerWithPartitionKey.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile reducerWithPartitionKey.py\n",
    "#!/usr/bin/python\n",
    "\n",
    "import sys\n",
    "\n",
    "cur_key = None\n",
    "cur_count = 0\n",
    "total_tokens = 0\n",
    "output_array = []\n",
    "sys.stderr.write(\"reporter:counter:Reducer Counters,Calls,1\\n\")\n",
    "\n",
    "# using the word count output to determine the total tokens\n",
    "# for relative frequency\n",
    "f = open('word_count_output','r')\n",
    "for line in f:\n",
    "    line = line.strip()\n",
    "    key, value = line.split(\"\\t\")\n",
    "    total_tokens += int(value)\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    group, key, value = line.split(\"\\t\") #one minor modification to process the parition key. I.e., drop it\n",
    "    value = int(value)\n",
    "#     sys.stderr.write(\"LINE: %s\" %line)\n",
    "#     sys.stderr.write(\"GROUP: %s\" %group)\n",
    "#     sys.stderr.write(\"KEY: %s\" %key)\n",
    "#     sys.stderr.write(\"VALUE: %s\" %value)\n",
    "#     sys.stderr.write(\"TEMP_COUNT: %s\" %temp_count)\n",
    "#     output_array.extend([{\"word\":key,\"count\":int(value)}])\n",
    "#     print output_array\n",
    "    print \"%s\\t%d\\t%.10f\" %(key,value,(float(value)/float(total_tokens)))\n",
    "# for record in output_array:\n",
    "#     print \"%s\\t%d\\t%.10f\" % (record[\"word\"], record[\"count\"], (record[\"count\"]/float(cur_count)))\n",
    "# print \"\\n########################################################\"\n",
    "# print \"### TOP 50 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "# print \"########################################################\\n\"\n",
    "# for i in range(0,50):\n",
    "#     print \"%s\\t%d\\t%.10f\" % (output_array[i][\"word\"], output_array[i][\"count\"], float(output_array[i][\"count\"]/float(cur_count)))\n",
    "\n",
    "# print \"\\n###########################################################\"\n",
    "# print \"### BOTTOM 10 TERMS BY FREQUENCY WITH RELATIVE FREQUENCY###\"\n",
    "# print \"###########################################################\\n\"\n",
    "# for i in range(1,11):\n",
    "#     i = -1*i\n",
    "#     print \"%s\\t%d\\t%.10f\" % (output_array[i][\"word\"], output_array[i][\"count\"], float(output_array[i][\"count\"]/float(cur_count)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x reducerWithPartitionKey.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "reporter:counter:Reducer Counters,Calls,1\n",
      "reporter:counter:Combiner,Calls,1\n",
      "\"loan\t107254\t0.1095956200\n",
      "modification\t70487\t0.0720259055\n",
      "servicing\t36767\t0.0375697145\n",
      "credit\t36126\t0.0369147199\n",
      "report\t30546\t0.0312128947\n",
      "incorrect\t29069\t0.0297036481\n",
      "information\t29069\t0.0297036481\n",
      "on\t29069\t0.0297036481\n",
      "or\t22533\t0.0230249511\n",
      "debt\t17966\t0.0183582422\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/* > word_count_output\n",
    "!hdfs dfs -cat /user/cloudera/w261/HW3/output-3_2_c/*  \\\n",
    "| ./mapperWithPartitionTable.py \\\n",
    "| ./combineWithPartitionTable.py \\\n",
    "| sort -k3,3nr -k2,2 | ./reducerWithPartitionKey.py | head\n",
    "!rm word_count_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW3.3 Shopping Cart Analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Product Recommendations: The action or practice of selling additional products or services \n",
    "to existing customers is called cross-selling. Giving product recommendation is \n",
    "one of the examples of cross-selling that are frequently used by online retailers. \n",
    "One simple method to give product recommendations is to recommend products that are frequently\n",
    "browsed together by the customers.\n",
    "\t\n",
    "For this homework use the online browsing behavior dataset located at: \n",
    "\n",
    "       https://www.dropbox.com/s/zlfyiwa70poqg74/ProductPurchaseData.txt?dl=0\n",
    "\n",
    "Each line in this dataset represents a browsing session of a customer. \n",
    "On each line, each string of 8 characters represents the id of an item browsed during that session. \n",
    "The items are separated by spaces.\n",
    "\n",
    "Here are the first few lines of the ProductPurchaseData \n",
    "FRO11987 ELE17451 ELE89019 SNA90258 GRO99222 \n",
    "GRO99222 GRO12298 FRO12685 ELE91550 SNA11465 ELE26917 ELE52966 FRO90334 SNA30755 ELE17451 FRO84225 SNA80192 \n",
    "ELE17451 GRO73461 DAI22896 SNA99873 FRO86643 \n",
    "ELE17451 ELE37798 FRO86643 GRO56989 ELE23393 SNA11465 \n",
    "ELE17451 SNA69641 FRO86643 FRO78087 SNA11465 GRO39357 ELE28573 ELE11375 DAI54444 \n",
    "\n",
    "\n",
    "Do some exploratory data analysis of this dataset guided by the following questions:. \n",
    "\n",
    "How many unique items are available from this supplier?\n",
    "\n",
    "Using a single reducer: Report your findings such as number of unique products; largest basket; report the top 50 most frequently purchased items,  their frequency,  and their relative frequency (break ties by sorting the products alphabetical order) etc. using Hadoop Map-Reduce. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
