{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MIDS UC Berkeley, Machine Learning at Scale \n",
    " \n",
    "__W261-1__ Summer 2016    \n",
    "__Week 7__: SSSP    \n",
    "\n",
    "__Name__   \n",
    "name@ischool.berkeley.edu  \n",
    "\n",
    "July 1, 2016   \n",
    "\n",
    "***"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General Description\n",
    "\n",
    "In this assignment you will explore networks and develop MRJob code for \n",
    "finding shortest path graph distances. To build up to large data \n",
    "you will develop your code on some very simple, toy networks.\n",
    "After this you will take your developed code forward and modify it and \n",
    "apply it to two larger datasets (performing EDA along the way).\n",
    "\n",
    "#### Undirected toy network dataset\n",
    "\n",
    "\n",
    "In an undirected network all links are symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' both of the links:\n",
    "\n",
    "A -> B and B -> A\n",
    "\n",
    "will exist. \n",
    "\n",
    "The toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "(node) \\t (dictionary of links)\n",
    "\n",
    "on AWS/Dropbox via the url:\n",
    "\n",
    "s3://ucb-mids-mls-networks/undirected_toy.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "The Data folder is in: https://db.tt/Kxu48mL1)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted).\n",
    "\n",
    "\n",
    "#### Directed toy network dataset\n",
    "\n",
    "In a directed network all links are not necessarily symmetric, \n",
    "i.e., for a pair of nodes 'A' and 'B,' it is possible for only one of:\n",
    "\n",
    "A -> B or B -> A\n",
    "\n",
    "to exist. \n",
    "\n",
    "These toy data are available in a sparse (stripes) representation:\n",
    "\n",
    "(node) \\t (dictionary of links)\n",
    "\n",
    "on AWS/Dropbox via the url:\n",
    "\n",
    "s3://ucb-mids-mls-networks/directed_toy.txt\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name\n",
    "(On Dropbox https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.0: Shortest path graph distances (toy networks)\n",
    "\n",
    "In this part of your assignment you will develop the base of your code for the week.\n",
    "\n",
    "Write MRJob classes to find shortest path graph distances, as described in the lectures. In addition to finding the distances, your code should also output a distance-minimizing path between the source and target.\n",
    "Work locally for this part of the assignment, and use both of the undirected and directed toy networks.\n",
    "\n",
    "To proof you code's function, run the following jobs\n",
    "\n",
    "- shortest path in the undirected network from node 1 to node 4\n",
    "Solution: 1,5,4. NOTE: There is another shortest path also (HINT: 1->5->4)! Either will suffice (you will find this also in the remaining problems. E.g., 7.2 and 7.4.\n",
    " \n",
    "\n",
    "- shortest path in the directed network from node 1 to node 5\n",
    "Solution: 1,2,4,5\n",
    "\n",
    "and report your output---make sure it is correct!\n",
    "\n",
    "#### Main dataset 1: NLTK synonyms\n",
    "\n",
    "In the next part of this assignment you will explore a network derived from the NLTK synonym database used for evaluation in HW 5. At a high level, this network is undirected, defined so that there exists link between two nodes/words if the pair or words are a synonym. These data may be found at the location:\n",
    "\n",
    "s3://ucb-mids-mls-networks/synNet/synNet.txt\n",
    "\n",
    "s3://ucb-mids-mls-networks/synNet/indices.txt\n",
    "\n",
    "On under the Data Subfolder for HW7 on Dropbox with the same file names\n",
    "\n",
    "where synNet.txt contains a sparse representation of the network:\n",
    "\n",
    "(index) \\t (dictionary of links)\n",
    "\n",
    "in indexed form, and indices.txt contains a lookup list\n",
    "\n",
    "(word) \\t (index)\n",
    "\n",
    "of indices and words. This network is small enough for you to explore and run\n",
    "scripts locally, but will also be good for a systems test (for later) on AWS.\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values \n",
    "(here, all weights are 1, i.e., the network is unweighted)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "Below we have the code that returns the shortest path and its length for both the directed and undirected toy graph examples.\n",
    "\n",
    "\n",
    "||Length of shortest path|Shortest path|\n",
    "|---|---|---|\n",
    "|__Undirected graph (1 to 4)__|2|1-5-4|\n",
    "|__Directed graph (1 to 5)__|3|1->2->4->5|\n",
    "\n",
    "Additionally, I wrote a mrjob that preprocesses the graph such that it may be traversed with the distributed SSSP algorithm. The resulting graph format is:\n",
    "\n",
    "    node \\t [comma_separated_out-edges, distance_from_source, node_state, path_from_source]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting adjacencyListToGraph.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile adjacencyListToGraph.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRadjacencyListToGraph(MRJob):\n",
    "    \n",
    "    ## setting a default source node\n",
    "    source_node = '7827'\n",
    "    \n",
    "    ## function permits us to pass an arbitrary source node\n",
    "    ## to our mrjob\n",
    "    def configure_options(self):\n",
    "        super(MRadjacencyListToGraph, self).configure_options()\n",
    "        self.add_passthrough_option('--source-node', type='int', default=10, help='...')\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                      mapper=self.mapper\n",
    "                      )]\n",
    "    ## setting the source node passed into the job\n",
    "    def mapper_init(self):\n",
    "        self.source_node = str(self.options.source_node)\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, neighbors = line.split(\"\\t\")\n",
    "        neighbors = ast.literal_eval(neighbors)\n",
    "        neighbors_temp = \"\"\n",
    "        ## process each neighbor to make neighbors array\n",
    "        for k in neighbors.keys():\n",
    "            if neighbors_temp == \"\":\n",
    "                neighbors_temp = neighbors_temp + k\n",
    "            else:\n",
    "                neighbors_temp = neighbors_temp + \",\" + k\n",
    "        if node == self.source_node:\n",
    "            yield node, (neighbors_temp, 0, \"Q\",self.source_node)\n",
    "        else:\n",
    "            yield node, (neighbors_temp, sys.maxint, \"U\", \"-\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRadjacencyListToGraph.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x adjacencyListToGraph.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/adjacencyListToGraph.cloudera.20160710.161931.661966\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/adjacencyListToGraph.cloudera.20160710.161931.661966/output...\n",
      "Removing temp directory /tmp/adjacencyListToGraph.cloudera.20160710.161931.661966...\n",
      "\"4\"\t[\"2,5\", 9223372036854775807, \"U\", \"-\"]\n",
      "\"5\"\t[\"1,2,4\", 9223372036854775807, \"U\", \"-\"]\n",
      "\"1\"\t[\"2,6\", 0, \"Q\", \"1\"]\n",
      "\"2\"\t[\"1,3,4\", 9223372036854775807, \"U\", \"-\"]\n",
      "\"3\"\t[\"2,4\", 9223372036854775807, \"U\", \"-\"]\n"
     ]
    }
   ],
   "source": [
    "!./adjacencyListToGraph.py ~/w261/HW7/data/directed_toy/directed_toy.txt \\\n",
    "--source-node 1 > initial_directed_graph.txt\n",
    "!cat initial_directed_graph.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/adjacencyListToGraph.cloudera.20160710.161942.936205\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/adjacencyListToGraph.cloudera.20160710.161942.936205/output...\n",
      "Removing temp directory /tmp/adjacencyListToGraph.cloudera.20160710.161942.936205...\n",
      "\"4\"\t[\"3,2,5\", 9223372036854775807, \"U\", \"-\"]\n",
      "\"5\"\t[\"1,2,4\", 9223372036854775807, \"U\", \"-\"]\n",
      "\"1\"\t[\"2,5\", 0, \"Q\", \"1\"]\n",
      "\"2\"\t[\"1,3,5,4\", 9223372036854775807, \"U\", \"-\"]\n",
      "\"3\"\t[\"2,4\", 9223372036854775807, \"U\", \"-\"]\n"
     ]
    }
   ],
   "source": [
    "!./adjacencyListToGraph.py /home/cloudera/w261/HW7/data/undirected_toy/undirected_toy.txt \\\n",
    "--source-node 1 > initial_undirected_graph.txt\n",
    "!cat initial_undirected_graph.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting undirectedShortestPath.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile undirectedShortestPath.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRundirectedShortestPath(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                      mapper=self.mapper,\n",
    "                      reducer=self.reducer\n",
    "                      )]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, payload = line.split(\"\\t\")\n",
    "        payload = ast.literal_eval(payload)\n",
    "        neighbors = payload[0]\n",
    "        distance = int(payload[1])\n",
    "        state = payload[2]\n",
    "        path = payload[3]\n",
    "        neighbors_list = neighbors.split(\",\")\n",
    "        ## If we are processing a node that is in\n",
    "        ## the frontier for the current exploration\n",
    "        ## step of the graph, we emit it as well as\n",
    "        ## its neighbors\n",
    "        if state == \"Q\":\n",
    "            for n in neighbors_list:\n",
    "                n_path = path + \"-\" + n\n",
    "                ## We never want to emit \"no neighbors\"\n",
    "                ## as a neighbor of the current node\n",
    "                if n != \"\":\n",
    "                    yield n, (\"*\", distance + 1, \"Q\", n_path)\n",
    "            ## Again, if by some processing error we arrive at\n",
    "            ## an \"empty node\", we do not want to emit it\n",
    "            if str(node.strip('\"')) != \"\":\n",
    "                yield str(node.strip('\"')), (neighbors, distance, \"V\", path)\n",
    "        ## If the node being processed is not in the frontier\n",
    "        ## we simply emit it unchanged\n",
    "        elif state == \"V\" or state == \"U\":\n",
    "            if str(node.strip('\"')) != \"\":\n",
    "                yield str(node.strip('\"')), (neighbors, distance, state, path)\n",
    "        else:\n",
    "            ## As a safeguard against unexpected behavior we have this\n",
    "            ## statement\n",
    "            print \"PROCESSING ERROR: UNEXPECTED STATE FOR NODE TO BE EMITTED BY MAPPER\"\n",
    "\n",
    "    def reducer(self, key, value):\n",
    "        ## I am holding all the states for a single node (key)\n",
    "        ## in this set\n",
    "        state_set = set()\n",
    "        ## The distances array holds the distance from the node (key)\n",
    "        ## to all other nodes connecting to the node. This is used to\n",
    "        ## calculate the minimum distance\n",
    "        distances = []\n",
    "        min_distance = 0\n",
    "        max_neighbors = \"\"\n",
    "        final_state = \"\"\n",
    "        final_path = \"\"\n",
    "        found_final_state = False\n",
    "        if key != \"\":\n",
    "            for v in value:\n",
    "                neighbors, distance, state, path = v\n",
    "                state_set.add(state)\n",
    "                if path != \"-\":\n",
    "                    ## given that these exercises are for unweighted\n",
    "                    ## graphs, the moment a node is reached from the\n",
    "                    ## source, it has by definition been reached\n",
    "                    ## through the shortest path\n",
    "                    final_path = path\n",
    "                if neighbors != \"*\":\n",
    "                    if max_neighbors == \"\":\n",
    "                        max_neighbors = max_neighbors + neighbors\n",
    "                    else:\n",
    "                        ## This is accessed if we are processing a value\n",
    "                        ## that is the result of an expanded nweighbor\n",
    "                        ## in a mapper\n",
    "                        max_neighbors = max_neighbors + \",\" + neighbors\n",
    "                ## all distances are collected to later obtain\n",
    "                ## the minimum distance\n",
    "                distances.append(int(distance))\n",
    "            if final_path == \"\":\n",
    "                final_path = \"-\"\n",
    "            if \"V\" in state_set:\n",
    "                final_state = \"V\"\n",
    "            elif \"Q\" in state_set:\n",
    "                final_state = \"Q\"\n",
    "            else:\n",
    "                final_state = \"U\"\n",
    "            min_distance = min(distances)\n",
    "            yield key, (max_neighbors, min_distance, final_state, final_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRundirectedShortestPath.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x undirectedShortestPath.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of traversal =\n",
      "iteration 1 of traversal =\n",
      "iteration 2 of traversal =\n",
      "Final graph state:\n",
      "\"1\"\t[\"2,5\", 0, \"V\", \"1\"]\n",
      "\"2\"\t[\"1,3,5,4\", 1, \"V\", \"1-2\"]\n",
      "\"3\"\t[\"2,4\", 2, \"V\", \"1-2-3\"]\n",
      "\"4\"\t[\"3,2,5\", 2, \"V\", \"1-5-4\"]\n",
      "\"5\"\t[\"1,2,4\", 1, \"V\", \"1-5\"]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "## preping the folders that will store the \n",
    "## inputs and outputs for the iterative\n",
    "## graph exploration procedure\n",
    "!rm -r toy_undirected_graph_input\n",
    "!rm -r toy_undirected_graph_output\n",
    "!mkdir toy_undirected_graph_input\n",
    "!cp initial_undirected_graph.txt toy_undirected_graph_input/toy_undirected_graph_0.input\n",
    "\n",
    "from undirectedShortestPath import MRundirectedShortestPath\n",
    "from numpy import random,array\n",
    "import ast\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    frontier_exists = False\n",
    "    j = i + 1\n",
    "    mr_job = MRundirectedShortestPath(args=[\"toy_undirected_graph_input\",\n",
    "                                         \"--output-dir\", \"toy_undirected_graph_output\"])\n",
    "    print \"iteration \"+str(i)+\" of traversal =\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            node, payload = line.split(\"\\t\")\n",
    "            node = node.strip('\"')\n",
    "            payload_raw = payload\n",
    "            payload = ast.literal_eval(payload)\n",
    "            state = payload[2]\n",
    "            ## for this toy example, we can simply exit\n",
    "            ## once there are no more nodes in the \n",
    "            ## frontier\n",
    "            if state == \"Q\":\n",
    "                frontier_exists = True\n",
    "    i = i + 1\n",
    "    ## cleaning folders to continue iterating\n",
    "    ## through algorithm\n",
    "    !rm -r toy_undirected_graph_input\n",
    "    !cp -rT toy_undirected_graph_output toy_undirected_graph_input\n",
    "    if not frontier_exists:\n",
    "        break\n",
    "print \"Final graph state:\"\n",
    "!cat toy_undirected_graph_output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting directedShortestPath.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile directedShortestPath.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRdirectedShortestPath(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                      mapper=self.mapper,\n",
    "                      reducer=self.reducer\n",
    "                      )]\n",
    "\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, payload = line.split(\"\\t\")\n",
    "        payload = ast.literal_eval(payload)\n",
    "        neighbors = payload[0]\n",
    "        distance = int(payload[1])\n",
    "        state = payload[2]\n",
    "        path = payload[3]\n",
    "        neighbors_list = neighbors.split(\",\")\n",
    "        ## If we are processing a node that is in\n",
    "        ## the frontier for the current exploration\n",
    "        ## step of the graph, we emit it as well as\n",
    "        ## its neighbors\n",
    "        if state == \"Q\":\n",
    "            for n in neighbors_list:\n",
    "                ## We never want to emit \"no neighbors\"\n",
    "                ## as a neighbor of the current node\n",
    "                n_path = path + \"->\" + n\n",
    "                if n != \"\":\n",
    "                    yield n, (\"*\", distance + 1, \"Q\", n_path)\n",
    "                if n == \"\":\n",
    "                    sys.stderr.write(\"This is an empty node\\n\")\n",
    "            ## Again, if by some processing error we arrive at\n",
    "            ## an \"empty node\", we do not want to emit it\n",
    "            if str(node.strip('\"')) != \"\":\n",
    "                yield str(node.strip('\"')), (neighbors, distance, \"V\", path)\n",
    "        ## If the node being processed is not in the frontier\n",
    "        ## we simply emit it\n",
    "        elif state == \"V\" or state == \"U\":\n",
    "            if str(node.strip('\"')) != \"\":\n",
    "                yield str(node.strip('\"')), (neighbors, distance, state, path)\n",
    "        else:\n",
    "            ## As a safeguard against unexpected behavior we have this\n",
    "            ## statement\n",
    "            print \"PROCESSING ERROR: UNEXPECTED STATE FOR NODE TO BE EMITTED BY MAPPER\"\n",
    "\n",
    "    def reducer(self, key, value):\n",
    "        ## I am holding all the states for a single node (key)\n",
    "        ## in this set\n",
    "        state_set = set()\n",
    "        ## The distances array holds the distance from the node (key)\n",
    "        ## to all other nodes connecting TO the node. This is used to\n",
    "        ## calculate the minimum distance\n",
    "        distances = []\n",
    "        min_distance = 0\n",
    "        max_neighbors = \"\"\n",
    "        final_state = \"\"\n",
    "        final_path = \"\"\n",
    "        found_final_state = False\n",
    "        if key != \"\":\n",
    "            for v in value:\n",
    "                neighbors, distance, state, path = v\n",
    "                state_set.add(state)\n",
    "                if path != \"-\":\n",
    "                    ## given that these exercises are for unweighted\n",
    "                    ## graphs, the moment a node is reached from the\n",
    "                    ## source, it has by definition been reached\n",
    "                    ## through the shortest path\n",
    "                    final_path = path\n",
    "                if neighbors != \"*\":\n",
    "                    if max_neighbors == \"\":\n",
    "                        max_neighbors = max_neighbors + neighbors\n",
    "                    else:\n",
    "                        max_neighbors = max_neighbors + \",\" + neighbors\n",
    "                distances.append(int(distance))\n",
    "            if final_path == \"\":\n",
    "                final_path = \"-\"\n",
    "            if \"V\" in state_set:\n",
    "                final_state = \"V\"\n",
    "            elif \"Q\" in state_set:\n",
    "                final_state = \"Q\"\n",
    "            else:\n",
    "                final_state = \"U\"\n",
    "            min_distance = min(distances)\n",
    "            yield key, (max_neighbors, min_distance, final_state, final_path)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MRdirectedShortestPath.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of traversal =\n",
      "iteration 1 of traversal =\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "This is an empty node\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 2 of traversal =\n",
      "iteration 3 of traversal =\n",
      "\"1\"\t[\"2,6\", 0, \"V\", \"1\"]\n",
      "\"2\"\t[\"1,3,4\", 1, \"V\", \"1->2\"]\n",
      "\"3\"\t[\"2,4\", 2, \"V\", \"1->2->3\"]\n",
      "\"4\"\t[\"2,5\", 2, \"V\", \"1->2->4\"]\n",
      "\"5\"\t[\"1,2,4\", 3, \"V\", \"1->2->4->5\"]\n",
      "\"6\"\t[\"\", 1, \"V\", \"1->6\"]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!rm -r toy_directed_graph_input\n",
    "!rm -r toy_directed_graph_output\n",
    "!mkdir toy_directed_graph_input\n",
    "!cp initial_directed_graph.txt toy_directed_graph_input/automated_graph_step_0.input\n",
    "\n",
    "from directedShortestPath import MRdirectedShortestPath\n",
    "from numpy import random,array\n",
    "import ast\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    frontier_exists = False\n",
    "    j = i + 1\n",
    "    mr_job = MRdirectedShortestPath(args=[\"toy_directed_graph_input\",\n",
    "                                         \"--output-dir\", \"toy_directed_graph_output\"])\n",
    "    print \"iteration \"+str(i)+\" of traversal =\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            node, payload = line.split(\"\\t\")\n",
    "            node = node.strip('\"')\n",
    "            payload_raw = payload\n",
    "            payload = ast.literal_eval(payload)\n",
    "            state = payload[2]\n",
    "            if state == \"Q\":\n",
    "                frontier_exists = True\n",
    "    i = i + 1\n",
    "\n",
    "    !rm -r toy_directed_graph_input\n",
    "    !cp -rT toy_directed_graph_output toy_directed_graph_input\n",
    "    if not frontier_exists:\n",
    "        break\n",
    "!cat toy_directed_graph_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.1: Exploratory data analysis (NLTK synonyms)\n",
    "\n",
    "Using MRJob, explore the synonyms network data.\n",
    "Consider plotting the degree distribution (does it follow a power law?),\n",
    "and determine some of the key features, like:\n",
    "\n",
    "number of nodes, \n",
    "number links,\n",
    "or the average degree (i.e., the average number of links per node),\n",
    "etc...\n",
    "\n",
    "As you develop your code, please be sure to run it locally first (though on the whole dataset). \n",
    "Once you have gotten you code to run locally, deploy it on AWS as a systems test\n",
    "in preparation for our next dataset (which will require AWS)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "Plotting the number of degrees of the nodes in the network we can clearly see that the distribution of degrees follows a power law; that is, the histogram shows an exponential decay that visually approximates $f(d)=b^{g(d)}$ where d is degrees and g is a function that evaluates to a negative number.\n",
    "\n",
    "Exploratory data analysis revealed that the __number of nodes in the synonyms graph is 8271, the average degree of nodes in the graph is 7.39, and the total number of links (edges) is 30567.__\n",
    "\n",
    "Following the recommendation in the instructions, I first ran the code locally and then verified the same behavior in the AWS EMR ecosystem. The EMR specifications were as follows:\n",
    "\n",
    "|Number of nodes|Node type|Runtime|\n",
    "|---|---|---|\n",
    "|6|m3.xlarge|95 seconds|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonyms_eda_plot.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonyms_eda_plot.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MR_synonyms_eda_plot(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "            mapper=self.mapper)]\n",
    "    \n",
    "    ## only using a mapper as I simply want to transform\n",
    "    ## the input data to do EDA\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, payload = line.split(\"\\t\")\n",
    "        payload = ast.literal_eval(payload)\n",
    "        neighbors = payload[0]\n",
    "        distance = int(payload[1])\n",
    "        state = payload[2]\n",
    "        path = payload[3]\n",
    "        neighbors_list = neighbors.split(\",\")\n",
    "        degrees = len(neighbors_list)\n",
    "        yield node, degrees\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MR_synonyms_eda_plot.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x synonyms_eda_plot.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/synonyms_eda_plot.cloudera.20160710.170605.976378\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/synonyms_eda_plot.cloudera.20160710.170605.976378/output...\n",
      "Removing temp directory /tmp/synonyms_eda_plot.cloudera.20160710.170605.976378...\n"
     ]
    }
   ],
   "source": [
    "!./synonyms_eda_plot.py synonyms_input/synonyms_input_0.input > synonyms_input_plot.txt\n",
    "# !head synonyms_input_plot.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x7f652b1d0b10>"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYQAAAEACAYAAACznAEdAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f652b4aeb10>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import pandas as pd\n",
    "\n",
    "\n",
    "df = pd.read_csv(\"synonyms_input_plot.txt\",\n",
    "                 sep='\\t',\n",
    "                header=None)\n",
    "\n",
    "counts = pd.Series(df[1])\n",
    "# print counts\n",
    "counts.hist(bins=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting synonyms_eda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile synonyms_eda.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MR_synonyms_eda(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "            mapper=self.mapper,\n",
    "            combiner=self.combiner,\n",
    "            reducer=self.reducer)]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, payload = line.split(\"\\t\")\n",
    "        payload = ast.literal_eval(payload)\n",
    "        neighbors = payload[0]\n",
    "        distance = int(payload[1])\n",
    "        state = payload[2]\n",
    "        path = payload[3]\n",
    "        neighbors_list = neighbors.split(\",\")\n",
    "        degrees = len(neighbors_list)\n",
    "        yield None, (1, degrees)\n",
    "        \n",
    "    def combiner(self, key, values):\n",
    "        count, degrees = 0, 0\n",
    "        for v in values:\n",
    "            count += v[0]\n",
    "            degrees += v[1]\n",
    "        yield None, (count, degrees)\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        node_count, average_degrees, link_count = 0, 0, 0\n",
    "        for v in values:\n",
    "            node_count += v[0]\n",
    "            link_count += v[1]\n",
    "        average_degrees = float(link_count) / float(node_count)\n",
    "        link_count = float(link_count) / 2.0\n",
    "        yield None, (node_count, average_degrees, link_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MR_synonyms_eda.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x synonyms_eda.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local EDA run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t[8271, 7.391367428364164, 30567.0]\r\n"
     ]
    }
   ],
   "source": [
    "# !./synonyms_eda.py synonyms_input/synonyms_input_0.input > synonyms_eda_local.output\n",
    "!cat synonyms_eda_local.output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS synonym EDA run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## first we spool up a cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-2 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 rm s3://cerc-w261/HW7/synonyms_eda --recursive\n",
    "!python /home/cloudera/w261/HW7/src/synonyms_eda.py \\\n",
    "-r emr s3://cerc-w261/HW7/synonyms_input \\\n",
    "    --cluster-id=j-TK4RCWUIOMPL \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW7/synonyms_eda \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t[8271, 7.391367428364164, 30567.0]\r\n"
     ]
    }
   ],
   "source": [
    "#!aws s3 cp s3://cerc-w261/HW7/synonyms_eda synonyns_eda_emr --recursive\n",
    "!cat synonyns_eda_emr/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.2: Shortest path graph distances (NLTK synonyms)\n",
    "\n",
    "Write (reuse your code from 7.0) an MRJob class to find shortest path graph distances, \n",
    "and apply it to the NLTK synonyms network dataset. \n",
    "\n",
    "Proof your code's function by running the job:\n",
    "\n",
    "- shortest path starting at \"walk\" (index=7827) and ending at \"make\" (index=536),\n",
    "\n",
    "and showing you code's output. Once again, your output should include the path and the distance.\n",
    "\n",
    "As you develop your code, please be sure to run it locally first (though on the whole dataset). \n",
    "Once you have gotten you code to run locally, deploy it on AWS as a systems test\n",
    "in preparation for our next dataset (which will require AWS).\n",
    "\n",
    "=====================================\n",
    "\n",
    "__NOTE: Dataset 2 English Wikipedia hyperlink network.data__\n",
    "\n",
    "The dataset is available via Dropbox at:\n",
    "\n",
    "https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "on S3 at \n",
    "\n",
    "s3://ucb-mids-mls-networks/wikipedia/\n",
    "\n",
    "s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt # Graph\n",
    "\n",
    "s3://ucb-mids-mls-networks/wikipedia/indices.txt # Page titles and page Ids\n",
    "\n",
    "For the remainder of this assignment you will explore the English Wikipedia hyperlink network.\n",
    "\n",
    "The dataset is built from the Sept. 2015 XML snapshot of English Wikipedia.\n",
    "For this directed network, a link between articles: \n",
    "\n",
    "A -> B\n",
    "\n",
    "is defined by the existence of a hyperlink in A pointing to B.\n",
    "This network also exists in the indexed format:\n",
    "\n",
    "Data: s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt\n",
    "\n",
    "Data: s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-in.txt\n",
    "\n",
    "Data: s3://ucb-mids-mls-networks/wikipedia/indices.txt\n",
    "\n",
    "but has an index with more detailed data:\n",
    "\n",
    "(article name) \\t (index) \\t (in degree) \\t (out degree)\n",
    "\n",
    "In the dictionary, target nodes are keys, link weights are values.\n",
    "Here, a weight indicates the number of time a page links to another.\n",
    "However, for the sake of this assignment, treat this an unweighted network,\n",
    "and set all weights to 1 upon data input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "##### ANSWER:\n",
    "\n",
    "Reusing the code written for 7.0, I found that one of __the shortest path from node 7827 (\"walk\") to node 536 (\"make\") has a length of three and is as follows (local and AWS runs returned different, but valid shortest paths):__\n",
    "\n",
    "    Local found: 7827-4655-631-536\n",
    "    AWS found: 7827-1426-264-536\n",
    "\n",
    "Following the recommendation in the instructions, I first ran the code locally and then verified the same behavior in the AWS EMR ecosystem. The EMR specifications were as follows:\n",
    "\n",
    "|Job|Number of nodes|Node type|Runtime|\n",
    "|---|---|---|---|\n",
    "|__Adjacency List to SSSP input__|6|m3.xlarge|62 seconds|\n",
    "|__SSP (node 7827 to 536)__|6|m3.xlarge|198 seconds|"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local implementation of synNet SSSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/adjacencyListToGraph.cloudera.20160710.180836.553287\n",
      "Running step 1 of 1...\n",
      "Streaming final output from /tmp/adjacencyListToGraph.cloudera.20160710.180836.553287/output...\n",
      "Removing temp directory /tmp/adjacencyListToGraph.cloudera.20160710.180836.553287...\n",
      "Processing iteration 0 of algorithm\n",
      "Processing iteration 1 of algorithm\n",
      "Processing iteration 2 of algorithm\n",
      "Processing iteration 3 of algorithm\n",
      "Processing iteration 4 of algorithm\n",
      "Processing iteration 5 of algorithm\n",
      "Processing iteration 6 of algorithm\n",
      "Processing iteration 7 of algorithm\n",
      "Processing iteration 8 of algorithm\n",
      "Processing iteration 9 of algorithm\n",
      "Processing iteration 10 of algorithm\n",
      "Final graph state reached!\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!rm -r synonyms_input\n",
    "!rm -r synonyms_output\n",
    "!mkdir synonyms_input\n",
    "!./adjacencyListToGraph.py --source-node 7827 /home/cloudera/w261/HW7/data/synNet-synNet/synNet.txt \\\n",
    "> synonyms_input/synonyms_input_0.input\n",
    "\n",
    "from undirectedShortestPath import MRundirectedShortestPath\n",
    "from numpy import random,array\n",
    "import ast\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    frontier_exists = False\n",
    "    j = i + 1\n",
    "    mr_job = MRundirectedShortestPath(args=[\"synonyms_input\",\n",
    "                                         \"--output-dir\", \"synonyms_output\"])\n",
    "    print \"Processing iteration \"+str(i)+\" of algorithm\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            node, payload = line.split(\"\\t\")\n",
    "            node = node.strip('\"')\n",
    "            payload_raw = payload\n",
    "            payload = ast.literal_eval(payload)\n",
    "            state = payload[2]\n",
    "#             print line\n",
    "            if state == \"Q\":\n",
    "                frontier_exists = True\n",
    "    i = i + 1\n",
    "    !rm -r synonyms_input\n",
    "    !cp -rT synonyms_output synonyms_input\n",
    "    if not frontier_exists:\n",
    "        break\n",
    "print \"Final graph state reached!\"\n",
    "#!cat automated_undirected_graph_step_output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"536\"\t[\"215,3648,662,57,3658,3656,3657,3651,3652,3653,3555,3554,6002,3552,6000,6001,404,2248,2249,3750,2246,2247,3993,3992,3760,3761,1313,3481,3769,3483,3482,3485,121,265,264,3647,1554,5593,5592,5594,5596,3241,534,533,532,531,1668,2257,2256,2255,2254,5911,2252,2251,2250,3775,3774,4320,585,4651,4652,3471,3478,3621,448,3749,5353,3742,3743,3740,3741,3310,6003,1647,3551,2253,4420,4421,641,2794,5912,2160,3593,4318,3595,3594,3751,4180,3599,3598,64,69,657,653,659,631,3601,3600,4608,4609,3688,1801,722,6058,6057,6056,2439,1209,5210,1162,3738,1661,3739,5211,3737,1199,5354,1810,1811,5351,1195,770,4316,4317,5281,4319,3622,616,73,5209,5208,4791,5352,6004,5366,5364,5365,1681,1685,1477,474,1688,3553\", 3, \"V\", \"7827-4655-631-536\"]\r\n"
     ]
    }
   ],
   "source": [
    "!cat synonyms_output/* |\\\n",
    "grep '\"536\"' | grep '536\"]'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS implementation of synNet SSSP\n",
    "\n",
    "The main difference between my AWS and local implementations is that I stopped by AWS algorithm as soon as I found the shortest path that I was looking for as opposed to continuing to compute the shortest path to all nodes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/synNet-synNet/synNet.txt to s3://cerc-w261/HW7/synNet-synNet/synNet.txt\r\n"
     ]
    }
   ],
   "source": [
    "## Upload raw materials to AWS\n",
    "!aws s3 cp /home/cloudera/w261/HW7/data/synNet-synNet/synNet.txt \\\n",
    "s3://cerc-w261/HW7/synNet-synNet/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/adjacencyListToGraph.cloudera.20160710.020708.989896\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/adjacencyListToGraph.cloudera.20160710.020708.989896/files/...\n",
      "Adding our job to existing cluster j-397C64HGINOL4\n",
      "Waiting for step 1 of 1 (s-2S88IO7YV55NL) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40302/cluster\n",
      "  RUNNING for 17.2s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 49.1s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-2S88IO7YV55NL on ec2-54-186-16-21.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-186-16-21.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-2S88IO7YV55NL/syslog\n",
      "Counters: 35\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1272976\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=636232\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=4128586\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3800\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=1272976\n",
      "\t\tS3: Number of bytes written=636232\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=40\n",
      "\t\tLaunched map tasks=40\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1490855040\n",
      "\t\tTotal time spent by all map tasks (ms)=1035316\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=46589220\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=1035316\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=47910\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=12813\n",
      "\t\tInput split bytes=3800\n",
      "\t\tMap input records=8271\n",
      "\t\tMap output records=8271\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=10664415232\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=12822511616\n",
      "\t\tVirtual memory (bytes) snapshot=78501793792\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/adjacencyListToGraph.cloudera.20160710.020708.989896/...\n",
      "Removing temp directory /tmp/adjacencyListToGraph.cloudera.20160710.020708.989896...\n",
      "Killing our SSH tunnel (pid 351)\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm s3://cerc-w261/HW7/synonyms_input --recursive\n",
    "!python /home/cloudera/w261/HW7/src/adjacencyListToGraph.py --source-node 7827 \\\n",
    "-r emr s3://cerc-w261/HW7/synNet-synNet \\\n",
    "    --cluster-id=j-TK4RCWUIOMPL \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW7/synonyms_input \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/adjacencyListToGraph.cloudera.20160710.181403.263675\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/adjacencyListToGraph.cloudera.20160710.181403.263675/files/...\n",
      "Adding our job to existing cluster j-TK4RCWUIOMPL\n",
      "Waiting for step 1 of 1 (s-3SRZLUPP4DKWN) to complete...\n",
      "  PENDING (cluster is WAITING: Cluster ready after last step completed.)\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40358/cluster\n",
      "  RUNNING for 29.7s\n",
      "     5.0% complete\n",
      "  RUNNING for 62.7s\n",
      "    90.5% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3SRZLUPP4DKWN on ec2-54-187-170-182.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-187-170-182.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3SRZLUPP4DKWN/syslog\n",
      "Counters: 35\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=1224736\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=636235\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=4128591\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=3800\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=1224736\n",
      "\t\tS3: Number of bytes written=636235\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=40\n",
      "\t\tLaunched map tasks=40\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=1292412960\n",
      "\t\tTotal time spent by all map tasks (ms)=897509\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=40387905\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=897509\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=46930\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=11884\n",
      "\t\tInput split bytes=3800\n",
      "\t\tMap input records=8271\n",
      "\t\tMap output records=8271\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=10971095040\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=12949389312\n",
      "\t\tVirtual memory (bytes) snapshot=78641229824\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/adjacencyListToGraph.cloudera.20160710.181403.263675/...\n",
      "Removing temp directory /tmp/adjacencyListToGraph.cloudera.20160710.181403.263675...\n",
      "Killing our SSH tunnel (pid 9264)\n",
      "iteration 0 of traversal =\n",
      "-\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00010\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00007\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00011\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00002\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00006\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00001\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00003\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00008\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00004\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00000\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00005\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00012\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00013\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00021\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00020\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00017\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00015\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00014\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00019\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00022\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00023\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00016\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00027\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00026\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00031\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00025\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00029\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00030\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00028\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00024\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00033\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00032\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00036\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00034\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00039\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00037\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00038\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00035\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00001 to s3://cerc-w261/HW7/synonyms_input/part-00001\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/_SUCCESS to s3://cerc-w261/HW7/synonyms_input/_SUCCESS\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00005 to s3://cerc-w261/HW7/synonyms_input/part-00005\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00004 to s3://cerc-w261/HW7/synonyms_input/part-00004\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00003 to s3://cerc-w261/HW7/synonyms_input/part-00003\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00007 to s3://cerc-w261/HW7/synonyms_input/part-00007\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00000 to s3://cerc-w261/HW7/synonyms_input/part-00000\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00002 to s3://cerc-w261/HW7/synonyms_input/part-00002\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00006 to s3://cerc-w261/HW7/synonyms_input/part-00006\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00008 to s3://cerc-w261/HW7/synonyms_input/part-00008\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00009 to s3://cerc-w261/HW7/synonyms_input/part-00009\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00010 to s3://cerc-w261/HW7/synonyms_input/part-00010\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00013 to s3://cerc-w261/HW7/synonyms_input/part-00013\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00016 to s3://cerc-w261/HW7/synonyms_input/part-00016\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00015 to s3://cerc-w261/HW7/synonyms_input/part-00015\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00011 to s3://cerc-w261/HW7/synonyms_input/part-00011\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00014 to s3://cerc-w261/HW7/synonyms_input/part-00014\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00012 to s3://cerc-w261/HW7/synonyms_input/part-00012\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00017 to s3://cerc-w261/HW7/synonyms_input/part-00017\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00018 to s3://cerc-w261/HW7/synonyms_input/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00010\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00004\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00003\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00001\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00000\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00011\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00006\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00002\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00008\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00012\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00013\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00016\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00015\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00017\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00007\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00005\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00014\n",
      "iteration 1 of traversal =\n",
      "-\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00010\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00005\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00007\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00006\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00001\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00011\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00004\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00000\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00002\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00003\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00008\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00012\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00014\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00013\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00015\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00016\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00017\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00018\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/_SUCCESS to s3://cerc-w261/HW7/synonyms_input/_SUCCESS\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00000 to s3://cerc-w261/HW7/synonyms_input/part-00000\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00005 to s3://cerc-w261/HW7/synonyms_input/part-00005\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00002 to s3://cerc-w261/HW7/synonyms_input/part-00002\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00006 to s3://cerc-w261/HW7/synonyms_input/part-00006\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00004 to s3://cerc-w261/HW7/synonyms_input/part-00004\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00008 to s3://cerc-w261/HW7/synonyms_input/part-00008\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00001 to s3://cerc-w261/HW7/synonyms_input/part-00001\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00003 to s3://cerc-w261/HW7/synonyms_input/part-00003\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00007 to s3://cerc-w261/HW7/synonyms_input/part-00007\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00012 to s3://cerc-w261/HW7/synonyms_input/part-00012\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00010 to s3://cerc-w261/HW7/synonyms_input/part-00010\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00015 to s3://cerc-w261/HW7/synonyms_input/part-00015\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00018 to s3://cerc-w261/HW7/synonyms_input/part-00018\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00014 to s3://cerc-w261/HW7/synonyms_input/part-00014\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00013 to s3://cerc-w261/HW7/synonyms_input/part-00013\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00017 to s3://cerc-w261/HW7/synonyms_input/part-00017\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00016 to s3://cerc-w261/HW7/synonyms_input/part-00016\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00011 to s3://cerc-w261/HW7/synonyms_input/part-00011\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00009 to s3://cerc-w261/HW7/synonyms_input/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00010\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00011\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00004\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00000\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00001\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00003\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00005\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00002\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00008\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00006\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00012\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00013\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00015\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00016\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00017\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00014\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00007\n",
      "iteration 2 of traversal =\n",
      "7827-1426-264-536\n",
      "####################################################\n",
      "####################################################\n",
      "\n",
      "\n",
      "The above path is the shortest we were looking for!!\n",
      "\n",
      "\n",
      "####################################################\n",
      "####################################################\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00010\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00011\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00004\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00000\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00005\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00003\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00001\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00008\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00007\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00006\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00002\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00012\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00013\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00014\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00015\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00016\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_input/part-00017\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00003 to s3://cerc-w261/HW7/synonyms_input/part-00003\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00002 to s3://cerc-w261/HW7/synonyms_input/part-00002\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00007 to s3://cerc-w261/HW7/synonyms_input/part-00007\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/_SUCCESS to s3://cerc-w261/HW7/synonyms_input/_SUCCESS\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00005 to s3://cerc-w261/HW7/synonyms_input/part-00005\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00000 to s3://cerc-w261/HW7/synonyms_input/part-00000\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00008 to s3://cerc-w261/HW7/synonyms_input/part-00008\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00004 to s3://cerc-w261/HW7/synonyms_input/part-00004\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00006 to s3://cerc-w261/HW7/synonyms_input/part-00006\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00001 to s3://cerc-w261/HW7/synonyms_input/part-00001\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00009 to s3://cerc-w261/HW7/synonyms_input/part-00009\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00011 to s3://cerc-w261/HW7/synonyms_input/part-00011\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00014 to s3://cerc-w261/HW7/synonyms_input/part-00014\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00012 to s3://cerc-w261/HW7/synonyms_input/part-00012\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00013 to s3://cerc-w261/HW7/synonyms_input/part-00013\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00010 to s3://cerc-w261/HW7/synonyms_input/part-00010\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00015 to s3://cerc-w261/HW7/synonyms_input/part-00015\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00016 to s3://cerc-w261/HW7/synonyms_input/part-00016\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00017 to s3://cerc-w261/HW7/synonyms_input/part-00017\n",
      "copy: s3://cerc-w261/HW7/synonyms_output/part-00018 to s3://cerc-w261/HW7/synonyms_input/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00009\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00010\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00011\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00005\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00002\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00007\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00001\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00008\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00003\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00004\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00006\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00000\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00012\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00013\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00016\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00018\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00017\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00015\n",
      "delete: s3://cerc-w261/HW7/synonyms_output/part-00014\n",
      "Final graph state reached!\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "\n",
    "!aws s3 rm s3://cerc-w261/HW7/synonyms_input/ --recursive --quiet\n",
    "!python /home/cloudera/w261/HW7/src/adjacencyListToGraph.py --source-node 7827 \\\n",
    "-r emr s3://cerc-w261/HW7/synNet-synNet \\\n",
    "    --cluster-id=j-TK4RCWUIOMPL \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW7/synonyms_input \\\n",
    "        --no-output\n",
    "\n",
    "!aws s3 rm s3://cerc-w261/HW7/synonyms_output --recursive --quiet\n",
    "\n",
    "from undirectedShortestPath import MRundirectedShortestPath\n",
    "from numpy import random,array\n",
    "import ast\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    frontier_exists = False\n",
    "    path_found = False\n",
    "    j = i + 1\n",
    "    mr_job = MRundirectedShortestPath(args=[\"-r\",\"emr\",\n",
    "                                            \"s3://cerc-w261/HW7/synonyms_input\",\n",
    "                                            \"--cluster-id=j-TK4RCWUIOMPL\",\n",
    "                                            \"--aws-region=us-west-2\",\n",
    "                                         \"--output-dir\", \"s3://cerc-w261/HW7/synonyms_output\"])\n",
    "    print \"Progressing through iteration \"+str(i)+\" of the algorithm\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            node, payload = line.split(\"\\t\")\n",
    "            node = node.strip('\"')\n",
    "            payload_raw = payload\n",
    "            payload = ast.literal_eval(payload)\n",
    "            state = payload[2]\n",
    "            path = payload[3]\n",
    "            if node == \"536\":\n",
    "                print path\n",
    "                if path != \"-\":\n",
    "                    print \"####################################################\"\n",
    "                    print \"####################################################\\n\\n\"\n",
    "                    print \"The above path is the shortest we were looking for!!\"\n",
    "                    print \"\\n\\n####################################################\"\n",
    "                    print \"####################################################\"\n",
    "                    path_found = True\n",
    "                    break\n",
    "#             print line\n",
    "            if state == \"Q\":\n",
    "                frontier_exists = True\n",
    "            \n",
    "    i = i + 1\n",
    "#     !rm -r synonyms_input\n",
    "#     !cp -rT synonyms_output synonyms_input\n",
    "    !aws s3 rm s3://cerc-w261/HW7/synonyms_input/ --recursive --quiet\n",
    "    !aws s3 cp s3://cerc-w261/HW7/synonyms_output s3://cerc-w261/HW7/synonyms_input/ --recursive --quiet\n",
    "    !aws s3 rm s3://cerc-w261/HW7/synonyms_output/ --recursive --quiet\n",
    "    if not frontier_exists or path_found:\n",
    "        break\n",
    "# print \"Final graph state reached!\"\n",
    "#!cat automated_undirected_graph_step_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.3: Exploratory data analysis (Wikipedia)\n",
    "\n",
    "Using MRJob, explore the Wikipedia network data on the AWS cloud. Reuse your code from HW 7.1---does is scale well? \n",
    "\n",
    "Be cautioned that Wikipedia is a directed network, where links are not symmetric. \n",
    "So, even though a node may be linked to, it will not appear as a primary record itself if it has no out-links. \n",
    "\n",
    "This means that you may have to ADJUST your code (depending on its design). \n",
    "\n",
    "To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "\n",
    "###### ANSWER:\n",
    "\n",
    "To conduct the same exploratory data analysis from HW 7.1 on the wikipedia graph, given that it is a directed graph, I opted not to divide the sum of the out degrees by two as was needed for the synNet graph. \n",
    "\n",
    "As to the question of whether the code scales appropriately to the wikipedia graph, I believe that it does, given the relatively short time to execute for the job (~3 minutes), and the fact that I designed the job to use combiners (taking advantage of the commutative intermediate operations that would be performed by the reducers).\n",
    "\n",
    "See below for the exploratory data analysis for the toy and wikipedia graphs:\n",
    "\n",
    "| |Number of nodes|Average degrees|Number of edges|\n",
    "|---|---|---|---|\n",
    "|__Toy graph__|5|2.4|12|\n",
    "|__Wikipedia graph__|5,781,290|24.58|142,114,057|\n",
    "\n",
    "The EMR specifications for running the wikipedia job in the cloud were as follows:\n",
    "\n",
    "|Number of nodes|Node type|Runtime|\n",
    "|---|---|---|\n",
    "|6|m3.xlarge|160 seconds|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting wiki_eda.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile wiki_eda.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MR_wiki_eda(MRJob):\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "            mapper=self.mapper,\n",
    "            combiner=self.combiner,\n",
    "            reducer=self.reducer)]\n",
    "        \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, payload = line.split(\"\\t\")\n",
    "        payload = ast.literal_eval(payload)\n",
    "        degrees = len(payload)\n",
    "        yield None, (1, degrees)\n",
    "        \n",
    "    def combiner(self, key, values):\n",
    "        count, degrees = 0, 0\n",
    "        for v in values:\n",
    "            count += v[0]\n",
    "            degrees += v[1]\n",
    "        yield None, (count, degrees)\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        node_count, average_degrees, link_count = 0, 0, 0\n",
    "        for v in values:\n",
    "            node_count += v[0]\n",
    "            link_count += v[1]\n",
    "        average_degrees = float(link_count) / float(node_count)\n",
    "        link_count = float(link_count)\n",
    "        yield None, (node_count, average_degrees, link_count)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    MR_wiki_eda.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x wiki_eda.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t[5, 4.0, 20.0]\r\n"
     ]
    }
   ],
   "source": [
    "# !./adjacencyListToGraph.py /home/cloudera/w261/HW7/data/directed_toy/directed_toy.txt \\\n",
    "# > /home/cloudera/w261/HW7/data/directed_toy/initial_directed_graph.txt\n",
    "# !cat /home/cloudera/w261/HW7/data/directed_toy/initial_directed_graph.txt\n",
    "# !./wiki_eda.py /home/cloudera/w261/HW7/data/directed_toy/initial_directed_graph.txt > wiki_eda_local.output\n",
    "!cat wiki_eda_local.output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!aws s3 sync s3://ucb-mids-mls-networks/wikipedia s3://cerc-w261/HW7/wiki/inputs --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AWS implementation of wikipedia EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Could not infer endpoint for bucket ucb-mids-mls-networks; assuming s3.amazonaws.com\n",
      "Creating temp directory /tmp/wiki_eda.cloudera.20160710.184020.659948\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/wiki_eda.cloudera.20160710.184020.659948/files/...\n",
      "Adding our job to existing cluster j-TK4RCWUIOMPL\n",
      "Waiting for step 1 of 1 (s-38TEYY9A7Y1P9) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40358/cluster\n",
      "  RUNNING for 5.0s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 36.6s\n",
      "  RUNNING for 67.7s\n",
      "  RUNNING for 99.1s\n",
      "  RUNNING for 129.6s\n",
      "  RUNNING for 160.2s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-38TEYY9A7Y1P9 on ec2-54-187-170-182.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-187-170-182.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-38TEYY9A7Y1P9/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2090919509\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=47\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=920\n",
      "\t\tFILE: Number of bytes written=6160871\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4560\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2090919509\n",
      "\t\tS3: Number of bytes written=47\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=40\n",
      "\t\tLaunched map tasks=40\n",
      "\t\tLaunched reduce tasks=19\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6200899200\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1391463360\n",
      "\t\tTotal time spent by all map tasks (ms)=4306180\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=193778100\n",
      "\t\tTotal time spent by all reduce tasks (ms)=483147\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=43483230\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4306180\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=483147\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1750530\n",
      "\t\tCombine input records=5781290\n",
      "\t\tCombine output records=40\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=18636\n",
      "\t\tInput split bytes=4560\n",
      "\t\tMap input records=5781290\n",
      "\t\tMap output bytes=73013977\n",
      "\t\tMap output materialized bytes=13160\n",
      "\t\tMap output records=5781290\n",
      "\t\tMerged Map outputs=760\n",
      "\t\tPhysical memory (bytes) snapshot=26563825664\n",
      "\t\tReduce input groups=1\n",
      "\t\tReduce input records=40\n",
      "\t\tReduce output records=1\n",
      "\t\tReduce shuffle bytes=13160\n",
      "\t\tShuffled Maps =760\n",
      "\t\tSpilled Records=80\n",
      "\t\tTotal committed heap usage (bytes)=30813454336\n",
      "\t\tVirtual memory (bytes) snapshot=140295225344\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/wiki_eda.cloudera.20160710.184020.659948/...\n",
      "Removing temp directory /tmp/wiki_eda.cloudera.20160710.184020.659948...\n",
      "Killing our SSH tunnel (pid 11471)\n"
     ]
    }
   ],
   "source": [
    "# !aws s3 sync s3://ucb-mids-mls-networks/wikipedia s3://cerc-w261/HW7/wiki/inputs\n",
    "!aws s3 rm s3://cerc-w261/HW7/wiki/output_eda/ --recursive\n",
    "!python /home/cloudera/w261/HW7/src/wiki_eda.py -r emr \\\n",
    "s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt \\\n",
    "    --cluster-id=j-TK4RCWUIOMPL \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW7/wiki/output_eda \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "null\t[5781290, 24.58172086160701, 142114057.0]\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp s3://cerc-w261/HW7/wiki/output_eda wiki_eda_output --recursive --quiet\n",
    "!cat wiki_eda_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.4: Shortest path graph distances (Wikipedia)\n",
    "\n",
    "Using MRJob, find shortest path graph distances in the Wikipedia network on the AWS cloud.\n",
    "Reuse your code from 7.2, but once again be warned of Wikipedia being a directed network.\n",
    "To be sure of your code's functionality in this context, run a systems test on the directed_toy.txt network.\n",
    "\n",
    "When running your code on the Wikipedia network, proof its function by running the job:\n",
    "\n",
    "- shortest path from \"Ireland\" (index=6176135) to \"University of California, Berkeley\" (index=13466359),\n",
    "\n",
    "and show your code's output. Show the shortest path in terms of just page IDS but also in terms of the name of page (show of your MapReduce join skills!!)\n",
    "\n",
    "Once your code is running, find some other shortest paths and report your results.\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "I found the __shortest path from node 6176135 to node 13466359 to be of length two:__\n",
    "\n",
    "    6176135->11607791->13466359\n",
    "    \n",
    "The AWS components and runtimes were as follows:\n",
    "\n",
    "|Job|Number of nodes|Node type|Runtime|\n",
    "|---|---|---|---|\n",
    "|__Adjacency List to SSSP input__|6|m3.xlarge|182 seconds|\n",
    "|__SSP (node 6176135 to 13466359)__|6|m3.xlarge|274 seconds|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting adjacencyListToGraph_wiki.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile adjacencyListToGraph_wiki.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRadjacencyListToGraph_wiki(MRJob):\n",
    "    \n",
    "    source_node = '6176135'\n",
    "    \n",
    "    def steps(self):\n",
    "        return [MRStep(\n",
    "                mapper_init=self.mapper_init,\n",
    "                      mapper=self.mapper\n",
    "                      )]\n",
    "    #####################################\n",
    "    #####################################\n",
    "        \n",
    "    def configure_options(self):\n",
    "        super(MRadjacencyListToGraph_wiki, self).configure_options()\n",
    "        self.add_passthrough_option('--source-node', type='int', default=1, help='...')\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.source_node = str(self.options.source_node)\n",
    "            \n",
    "    #####################################\n",
    "    #####################################\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        node, payload = line.split(\"\\t\")\n",
    "        neighbors = ast.literal_eval(payload)\n",
    "        neighbors_temp = \"\"\n",
    "        for k in neighbors.keys():\n",
    "            if neighbors_temp == \"\":\n",
    "                neighbors_temp = neighbors_temp + k\n",
    "            else:\n",
    "                neighbors_temp = neighbors_temp + \",\" + k\n",
    "        if node == self.source_node:\n",
    "            yield node, (neighbors_temp, 0, \"Q\",self.source_node)\n",
    "        else:\n",
    "            yield node, (neighbors_temp, sys.maxint, \"U\", \"-\")\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRadjacencyListToGraph_wiki.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x adjacencyListToGraph_wiki.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option emr_cluster_id from /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Could not infer endpoint for bucket ucb-mids-mls-networks; assuming s3.amazonaws.com\n",
      "Creating temp directory /tmp/adjacencyListToGraph_wiki.cloudera.20160710.042656.049026\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/adjacencyListToGraph_wiki.cloudera.20160710.042656.049026/files/...\n",
      "Adding our job to existing cluster j-397C64HGINOL4\n",
      "Waiting for step 1 of 1 (s-3A33DQB1DB0YU) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40302/cluster\n",
      "  RUNNING for 24.8s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 57.2s\n",
      "  RUNNING for 88.5s\n",
      "  RUNNING for 119.1s\n",
      "  RUNNING for 150.0s\n",
      "  RUNNING for 181.2s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3A33DQB1DB0YU on ec2-54-186-16-21.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-186-16-21.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3A33DQB1DB0YU/syslog\n",
      "Counters: 36\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=2090890301\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=1445499451\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=4131356\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4560\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=40\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=2090890301\n",
      "\t\tS3: Number of bytes written=1445499451\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=41\n",
      "\t\tKilled map tasks=1\n",
      "\t\tLaunched map tasks=41\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=6587592480\n",
      "\t\tTotal time spent by all map tasks (ms)=4574717\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=205862265\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=4574717\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1867390\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=16001\n",
      "\t\tInput split bytes=4560\n",
      "\t\tMap input records=5781290\n",
      "\t\tMap output records=5781290\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=14535380992\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=16768827392\n",
      "\t\tVirtual memory (bytes) snapshot=78599467008\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/adjacencyListToGraph_wiki.cloudera.20160710.042656.049026/...\n",
      "Removing temp directory /tmp/adjacencyListToGraph_wiki.cloudera.20160710.042656.049026...\n",
      "Killing our SSH tunnel (pid 13339)\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm s3://cerc-w261/HW7/wiki_path_input/ --recursive\n",
    "# !aws s3 rm s3://cerc-w261/HW7/synonyms_input --recursive\n",
    "!python /home/cloudera/w261/HW7/src/adjacencyListToGraph_wiki.py --source-node 6176135 \\\n",
    "-r emr s3://ucb-mids-mls-networks/wikipedia/all-pages-indexed-out.txt \\\n",
    "    --cluster-id=j-397C64HGINOL4 \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW7/wiki_path_input \\\n",
    "        --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "MRdirectedShortestPath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0 of traversal =\n",
      "-\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00009\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00010\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00011\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00000\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00001\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00006\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00002\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00003\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00012\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00005\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00004\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00007\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00008\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00014\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00013\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00017\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00015\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00016\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00020\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00021\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00019\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00018\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00022\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00023\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00024\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00025\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00026\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00027\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00029\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00030\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00032\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00028\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00031\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00033\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00034\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00036\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00037\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00038\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00039\n",
      "delete: s3://cerc-w261/HW7/wiki_path_input/part-00035\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/_SUCCESS to s3://cerc-w261/HW7/wiki_path_input/_SUCCESS\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00000 to s3://cerc-w261/HW7/wiki_path_input/part-00000\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00001 to s3://cerc-w261/HW7/wiki_path_input/part-00001\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00002 to s3://cerc-w261/HW7/wiki_path_input/part-00002\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00003 to s3://cerc-w261/HW7/wiki_path_input/part-00003\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00004 to s3://cerc-w261/HW7/wiki_path_input/part-00004\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00005 to s3://cerc-w261/HW7/wiki_path_input/part-00005\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00006 to s3://cerc-w261/HW7/wiki_path_input/part-00006\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00007 to s3://cerc-w261/HW7/wiki_path_input/part-00007\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00008 to s3://cerc-w261/HW7/wiki_path_input/part-00008\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00009 to s3://cerc-w261/HW7/wiki_path_input/part-00009\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00010 to s3://cerc-w261/HW7/wiki_path_input/part-00010\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00011 to s3://cerc-w261/HW7/wiki_path_input/part-00011\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00012 to s3://cerc-w261/HW7/wiki_path_input/part-00012\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00013 to s3://cerc-w261/HW7/wiki_path_input/part-00013\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00014 to s3://cerc-w261/HW7/wiki_path_input/part-00014\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00015 to s3://cerc-w261/HW7/wiki_path_input/part-00015\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00016 to s3://cerc-w261/HW7/wiki_path_input/part-00016\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00017 to s3://cerc-w261/HW7/wiki_path_input/part-00017\n",
      "copy: s3://cerc-w261/HW7/wiki_path_output/part-00018 to s3://cerc-w261/HW7/wiki_path_input/part-00018\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/_SUCCESS\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00009\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00010\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00005\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00011\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00000\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00002\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00004\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00003\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00007\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00001\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00008\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00006\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00012\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00013\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00014\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00015\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00018\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00016\n",
      "delete: s3://cerc-w261/HW7/wiki_path_output/part-00017\n",
      "iteration 1 of traversal =\n",
      "6176135->11607791->13466359\n",
      "The above path is the shortest we were looking for!!\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "I/O operation on closed file",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-80-604bb422f6b2>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     55\u001b[0m \u001b[1;31m#     !rm -r synonyms_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;31m#     !cp -rT synonyms_output synonyms_input\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 57\u001b[1;33m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'aws s3 rm s3://cerc-w261/HW7/wiki_path_input/ --recursive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     58\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'aws s3 cp s3://cerc-w261/HW7/wiki_path_output s3://cerc-w261/HW7/wiki_path_input/ --recursive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m     \u001b[0mget_ipython\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mu'aws s3 rm s3://cerc-w261/HW7/wiki_path_output/ --recursive'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/IPython/core/interactiveshell.pyc\u001b[0m in \u001b[0;36msystem_piped\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m   2212\u001b[0m         \u001b[1;31m# a non-None value would trigger :func:`sys.displayhook` calls.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2213\u001b[0m         \u001b[1;31m# Instead, we store the exit_code in user_ns.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 2214\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muser_ns\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'_exit_code'\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0msystem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   2215\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   2216\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msystem_raw\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcmd\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/IPython/utils/_process_posix.pyc\u001b[0m in \u001b[0;36msystem\u001b[1;34m(self, cmd)\u001b[0m\n\u001b[0;32m    161\u001b[0m                 \u001b[1;31m# know whether we've finished (if we matched EOF) or not\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    162\u001b[0m                 \u001b[0mres_idx\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexpect_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpatterns\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mread_timeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 163\u001b[1;33m                 \u001b[1;32mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mchild\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbefore\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mout_size\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdecode\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0menc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'replace'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mend\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    164\u001b[0m                 \u001b[0mflush\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    165\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mres_idx\u001b[0m\u001b[1;33m==\u001b[0m\u001b[0mEOF_index\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/ipykernel/iostream.pyc\u001b[0m in \u001b[0;36mwrite\u001b[1;34m(self, string)\u001b[0m\n\u001b[0;32m    315\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    316\u001b[0m             \u001b[0mis_child\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_master_process\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 317\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_buffer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    318\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mis_child\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    319\u001b[0m                 \u001b[1;31m# newlines imply flush in subprocesses\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: I/O operation on closed file"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!aws s3 rm s3://cerc-w261/HW7/wiki_path_output --recursive\n",
    "\n",
    "from directedShortestPath import MRdirectedShortestPath\n",
    "from numpy import random,array\n",
    "import ast\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "    frontier_exists = False\n",
    "    path_found = False\n",
    "    j = i + 1\n",
    "    mr_job = MRdirectedShortestPath(args=[\"-r\",\"emr\",\n",
    "                                            \"s3://cerc-w261/HW7/wiki_path_input\",\n",
    "                                            \"--cluster-id=j-397C64HGINOL4\",\n",
    "                                            \"--aws-region=us-west-2\",\n",
    "                                         \"--output-dir\", \"s3://cerc-w261/HW7/wiki_path_output\"])\n",
    "    print \"iteration \"+str(i)+\" of traversal =\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        for line in runner.stream_output():\n",
    "            node, payload = line.split(\"\\t\")\n",
    "            node = node.strip('\"')\n",
    "            payload_raw = payload\n",
    "            payload = ast.literal_eval(payload)\n",
    "            state = payload[2]\n",
    "            path = payload[3]\n",
    "            if node == \"13466359\":\n",
    "                print path\n",
    "                if path != \"-\":\n",
    "                    print \"The above path is the shortest we were looking for!!\"\n",
    "                    path_found = True\n",
    "                    break\n",
    "#             print line\n",
    "            if state == \"Q\":\n",
    "                frontier_exists = True\n",
    "            \n",
    "    i = i + 1\n",
    "\n",
    "    !aws s3 rm s3://cerc-w261/HW7/wiki_path_input/ --recursive\n",
    "    !aws s3 cp s3://cerc-w261/HW7/wiki_path_output s3://cerc-w261/HW7/wiki_path_input/ --recursive\n",
    "    !aws s3 rm s3://cerc-w261/HW7/wiki_path_output/ --recursive\n",
    "    if not frontier_exists or path_found:\n",
    "        break\n",
    "print \"Final graph state reached!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.5: Conceptual exercise: Largest single-source network distances\n",
    "\n",
    "Suppose you wanted to find the largest network distance from a single source,\n",
    "i.e., a node that is the furthest (but still reachable) from a single source.\n",
    "\n",
    "How would you implement this task? \n",
    "How is this different from finding the shortest path graph distances?\n",
    "\n",
    "Is this task more difficult to implement than the shortest path distance?\n",
    "\n",
    "As you respond, please comment on program structure, runtimes, iterations, general system requirements, etc...\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "Implementing the task of finding the node that is furthest (but still reachable) in a graph from a single source would be implemented in a fashion not too different from that which we used to find the shortest path from the source to all reachable nodes in the network; __since it is not specified in the question and we primarily dealt with unweighted graphs in this homework assignment I am providing an answer for the largest distance in an unweighted graph problem__.\n",
    "\n",
    "To solve this problem I would iterate through the graph in a distributed BFS fashion similar to how it was exected in the distributed SSSP algorithm. Moving forward I will assume that if there are multiple paths from the source to a node in the network the path to be considered as a candidate for the \"longest path distance\" in the network is the shortest of the paths between the source and the node. With this assumption, we can proceed with the distributed breadth first search algorithm, having our iterating master process hold a global maximum distance to be compared against the distances found in the latest iteration of the algorithm.\n",
    "\n",
    "That said, we can simplify this as the breadth first search of an unweighted graph, given the design of our MR algorithm, will always have in the boundary the known/discovered nodes that are the farthest from the source node. Hence if we sort in descending order the distances for the nodes in the boundary (hint, in the BFS unweighted case the distances for the frontier nodes will all be the same!) and output the top distance, once the algorithm terminates (there are no more nodes in the frontier), all we need do is see the latest of these top distances output!\n",
    "\n",
    "To reiterate, given the above implementation, this algorithm is clearly not that different from SSSP. Moreover, this task would not be significantly more difficult to implement than SSSP.\n",
    "\n",
    "In terms of __program structure__, it would be very similar to SSSP in that the algorithm would be iterative: a MRJob would be iteratively applied to the graph by a master process that would monitor progress for the stopping condition (i.e. there are no more nodes in the frontier).\n",
    "\n",
    "In terms of __runtimes__, this algorithm would be $O(V+E)$ much like SSSP as we would need to explore all reachable nodes and edges to find the maximum distance.\n",
    "\n",
    "In terms of __general system requirements__, these would not differ from those of SSP for a similarly sized graph."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.5.1:\n",
    "\n",
    "Can we utilize combiners in the HW 7 to perform the shortest path implementation?\n",
    "\n",
    "Does order inversion help with the HW 7 shortest path implementation?\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "Since the operations of finding the minimum distance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 7.5.2: OPTIONAL\n",
    "\n",
    "Implement combiners in the context of HW 7.5 and contrast the performance of this implementation versus the implementation with no combiners. \n",
    "\n",
    "Please report the cluster configuration and runtimes in tabular format for both experiments and comment on your findings.\n",
    "\n",
    "##### ANSWER:\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW 7.6: Computational exercise: Largest single-source network distances: OPTIONAL\n",
    "\n",
    "Using MRJob, write a code to find the largest graph distance and distance-maximizing nodes from a single-source.\n",
    "Test your code first on the toy networks and synonyms network to proof its function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "==================END HW 7=================="
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
