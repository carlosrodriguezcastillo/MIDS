{\rtf1\ansi\ansicpg1252\cocoartf1404\cocoasubrtf470
{\fonttbl\f0\fswiss\fcharset0 Helvetica;}
{\colortbl;\red255\green255\blue255;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 \{\
 "cells": [\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "# DATASCI W261: Machine Learning at Scale \\n",\
    "\\n",\
    "**Name: Carlos Eduardo Rodriguez Castillo**\\n",\
    "\\n",\
    "**email: cerodriguez@berkeley.edu**\\n",\
    "\\n",\
    "**Midterm**\\n",\
    "\\n",\
    "**Section 2**"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 1"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "#### ANSWER:\\n",\
    "\\n",\
    "(b) is not true, additional hardware does not necessarrily lead to an N-Fold speedup when using mapReduce. The speedup only holds true if the task being executed on my the mapReduce framework can be parallelized.\\n",\
    "\\n",\
    "(d) is true. We have a master node that accumulates the gradients from the reducers to compute the weights update."\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": true\
   \},\
   "outputs": [],\
   "source": [\
    "%%writefile problem1job.py\\n",\
    "#!/usr/bin/env python\\n",\
    "# -*- coding: utf-8 -*-\\n",\
    "\\n",\
    "from __future__ import division\\n",\
    "import re\\n",\
    "import numpy as np\\n",\
    "import mrjob\\n",\
    "from mrjob.job import MRJob\\n",\
    "from mrjob.step import MRStep\\n",\
    "from mrjob.protocol import RawValueProtocol\\n",\
    "from mrjob.protocol import RawProtocol\\n",\
    "\\n",\
    "class MRbuildSample(MRJob):\\n",\
    "\\n",\
    "    MRJob.SORT_VALUES = True\\n",\
    "    OUTPUT_PROTOCOL = RawProtocol\\n",\
    "    \\n",\
    "     def steps(self):\\n",\
    "        JOBCONF_STEP1 = \{    \\n",\
    "            \\"mapred.reduce.tasks\\":1\\n",\
    "            # ^^ Since we will end up with a relatively \\n",\
    "            # small file, letting hadoop handle the sort\\n",\
    "        \}\\n",\
    "        return [MRStep(jobconf=JOBCONF_STEP1,\\n",\
    "                    mapper=self.mapper,\\n",\
    "                    reducer=self.reducer)\\n",\
    "            ]\\n",\
    "\\n",\
    "    def mapper_init(self):\\n",\
    "#         yield\\n",\
    "\\n",\
    "    def mapper(self,_,line):\\n",\
    "        #yield\\n",\
    "\\n",\
    "    def mapper_final(self):\\n",\
    "#         yield\\n",\
    "\\n",\
    "    def combiner(self, key, value):\\n",\
    "#         yield\\n",\
    "\\n",\
    "    def reducer(self,key,values):  \\n",\
    "#         for v in values:\\n",\
    "#             yield str(key), \\",\\"+str(v)\\n",\
    "    \\n",\
    "if __name__ == '__main__':\\n",\
    "    MRbuildSample.run()"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": true\
   \},\
   "outputs": [],\
   "source": [\
    "!chmod a+x problem1job.py"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": null,\
   "metadata": \{\
    "collapsed": true\
   \},\
   "outputs": [],\
   "source": [\
    "!./problem1job.py "\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 2"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "##### ANSWER:\\n",\
    "\\n",\
    "While emitting (*, word) ensures that we can calculate the total ahead of time, in order to do this for multiple reducers, we need an additional step which is to create custom partitioners, and custom keys for the partitioners, making sure that each of the custom partition keys (one for each of the reducers) is emitted with the special (*,word) and that a secondary sort is being done to ensure that the (*,word) keys are always processed first."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 3"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "##### ANSWER:\\n",\
    "\\n",\
    "The input to the reduce function in MRJob is a key and a list of all the values associated to said key."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 4"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "##### ANSWER:\\n",\
    "\\n",\
    "Laplace smoothing ensures that there is no multiplication by zero in the posterior distribution when doing (Naive) Bayesian document classification."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 5"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "##### ANSWER:\\n",\
    "\\n",\
    "Increasing the complexity of a model regressed on some samples of data will result in lower bias but higher variance."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 6"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "##### ANSWER:\\n",\
    "\\n",\
    "Given that the Hadoop shuffle is not directly concerned with reducer workload but rather is directly concerned with network traffic, the correct answer is (c)."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 7"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 3,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "Writing kltext.txt\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "%%writefile kltext.txt\\n",\
    "1.Data Science is an interdisciplinary field about processes and systems to extract knowledge or insights from large volumes of data in various forms (data in various forms, data in various forms, data in various forms), either structured or unstructured,[1][2] which is a continuation of some of the data analysis fields such as statistics, data mining and predictive analytics, as well as Knowledge Discovery in Databases.\\n",\
    "2.Machine learning is a subfield of computer science[1] that evolved from the study of pattern recognition and computational learning theory in artificial intelligence.[1] Machine learning explores the study and construction of algorithms that can learn from and make predictions on data.[2] Such algorithms operate by building a model from example inputs in order to make data-driven predictions or decisions,[3]:2 rather than following strictly static program instructions."\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 1,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "data": \{\
      "text/plain": [\
       "1.0986122886681098"\
      ]\
     \},\
     "execution_count": 1,\
     "metadata": \{\},\
     "output_type": "execute_result"\
    \}\
   ],\
   "source": [\
    "import numpy as np\\n",\
    "np.log(3)"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 30,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "Overwriting kldivergence.py\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "%%writefile kldivergence.py\\n",\
    "#!/usr/bin/env python\\n",\
    "# -*- coding: utf-8 -*-\\n",\
    "from mrjob.job import MRJob\\n",\
    "import re\\n",\
    "import numpy as np\\n",\
    "\\n",\
    "\\n",\
    "class kldivergence(MRJob):\\n",\
    "    \\n",\
    "    def mapper1(self, _, line):\\n",\
    "        index = int(line.split('.',1)[0])\\n",\
    "        letter_list = re.sub(r\\"[^A-Za-z]+\\", '', line).lower()\\n",\
    "        count = \{\}\\n",\
    "        for l in letter_list:\\n",\
    "            if count.has_key(l):\\n",\
    "                count[l] += 1\\n",\
    "            else:\\n",\
    "                count[l] = 1\\n",\
    "        for key in count:\\n",\
    "            yield key, [index, count[key]*1.0/len(letter_list)]\\n",\
    "\\n",\
    "\\n",\
    "    def reducer1(self, key, values):\\n",\
    "        result = \\"\\"\\n",\
    "        for v in values:\\n",\
    "            index = v[0]\\n",\
    "            probability = v[1]\\n",\
    "            if result == \\"\\":\\n",\
    "                result = str(index)+\\",\\"+str(probability)+\\",\\"\\n",\
    "            else:\\n",\
    "                result = result + str(index)+\\",\\"+str(probability)\\n",\
    "#             result[index] = probability\\n",\
    "        yield None, result\\n",\
    "        \\n",\
    "    def mapper2(self, key, line):\\n",\
    "#         line = line.strip()\\n",\
    "#         character, distributions = line.split(\\"\\\\t\\")\\n",\
    "        character = key\\n",\
    "        distributions = line\\n",\
    "        distributions = distributions.split(\\",\\")\\n",\
    "        class_1, class_2 = distributions[0], distributions[2]\\n",\
    "        distribution_1, distribution_2 = distributions[1], distributions[3]\\n",\
    "        yield class_1, (character, distribution_1)\\n",\
    "        yield class_2, (character, distribution_2)\\n",\
    "    \\n",\
    "    def reducer2(self, key, values):\\n",\
    "        kl_sum = 0\\n",\
    "        for value in values:\\n",\
    "            distributions = value\\n",\
    "            distributions = distributions.split(\\",\\")\\n",\
    "            class_1, class_2 = distributions[0], distributions[2]\\n",\
    "            distribution_1, distribution_2 = distributions[1], distributions[3]\\n",\
    "            temp = float(distribution_1) * np.log(float(distribution_1) / float(distribution_2))\\n",\
    "            kl_sum = kl_sum + temp\\n",\
    "        yield None, kl_sum\\n",\
    "            \\n",\
    "    def steps(self):\\n",\
    "        return [self.mr(mapper=self.mapper1,\\n",\
    "                        reducer=self.reducer1),\\n",\
    "                self.mr(reducer=self.reducer2)]\\n",\
    "\\n",\
    "if __name__ == '__main__':\\n",\
    "    kldivergence.run()"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 31,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "Using configs in /home/cloudera/.mrjob.conf\\n",\
      "Creating temp directory /tmp/kldivergence.cloudera.20160630.000656.123994\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "Running step 1 of 2...\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "Running step 2 of 2...\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\n",\
      "Streaming final output from /tmp/kldivergence.cloudera.20160630.000656.123994/output...\\n",\
      "null\\t0.08088278445421844\\n",\
      "Removing temp directory /tmp/kldivergence.cloudera.20160630.000656.123994...\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "!chmod a+x kldivergence.py\\n",\
    "!./kldivergence.py kltext.txt"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 32,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "('n', '1,0.064139941691,2,0.0899742930591')\\n",\
      "('o', '1,0.069970845481,2,0.0796915167095')\\n",\
      "('p', '1,0.00874635568513,2,0.025706940874')\\n",\
      "('r', '1,0.067055393586,2,0.0745501285347')\\n",\
      "('s', '1,0.110787172012,2,0.0488431876607')\\n",\
      "('t', '1,0.0816326530612,2,0.0925449871465')\\n",\
      "('u', '1,0.0379008746356,2,0.025706940874')\\n",\
      "('v', '1,0.0204081632653,2,0.00771208226221')\\n",\
      "('w', '1,0.0116618075802,2,0.0025706940874')\\n",\
      "('x', '1,0.00291545189504,2,0.00514138817481')\\n",\
      "('y', '1,0.0145772594752,2,0.012853470437')\\n",\
      "('a', '1,0.110787172012,2,0.0848329048843')\\n",\
      "('b', '1,0.00583090379009,2,0.00771208226221')\\n",\
      "('c', '1,0.0408163265306,2,0.0488431876607')\\n",\
      "('d', '1,0.0553935860058,2,0.0411311053985')\\n",\
      "('e', '1,0.0758017492711,2,0.0899742930591')\\n",\
      "('f', '1,0.0291545189504,2,0.0231362467866')\\n",\
      "('g', '1,0.0145772594752,2,0.025706940874')\\n",\
      "('h', '1,0.0174927113703,2,0.0308483290488')\\n",\
      "('i', '1,0.0962099125364,2,0.0925449871465')\\n",\
      "('k', '1,0.00583090379009,2,0.00514138817481')\\n",\
      "('l', '1,0.0320699708455,2,0.0488431876607')\\n",\
      "('m', '1,0.0262390670554,2,0.0359897172237')\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "from kldivergence import kldivergence\\n",\
    "mr_job = kldivergence(args=['kltext.txt'])\\n",\
    "with mr_job.make_runner() as runner: \\n",\
    "    runner.run()\\n",\
    "    # stream_output: get access of the output \\n",\
    "    for line in runner.stream_output():\\n",\
    "        print mr_job.parse_output_line(line)"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 8"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "##### ANSWER:\\n",\
    "\\n",\
    "From the above we can see that j and q are missing from the character vectors (answer (c))."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 9"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 56,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "Overwriting kldivergenceSMOOTH.py\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "%%writefile kldivergenceSMOOTH.py\\n",\
    "#!/usr/bin/env python\\n",\
    "# -*- coding: utf-8 -*-\\n",\
    "from mrjob.job import MRJob\\n",\
    "import re\\n",\
    "import numpy as np\\n",\
    "\\n",\
    "\\n",\
    "class kldivergence(MRJob):\\n",\
    "    \\n",\
    "    def mapper1(self, _, line):\\n",\
    "        index = int(line.split('.',1)[0])\\n",\
    "        letter_list = re.sub(r\\"[^A-Za-z]+\\", '', line).lower()\\n",\
    "        count = \{\}\\n",\
    "        for l in letter_list:\\n",\
    "            if count.has_key(l):\\n",\
    "                count[l] += 1\\n",\
    "            else:\\n",\
    "                count[l] = 1\\n",\
    "        for key in count:\\n",\
    "            yield key, [index, (count[key] + 1)*1.0/(len(letter_list) + 24)]\\n",\
    "\\n",\
    "\\n",\
    "    def reducer1(self, key, values):\\n",\
    "        result = \\"\\"\\n",\
    "        for v in values:\\n",\
    "            index = v[0]\\n",\
    "            probability = v[1]\\n",\
    "            if result == \\"\\":\\n",\
    "                result = str(index)+\\",\\"+str(probability)+\\",\\"\\n",\
    "            else:\\n",\
    "                result = result + str(index)+\\",\\"+str(probability)\\n",\
    "#             result[index] = probability\\n",\
    "        yield None, result\\n",\
    "        \\n",\
    "    def mapper2(self, key, line):\\n",\
    "#         line = line.strip()\\n",\
    "#         character, distributions = line.split(\\"\\\\t\\")\\n",\
    "        character = key\\n",\
    "        distributions = line\\n",\
    "        distributions = distributions.split(\\",\\")\\n",\
    "        class_1, class_2 = distributions[0], distributions[2]\\n",\
    "        distribution_1, distribution_2 = distributions[1], distributions[3]\\n",\
    "        yield class_1, (character, distribution_1)\\n",\
    "        yield class_2, (character, distribution_2)\\n",\
    "    \\n",\
    "    def reducer2(self, key, values):\\n",\
    "        kl_sum = 0\\n",\
    "        for value in values:\\n",\
    "            distributions = value\\n",\
    "            distributions = distributions.split(\\",\\")\\n",\
    "            class_1, class_2 = distributions[0], distributions[2]\\n",\
    "            distribution_1, distribution_2 = distributions[1], distributions[3]\\n",\
    "            temp = float(distribution_1) * np.log(float(distribution_1) / float(distribution_2))\\n",\
    "            kl_sum = kl_sum + temp\\n",\
    "        yield None, kl_sum\\n",\
    "            \\n",\
    "    def steps(self):\\n",\
    "        return [self.mr(mapper=self.mapper1,\\n",\
    "                        reducer=self.reducer1),\\n",\
    "                self.mr(reducer=self.reducer2)]\\n",\
    "\\n",\
    "if __name__ == '__main__':\\n",\
    "    kldivergence.run()"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 57,\
   "metadata": \{\
    "collapsed": true\
   \},\
   "outputs": [],\
   "source": [\
    "!chmod a+x kldivergenceSMOOTH.py"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 58,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "Using configs in /home/cloudera/.mrjob.conf\\r\\n",\
      "Creating temp directory /tmp/kldivergenceSMOOTH.cloudera.20160630.003544.953028\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "Running step 1 of 2...\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "Running step 2 of 2...\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "mr() is deprecated and will be removed in v0.6.0. Use mrjob.step.MRStep directly instead.\\r\\n",\
      "Streaming final output from /tmp/kldivergenceSMOOTH.cloudera.20160630.003544.953028/output...\\r\\n",\
      "null\\t0.06726997279223423\\r\\n",\
      "Removing temp directory /tmp/kldivergenceSMOOTH.cloudera.20160630.003544.953028...\\r\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "!./kldivergenceSMOOTH.py kltext.txt"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 10"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "(a) is not true - alpha dies not need to be systematically reduced for the algorithm to converge. Additionally (b) is also not true, gradient descent only guarantees to find the global minimum for convex problems (and even then it may not converge due to too large and alpha). A static alpha can have gradient descent converge. Also, in linear regression, if J() is the OLS cost function, then the local minimum is the global minimum."\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 11"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 77,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "Overwriting KmeansTest.py\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "%%writefile KmeansTest.py\\n",\
    "#!/usr/bin/env python\\n",\
    "# -*- coding: utf-8 -*-\\n",\
    "from numpy import argmin, array, random\\n",\
    "from mrjob.job import MRJob\\n",\
    "from mrjob.step import MRJobStep\\n",\
    "from itertools import chain\\n",\
    "\\n",\
    "#Calculate find the nearest centroid for data point \\n",\
    "def MinDist(datapoint, centroid_points):\\n",\
    "    datapoint = array(datapoint)\\n",\
    "    centroid_points = array(centroid_points)\\n",\
    "    diff = datapoint - centroid_points \\n",\
    "    diffsq = diff**2\\n",\
    "    \\n",\
    "    distances = (diffsq.sum(axis = 1))**0.5\\n",\
    "    # Get the nearest centroid for each instance\\n",\
    "    min_idx = argmin(distances)\\n",\
    "    return min_idx\\n",\
    "\\n",\
    "#Check whether centroids converge\\n",\
    "# def stop_criterion(centroid_points_old, centroid_points_new,T):\\n",\
    "#     print centroid_points_old\\n",\
    "#     print centroid_points\\n",\
    "#     oldvalue = list(chain(*centroid_points_old))\\n",\
    "#     newvalue = list(chain(*centroid_points_new))\\n",\
    "#     Diff = [abs(x-y) for x, y in zip(oldvalue, newvalue)]\\n",\
    "#     Flag = True\\n",\
    "#     for i in Diff:\\n",\
    "#         if(i>T):\\n",\
    "#             Flag = False\\n",\
    "#             break\\n",\
    "#     return Flag\\n",\
    "\\n",\
    "\\n",\
    "class MRKmeans(MRJob):\\n",\
    "    centroid_points=[]\\n",\
    "    k=3    \\n",\
    "    def steps(self):\\n",\
    "        return [\\n",\
    "            MRJobStep(mapper_init = self.mapper_init,\\n",\
    "                      mapper=self.mapper,\\n",\
    "                      combiner = self.combiner,\\n",\
    "                      reducer=self.reducer)\\n",\
    "               ]\\n",\
    "    #load centroids info from file\\n",\
    "    def mapper_init(self):\\n",\
    "        self.centroid_points = [map(float,s.split('\\\\n')[0].split(',')) for s in open(\\"Centroids.txt\\").readlines()]\\n",\
    "#         open('Centroids.txt', 'w').close()\\n",\
    "    #load data and output the nearest centroid index and data point \\n",\
    "    def mapper(self, _, line):\\n",\
    "        D = (map(float,line.split(',')))\\n",\
    "        idx = MinDist(D,self.centroid_points)\\n",\
    "        yield int(idx), (D[0],D[1],1)\\n",\
    "    #Combine sum of data points locally\\n",\
    "    def combiner(self, idx, inputdata):\\n",\
    "        sumx = sumy = num = 0\\n",\
    "        for x,y,n in inputdata:\\n",\
    "            num = num + n\\n",\
    "            sumx = sumx + x\\n",\
    "            sumy = sumy + y\\n",\
    "        yield int(idx),(sumx,sumy,num)\\n",\
    "    #Aggregate sum for each cluster and then calculate the new centroids\\n",\
    "    def reducer(self, idx, inputdata): \\n",\
    "        centroids = []\\n",\
    "        num = [0]*self.k \\n",\
    "        distances = 0\\n",\
    "        for i in range(self.k):\\n",\
    "            centroids.append([0,0])\\n",\
    "        for x, y, n in inputdata:\\n",\
    "            num[idx] = num[idx] + n\\n",\
    "            centroids[idx][0] = centroids[idx][0] + x\\n",\
    "            centroids[idx][1] = centroids[idx][1] + y\\n",\
    "        centroids[idx][0] = centroids[idx][0]/num[idx]\\n",\
    "        centroids[idx][1] = centroids[idx][1]/num[idx]\\n",\
    "        with open('Centroids.txt', 'a') as f:\\n",\
    "            f.writelines(str(centroids[idx][0]) + ',' + str(centroids[idx][1]) + '\\\\n')\\n",\
    "        yield idx,(centroids[idx][0],centroids[idx][1])\\n",\
    "        \\n",\
    "if __name__ == '__main__':\\n",\
    "    MRKmeans.run()"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 78,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "0,0\\r\\n",\
      "6,3\\r\\n",\
      "3,6\\r\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "!chmod a+x KmeansTest.py\\n",\
    "!cat Centroids.txt"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 79,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "iteration1:\\n",\
      "Current path: /home/cloudera/w261/Midterm\\n"\
     ]\
    \},\
    \{\
     "ename": "ValueError",\
     "evalue": "operands could not be broadcast together with shapes (0,) (4,1000) ",\
     "output_type": "error",\
     "traceback": [\
      "\\u001b[1;31m---------------------------------------------------------------------------\\u001b[0m",\
      "\\u001b[1;31mValueError\\u001b[0m                                Traceback (most recent call last)",\
      "\\u001b[1;32m<ipython-input-79-c20801459905>\\u001b[0m in \\u001b[0;36m<module>\\u001b[1;34m()\\u001b[0m\\n\\u001b[0;32m     15\\u001b[0m     \\u001b[1;32mprint\\u001b[0m \\u001b[1;34m\\"iteration\\"\\u001b[0m\\u001b[1;33m+\\u001b[0m\\u001b[0mstr\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mi\\u001b[0m\\u001b[1;33m+\\u001b[0m\\u001b[1;36m1\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m+\\u001b[0m\\u001b[1;34m\\":\\"\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m     16\\u001b[0m     \\u001b[1;32mwith\\u001b[0m \\u001b[0mmr_job\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mmake_runner\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m \\u001b[1;32mas\\u001b[0m \\u001b[0mrunner\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m---> 17\\u001b[1;33m         \\u001b[0mrunner\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mrun\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m     18\\u001b[0m         \\u001b[1;31m# stream_output: get access of the output\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m     19\\u001b[0m         \\u001b[1;32mfor\\u001b[0m \\u001b[0mline\\u001b[0m \\u001b[1;32min\\u001b[0m \\u001b[0mrunner\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mstream_output\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/runner.pyc\\u001b[0m in \\u001b[0;36mrun\\u001b[1;34m(self)\\u001b[0m\\n\\u001b[0;32m    471\\u001b[0m             \\u001b[1;32mraise\\u001b[0m \\u001b[0mAssertionError\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;34m\\"Job already ran!\\"\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    472\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m--> 473\\u001b[1;33m         \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0m_run\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    474\\u001b[0m         \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0m_ran_job\\u001b[0m \\u001b[1;33m=\\u001b[0m \\u001b[0mTrue\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    475\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/sim.pyc\\u001b[0m in \\u001b[0;36m_run\\u001b[1;34m(self)\\u001b[0m\\n\\u001b[0;32m    170\\u001b[0m             \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0m_counters\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mappend\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m\{\\u001b[0m\\u001b[1;33m\}\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    171\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m--> 172\\u001b[1;33m             \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0m_invoke_step\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mstep_num\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[1;34m'mapper'\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    173\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    174\\u001b[0m             \\u001b[1;32mif\\u001b[0m \\u001b[1;34m'reducer'\\u001b[0m \\u001b[1;32min\\u001b[0m \\u001b[0mstep\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/sim.pyc\\u001b[0m in \\u001b[0;36m_invoke_step\\u001b[1;34m(self, step_num, step_type)\\u001b[0m\\n\\u001b[0;32m    257\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    258\\u001b[0m             self._run_step(step_num, step_type, input_path, output_path,\\n\\u001b[1;32m--> 259\\u001b[1;33m                            working_dir, env)\\n\\u001b[0m\\u001b[0;32m    260\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    261\\u001b[0m             \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0m_prev_outfiles\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mappend\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0moutput_path\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/inline.pyc\\u001b[0m in \\u001b[0;36m_run_step\\u001b[1;34m(self, step_num, step_type, input_path, output_path, working_dir, env, child_stdin)\\u001b[0m\\n\\u001b[0;32m    155\\u001b[0m                     child_instance.sandbox(stdin=child_stdin,\\n\\u001b[0;32m    156\\u001b[0m                                            stdout=child_stdout)\\n\\u001b[1;32m--> 157\\u001b[1;33m                     \\u001b[0mchild_instance\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mexecute\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    158\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    159\\u001b[0m             \\u001b[1;32mif\\u001b[0m \\u001b[0mhas_combiner\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.pyc\\u001b[0m in \\u001b[0;36mexecute\\u001b[1;34m(self)\\u001b[0m\\n\\u001b[0;32m    437\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    438\\u001b[0m         \\u001b[1;32melif\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0moptions\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mrun_mapper\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m--> 439\\u001b[1;33m             \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mrun_mapper\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0moptions\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mstep_num\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    440\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    441\\u001b[0m         \\u001b[1;32melif\\u001b[0m \\u001b[0mself\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0moptions\\u001b[0m\\u001b[1;33m.\\u001b[0m\\u001b[0mrun_combiner\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.pyc\\u001b[0m in \\u001b[0;36mrun_mapper\\u001b[1;34m(self, step_num)\\u001b[0m\\n\\u001b[0;32m    502\\u001b[0m         \\u001b[1;31m# run the mapper on each line\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    503\\u001b[0m         \\u001b[1;32mfor\\u001b[0m \\u001b[0mkey\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mvalue\\u001b[0m \\u001b[1;32min\\u001b[0m \\u001b[0mread_lines\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m--> 504\\u001b[1;33m             \\u001b[1;32mfor\\u001b[0m \\u001b[0mout_key\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mout_value\\u001b[0m \\u001b[1;32min\\u001b[0m \\u001b[0mmapper\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mkey\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mvalue\\u001b[0m\\u001b[1;33m)\\u001b[0m \\u001b[1;32mor\\u001b[0m \\u001b[1;33m(\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m:\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m    505\\u001b[0m                 \\u001b[0mwrite_line\\u001b[0m\\u001b[1;33m(\\u001b[0m\\u001b[0mout_key\\u001b[0m\\u001b[1;33m,\\u001b[0m \\u001b[0mout_value\\u001b[0m\\u001b[1;33m)\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m    506\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/w261/Midterm/KmeansTest.py\\u001b[0m in \\u001b[0;36mmapper\\u001b[1;34m(self, _, line)\\u001b[0m\\n",\
      "\\u001b[1;32m/home/cloudera/w261/Midterm/KmeansTest.py\\u001b[0m in \\u001b[0;36mMinDist\\u001b[1;34m(datapoint, centroid_points)\\u001b[0m\\n\\u001b[0;32m     18\\u001b[0m     \\u001b[1;32mreturn\\u001b[0m \\u001b[0mmin_idx\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m     19\\u001b[0m \\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[1;32m---> 20\\u001b[1;33m \\u001b[1;31m#Check whether centroids converge\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0m\\u001b[0;32m     21\\u001b[0m \\u001b[1;31m# def stop_criterion(centroid_points_old, centroid_points_new,T):\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n\\u001b[0;32m     22\\u001b[0m \\u001b[1;31m#     print centroid_points_old\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[1;33m\\u001b[0m\\u001b[0m\\n",\
      "\\u001b[1;31mValueError\\u001b[0m: operands could not be broadcast together with shapes (0,) (4,1000) "\
     ]\
    \}\
   ],\
   "source": [\
    "from numpy import random, array\\n",\
    "from KmeansTest import MRKmeans#, stop_criterion\\n",\
    "mr_job = MRKmeans(args=['Kmeandata.csv'])\\n",\
    "\\n",\
    "#Geneate initial centroids\\n",\
    "centroid_points = [[0,0],[6,3],[3,6]]\\n",\
    "k = 3\\n",\
    "with open('Centroids.txt', 'w+') as f:\\n",\
    "        f.writelines(','.join(str(j) for j in i) + '\\\\n' for i in centroid_points)\\n",\
    "\\n",\
    "# Update centroids iteratively\\n",\
    "for i in range(10):\\n",\
    "    # save previous centoids to check convergency\\n",\
    "    centroid_points_old = centroid_points[:]\\n",\
    "    print \\"iteration\\"+str(i+1)+\\":\\"\\n",\
    "    with mr_job.make_runner() as runner: \\n",\
    "        runner.run()\\n",\
    "        # stream_output: get access of the output \\n",\
    "        for line in runner.stream_output():\\n",\
    "            key,value =  mr_job.parse_output_line(line)\\n",\
    "            print key, value\\n",\
    "            centroid_points[key] = value\\n",\
    "    print \\"\\\\n\\"\\n",\
    "    i = i + 1\\n",\
    "print \\"Centroids\\\\n\\"\\n",\
    "print centroid_points"\
   ]\
  \},\
  \{\
   "cell_type": "code",\
   "execution_count": 38,\
   "metadata": \{\
    "collapsed": false\
   \},\
   "outputs": [\
    \{\
     "name": "stdout",\
     "output_type": "stream",\
     "text": [\
      "/home/cloudera/w261/Midterm\\r\\n"\
     ]\
    \}\
   ],\
   "source": [\
    "!pwd"\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 13\\n",\
    "##### ANSWER:\\n",\
    "\\n",\
    "(c) and (d) are correct! "\
   ]\
  \},\
  \{\
   "cell_type": "markdown",\
   "metadata": \{\},\
   "source": [\
    "### Problem 16\\n",\
    "\\n",\
    "##### ANSWER:\\n",\
    "\\n",\
    "A developer would choose to create a map-reduce job without the reduce step if (i) she is developping and looking to study the mapper output or if (ii) she is exclusively looking to attempt a transformation of each of the input lines to the job (and would like to save on the cost of the shuffle). None of the options presented in the question quite convey this; as such, I chose option (d) as it somewhat aligns with my reason (i) above and I interpret it as using the identity reducer.\\n",\
    "\\n",\
    "It is indeed possible to create mapReduce jobs without reducers by setting the number of reducers to zero (-D mapred.reduce.tasks=0 \\n",\
    "): http://hadoop.apache.org/docs/current/api/org/apache/hadoop/mapreduce/Job.html#setNumReduceTasks(int)"\
   ]\
  \}\
 ],\
 "metadata": \{\
  "kernelspec": \{\
   "display_name": "Python 2",\
   "language": "python",\
   "name": "python2"\
  \},\
  "language_info": \{\
   "codemirror_mode": \{\
    "name": "ipython",\
    "version": 2\
   \},\
   "file_extension": ".py",\
   "mimetype": "text/x-python",\
   "name": "python",\
   "nbconvert_exporter": "python",\
   "pygments_lexer": "ipython2",\
   "version": "2.7.11"\
  \}\
 \},\
 "nbformat": 4,\
 "nbformat_minor": 0\
\}}