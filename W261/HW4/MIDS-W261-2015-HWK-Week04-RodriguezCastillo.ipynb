{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261: Machine Learning at Scale \n",
    "\n",
    "**Name: Carlos Eduardo Rodriguez Castillo**\n",
    "\n",
    "**email: cerodriguez@berkeley.edu**\n",
    "\n",
    "**Week 4**\n",
    "\n",
    "**Section 2**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### HW4.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is MrJob? How is it different to Hadoop MapReduce?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "MrJob is a python framework for interacting with the Hadoop MapReduce programming infrastructure.\n",
    "\n",
    "It is different from Hadoop MapReduce programming framework in that it is a purely based framework (Hadoop MapReduce supports scripts in several languages). \n",
    "\n",
    "Additionally, MrJob allows the coder to run MapReduce jobs locally (without Hadoop) while Hadoop MapReduce by definition requires a running hadoop infrastructure to function.\n",
    "\n",
    "MrJob also abstracts the coder away from manual tasks that are required in the Hadoop MapReduce framework such as data serialization and deserialization between tasks.\n",
    "\n",
    "Finally, MrJob has a native integration with Amazon Elastic MapReduce which permits us to easily run large scale jobs on a cluster in the AWS cloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What are the mapper_init, mapper_final(), combiner_final(), reducer_final() methods? When are they called?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "The methods can be described as follows:\n",
    "\n",
    "- __mapper_init__: a function with the initialization code for each of the mapper nodes. The function defines an action to take before the mapper processes any input.\n",
    "- __mapper_final()__: a function with the termination code for each of the mapper nodes. The function defines an action to be taken by the mapper after it gets to the end of its input.\n",
    "- __combiner_final()__: a function with the termination code for each of the combiners. The function defines an action to be taken by the combiner after it gets to the end of its input.\n",
    "- __reduce_final()__: a function with the termination code for each of the reducers. The function defines an action to be taken by the reducer after it gets to the end of its input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW 4.0.1 Total sort using 1 and 3 reducers (a repeat of HW2.1) [OPTIONAL]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "WARNING: per-step jobconf has bugs that affect Total sorts/partitions etc.\n",
    "For MRJob,  Sort, partition code via the MRJob config does NOT work in local mode (known bug/feature which I believe has not been fixed as of June 2016). \n",
    "So you will need to run in the cloud (e.g.  in AWS).\n",
    "It's issue #616 in github:  \"Inline and Local modes should support per-step jobconf #616\".  https://github.com/Yelp/mrjob/issues/616\n",
    "To overcome this issue run your MRJob jobs on the cloud using -r hadoop or -r emr:\n",
    "\n",
    "       #!python MostFrequentVisits.py -r hadoop anonymous-msweb_converted.data\n",
    "\n",
    "NOTE:  \n",
    "  Hadoop will always give a total sort on the key (i.e., key part of the key-value pairs produced by the mappers) when using just one reducer.\n",
    "  When using multiple reducers Hadoop will by default give you a partial sort (i.e., all records within a partition will be sorted by the key (i.e., key part of the key-value pairs produced by the mappers) .\n",
    "  To achieve a total sort one needs to write a customer mapper to to prepend a partition key to each record,  and then do a secondary sort or the resulting records (This can be done with two map-reduce jobs or combined into one only job)\n",
    "\n",
    "\n",
    "[next up: TOTAL SORT using a single reducer]\n",
    "Given as input: Records of the form <integer, “NA”>, where integer is any integer, and “NA” is just the empty string.\n",
    "Output: sorted key value pairs of the form <integer, “NA”> in decreasing order using a single reducer. \n",
    "Write code to generate N  random records of the form <integer, “NA”>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort. Display the top 10 biggest numbers. Display the 10 smallest numbers.\n",
    "\n",
    "[next up: TOTAL SORT using multiple reducers] \n",
    "What happens if you have multiple reducers? Do you need additional steps? Explain. Feel free to code this up (This is an optional task). \n",
    "Write code to generate N  random records of the form <integer, “NA”>. Let N = 10,000.\n",
    "Write the python Hadoop streaming map-reduce job to perform this sort using 3 reducers. Display the top 10 biggest numbers. Display the 10 smallest numbers.\n",
    "\n",
    "HINT: you might need a jobconf for the sort step in your overall MRJob Script      \n",
    "        JOBCONF_STEP2 = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',    \n",
    "            'mapreduce.partition.keycomparator.options': '-k2,2nr -k1,1',\n",
    "            'mapreduce.job.reduces': '1',\n",
    "        }\n",
    "\n",
    "HINT2: Use the proper separator to separate the key from the key-value pair generated by the mapper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW4.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is serialization in the context of MrJob or Hadoop?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "In the context of MrJob or Hadoop, serialization is the process of turning data that is passed from one task to another into a format for efficiently shuffling the data across the network. Data is also serialized for efficient storage of the data (be it in memory or disk).\n",
    "\n",
    "Finally, the performance of MrJob/Hadoop is reliant on using an efficient serialization of the data being processed."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When is it used in these frameworks?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "Serialization is used in these frameworks when writing to disk and when passing data from one task to the next (that is from mapper to reducer, mapper to combiner, or combiner to reducer)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What is the default serialization mode for input and outputs for MrJob?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "__ANSWER__:\n",
    "\n",
    "The default serialization mode for input for MrJob is 'RawValueProtocol' which means that MrJob just reads in each line from the input as a string.\n",
    "\n",
    "The default serialization mode for output for MrJob 'JSONProtocol' which means that MrJob writes as a final output JSON strings separated by a tab character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW4.2 \n",
    "\n",
    "Recall the Microsoft logfiles data from the async lecture. The logfiles are respectively described and located at:\n",
    "\n",
    "https://kdd.ics.uci.edu/databases/msweb/msweb.html\n",
    "http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/\n",
    "\n",
    "This dataset records which areas (Vroots) of www.microsoft.com each user visited in a one-week timeframe in Feburary 1998.\n",
    "\n",
    " Here, you must preprocess the data on a single node (i.e., not on a cluster of nodes) from the format:\n",
    "\n",
    "C,\"10001\",10001   #Visitor id 10001\n",
    "\n",
    "V,1000,1          #Visit by Visitor 10001 to page id 1000\n",
    "\n",
    "V,1001,1          #Visit by Visitor 10001 to page id 1001\n",
    "\n",
    "V,1002,1          #Visit by Visitor 10001 to page id 1002\n",
    "\n",
    "C,\"10002\",10002   #Visitor id 10001\n",
    "\n",
    "V\n",
    "Note: #denotes comments\n",
    "to the format:\n",
    "\n",
    "V,1000,1,C, 10001\n",
    "\n",
    "V,1001,1,C, 10001\n",
    "\n",
    "V,1002,1,C, 10001\n",
    "\n",
    "Write the python code to accomplish this."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "Please see python code below to accomplish this. First, I pull the data down:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cd /home/cloudera/w261/HW4\n",
    "!mkdir -p data\n",
    "!mkdir -p src\n",
    "!cd data\n",
    "!wget \"http://archive.ics.uci.edu/ml/machine-learning-databases/anonymous/anonymous-msweb.data\"\n",
    "!mv anonymous-msweb.data data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then I wrote the file to do the preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting preprocess.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile preprocess.py\n",
    "#!/usr/bin/python\n",
    "\"\"\"\n",
    "Single node data preprocessing for HW4.2\n",
    "\"\"\"\n",
    "__author__ = \"Carlos Eduardo Rodriguez Castillo\"\n",
    "__email__ = \"cerodriguez@berkeley.edu\"\n",
    "\n",
    "import sys\n",
    "import re\n",
    "\n",
    "url_dict = {}\n",
    "\n",
    "for line in sys.stdin:\n",
    "    line = line.strip()\n",
    "    elements = line.split(\",\")\n",
    "    ## Given the design of the raw data file, \n",
    "    ## the script will first populate the url_dict\n",
    "    if elements[0] == 'A':\n",
    "        url_dict[elements[1]] = 'http://www.microsoft.com' + elements[4].strip('\\\"')\n",
    "    elif elements[0] == 'C':\n",
    "        visitor_data = elements[0] + ',' + elements[2]\n",
    "        continue\n",
    "    elif elements[0] == 'V':\n",
    "        ## this is formatted as 'V,[URL_ID],[URL]\n",
    "        visit_data = elements[0] + ',' + elements[1] + ',' + url_dict[elements[1]]\n",
    "        ## this is formatted as 'V,[URL_ID],[URL],C,[USER_ID]\n",
    "        processed_line = visit_data + ',' + visitor_data\n",
    "        print processed_line"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, I tested the script on a small dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x preprocess.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I,4,\"www.microsoft.com\",\"created by getlog.pl\"\r\n",
      "T,1,\"VRoot\",0,0,\"VRoot\"\r\n",
      "N,0,\"0\"\r\n",
      "N,1,\"1\"\r\n",
      "T,2,\"Hide1\",0,0,\"Hide\"\r\n",
      "N,0,\"0\"\r\n",
      "N,1,\"1\"\r\n",
      "A,1287,1,\"International AutoRoute\",\"/autoroute\"\r\n",
      "A,1288,1,\"library\",\"/library\"\r\n",
      "A,1289,1,\"Master Chef Product Information\",\"/masterchef\"\r\n"
     ]
    }
   ],
   "source": [
    "!head -n 314 /home/cloudera/w261/HW4/data/anonymous-msweb.data > \\\n",
    "/home/cloudera/w261/HW4/data/mini-anonymous-msweb.data\n",
    "!head -n 10 /home/cloudera/w261/HW4/data/anonymous-msweb.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,http://www.microsoft.com/regwiz,C,10001\r\n",
      "V,1001,http://www.microsoft.com/support,C,10001\r\n",
      "V,1002,http://www.microsoft.com/athome,C,10001\r\n",
      "V,1001,http://www.microsoft.com/support,C,10002\r\n",
      "V,1003,http://www.microsoft.com/kb,C,10002\r\n",
      "V,1001,http://www.microsoft.com/support,C,10003\r\n",
      "V,1003,http://www.microsoft.com/kb,C,10003\r\n",
      "V,1004,http://www.microsoft.com/search,C,10003\r\n",
      "V,1005,http://www.microsoft.com/norge,C,10004\r\n"
     ]
    }
   ],
   "source": [
    "!./preprocess.py < \\\n",
    "/home/cloudera/w261/HW4/data/mini-anonymous-msweb.data # testing the preprocessing phase"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems to work, woohoo! Now, let's run the preprocessing script on the whole dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anonymous-msweb.data  anonymous-msweb-processed.data  mini-anonymous-msweb.data\r\n"
     ]
    }
   ],
   "source": [
    "!./preprocess.py < \\\n",
    "/home/cloudera/w261/HW4/data/anonymous-msweb.data \\\n",
    "> /home/cloudera/w261/HW4/data/anonymous-msweb-processed.data\n",
    "!ls /home/cloudera/w261/HW4/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "V,1000,http://www.microsoft.com/regwiz,C,10001\r\n",
      "V,1001,http://www.microsoft.com/support,C,10001\r\n",
      "V,1002,http://www.microsoft.com/athome,C,10001\r\n",
      "V,1001,http://www.microsoft.com/support,C,10002\r\n",
      "V,1003,http://www.microsoft.com/kb,C,10002\r\n",
      "V,1001,http://www.microsoft.com/support,C,10003\r\n",
      "V,1003,http://www.microsoft.com/kb,C,10003\r\n",
      "V,1004,http://www.microsoft.com/search,C,10003\r\n",
      "V,1005,http://www.microsoft.com/norge,C,10004\r\n",
      "V,1006,http://www.microsoft.com/misc,C,10005\r\n"
     ]
    }
   ],
   "source": [
    "!head /home/cloudera/w261/HW4/data/anonymous-msweb-processed.data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW4.3\n",
    "\n",
    "Find the 5 most frequently visited pages using MrJob from the output of 4.2 (i.e., transfromed log file).\n",
    "\n",
    "WARNING: per-step jobconf has bugs that affect Total sorts/partitions etc.\n",
    "For MRJob,  Sort, partition code via the MRJob config does NOT work in local mode (known bug/feature which I believe has not been fixed as of June 2016). \n",
    "So you will need to run in the cloud (e.g.  in AWS).\n",
    "It's issue #616 in github:  \"Inline and Local modes should support per-step jobconf #616\".  https://github.com/Yelp/mrjob/issues/616\n",
    "To overcome this issue run your MRJob jobs on the cloud using -r hadoop or -r emr:\n",
    "\n",
    "       #!python MostFrequentVisits.py -r hadoop anonymous-msweb_converted.data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "The five most frequently visited pages (identified by page ID) are as follows:\n",
    "\n",
    "1. Page ID: \"1008\"\tCount: 10836\n",
    "2. Page ID: \"1034\"\tCount: 9383\n",
    "3. Page ID: \"1004\"\tCount: 8463\n",
    "4. Page ID: \"1018\"\tCount: 5330\n",
    "5. Page ID: \"1017\"\tCount: 5108\n",
    "\n",
    "Below is the MrJob I used to come to this solution:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFrequentlyVisited.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFrequentlyVisited.py\n",
    "#!/home/cloudera/anaconda2/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class MRMostFrequentlyVisited(MRJob):\n",
    "    \n",
    "    COUNTER = 0\n",
    "    def mapper_count_visits(self, _, line):\n",
    "        line = line.strip()\n",
    "        record_elements = line.split(\",\")\n",
    "        page = record_elements[1]\n",
    "        yield (page, 1)\n",
    "            \n",
    "    def reducer_sum_visits(self, page, counts):\n",
    "        yield (page, sum(counts))\n",
    "    \n",
    "    def reducer_sort_sumVisits(self, page, counts):\n",
    "        yield (page, sum(counts))\n",
    "    \n",
    "    def steps(self):\n",
    "        JOB_CONF_STEP2 = {\n",
    "            'mapreduce.job.output.key.comparator.class': 'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'stream.num.map.output.key.field': 2,\n",
    "            'stream.map.output.field.separator':',',\n",
    "            'mapreduce.partition.keycomparator.options': \"-k2,2nr -k1,1\", # sort pages by count and then by ID\n",
    "             'mapreduce.job.reduces': 1\n",
    "        }\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_visits,\n",
    "                  reducer=self.reducer_sum_visits),\n",
    "            MRStep(jobconf=JOB_CONF_STEP2,\n",
    "                   reducer=self.reducer_sort_sumVisits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MRMostFrequentlyVisited().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x MostFrequentlyVisited.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/MostFrequentlyVisited.cloudera.20160610.225613.275472\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/MostFrequentlyVisited.cloudera.20160610.225613.275472/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob5487563921880336629.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1465484910625_0030\n",
      "  Submitted application application_1465484910625_0030\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465484910625_0030/\n",
      "  Running job: job_1465484910625_0030\n",
      "  Job job_1465484910625_0030 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1465484910625_0030 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/MostFrequentlyVisited.cloudera.20160610.225613.275472/step-output/0000\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4799499\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2903\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=52663\n",
      "\t\tFILE: Number of bytes written=473946\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4799897\n",
      "\t\tHDFS: Number of bytes written=2903\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=5123328\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1518080\n",
      "\t\tTotal time spent by all map tasks (ms)=40026\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5123328\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11860\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1518080\n",
      "\t\tTotal vcore-seconds taken by all map tasks=40026\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11860\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=5060\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=224\n",
      "\t\tInput split bytes=398\n",
      "\t\tMap input records=98654\n",
      "\t\tMap output bytes=887886\n",
      "\t\tMap output materialized bytes=53718\n",
      "\t\tMap output records=98654\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=366456832\n",
      "\t\tReduce input groups=285\n",
      "\t\tReduce input records=98654\n",
      "\t\tReduce output records=285\n",
      "\t\tReduce shuffle bytes=53718\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=197308\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\t\tVirtual memory (bytes) snapshot=2098880512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Running step 2 of 2...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7818483696208320094.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/127.0.0.1:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1465484910625_0031\n",
      "  Submitted application application_1465484910625_0031\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1465484910625_0031/\n",
      "  Running job: job_1465484910625_0031\n",
      "  Job job_1465484910625_0031 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1465484910625_0031 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/MostFrequentlyVisited.cloudera.20160610.225613.275472/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=4355\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=2903\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=1949\n",
      "\t\tFILE: Number of bytes written=373226\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=4735\n",
      "\t\tHDFS: Number of bytes written=2903\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3890048\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1187968\n",
      "\t\tTotal time spent by all map tasks (ms)=30391\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3890048\n",
      "\t\tTotal time spent by all reduce tasks (ms)=9281\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1187968\n",
      "\t\tTotal vcore-seconds taken by all map tasks=30391\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=9281\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2530\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=238\n",
      "\t\tInput split bytes=380\n",
      "\t\tMap input records=285\n",
      "\t\tMap output bytes=3188\n",
      "\t\tMap output materialized bytes=2077\n",
      "\t\tMap output records=285\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=403283968\n",
      "\t\tReduce input groups=285\n",
      "\t\tReduce input records=285\n",
      "\t\tReduce output records=285\n",
      "\t\tReduce shuffle bytes=2077\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=570\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\t\tVirtual memory (bytes) snapshot=2098720768\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/MostFrequentlyVisited.cloudera.20160610.225613.275472/output...\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/MostFrequentlyVisited.cloudera.20160610.225613.275472...\n",
      "\"1008\"\t10836\n",
      "\"1034\"\t9383\n",
      "\"1004\"\t8463\n",
      "\"1018\"\t5330\n",
      "\"1017\"\t5108\n",
      "Removing temp directory /tmp/MostFrequentlyVisited.cloudera.20160610.225613.275472...\n"
     ]
    }
   ],
   "source": [
    "!./MostFrequentlyVisited.py -r hadoop \\\n",
    "/home/cloudera/w261/HW4/data/anonymous-msweb-processed.data \\\n",
    "| head -n 5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW4.4\n",
    "\n",
    "Find the most frequent visitor of each page using MrJob and the output of 4.2  (i.e., transfromed log file). In this output please include the webpage URL, webpageID and Visitor ID."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ANSWER:\n",
    "\n",
    "Using the two MapReduce jobs defined below we find the most frequent visitor of each of the pages and we deposit the answers in the _hw4_4_output file_. __By exploring the file we find that the maximum number of visits that any given user in the dataset makes to any given page is one!__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting MostFrequentVisitorPerPageMRJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile MostFrequentVisitorPerPageMRJob.py\n",
    "#!/home/cloudera/anaconda2/bin/python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "\n",
    "class MostFrequentVisitorPerPageMRJob(MRJob):\n",
    "    \n",
    "    COUNTER = 0\n",
    "    def mapper_count_visits(self, _, line):\n",
    "        line = line.strip()\n",
    "        record_elements = line.split(\",\")\n",
    "        page_id = record_elements[1]\n",
    "        page_url = record_elements[2]\n",
    "        user_id = record_elements[4]\n",
    "        key = page_id + ',' + page_url + ',' + user_id\n",
    "        yield (key, 1)\n",
    "            \n",
    "    def reducer_sum_visits(self, key, counts):\n",
    "        yield (key, sum(counts))\n",
    "    \n",
    "    def reducer_sort_sumVisits(self, key, counts):\n",
    "        yield (key, max(counts))\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper=self.mapper_count_visits,\n",
    "                  reducer=self.reducer_sum_visits),\n",
    "            MRStep(reducer=self.reducer_sort_sumVisits)\n",
    "        ]\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    MostFrequentVisitorPerPageMRJob().run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x MostFrequentVisitorPerPageMRJob.py\n",
    "!./MostFrequentVisitorPerPageMRJob.py -r hadoop \\\n",
    "/home/cloudera/w261/HW4/data/anonymous-msweb-processed.data \\\n",
    "> hw4_4_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1000,http://www.microsoft.com/regwiz,10001\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10010\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10039\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10073\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10087\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10101\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10132\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10141\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10154\"\t1\r\n",
      "\"1000,http://www.microsoft.com/regwiz,10162\"\t1\r\n"
     ]
    }
   ],
   "source": [
    "!head hw4_4_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### HW4.5\n",
    "\n",
    "HW 4.5 Clustering Tweet Dataset\n",
    "\n",
    "Here you will use a different dataset consisting of word-frequency distributions \n",
    "for 1,000 Twitter users. These Twitter users use language in very different ways,\n",
    "and were classified by hand according to the criteria:\n",
    "\n",
    "0: Human, where only basic human-human communication is observed.\n",
    "\n",
    "1: Cyborg, where language is primarily borrowed from other sources\n",
    "(e.g., jobs listings, classifieds postings, advertisements, etc...).\n",
    "\n",
    "2: Robot, where language is formulaically derived from unrelated sources\n",
    "(e.g., weather/seismology, police/fire event logs, etc...).\n",
    "\n",
    "3: Spammer, where language is replicated to high multiplicity\n",
    "(e.g., celebrity obsessions, personal promotion, etc... )\n",
    "\n",
    "Check out the preprints of  recent research,\n",
    "which spawned this dataset:\n",
    "\n",
    "http://arxiv.org/abs/1505.04342\n",
    "http://arxiv.org/abs/1508.01843\n",
    "\n",
    "The main data lie in the accompanying file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words.txt\n",
    "\n",
    "and are of the form:\n",
    "\n",
    "USERID,CODE,TOTAL,WORD1_COUNT,WORD2_COUNT,...\n",
    ".\n",
    ".\n",
    "\n",
    "where\n",
    "\n",
    "USERID = unique user identifier\n",
    "CODE = 0/1/2/3 class code\n",
    "TOTAL = sum of the word counts\n",
    "\n",
    "Using this data, you will implement a 1000-dimensional K-means algorithm in MrJob on the users\n",
    "by their 1000-dimensional word stripes/vectors using several \n",
    "centroid initializations and values of K.\n",
    "\n",
    "Note that each \"point\" is a user as represented by 1000 words, and that\n",
    "word-frequency distributions are generally heavy-tailed power-laws\n",
    "(often called Zipf distributions), and are very rare in the larger class\n",
    "of discrete, random distributions. For each user you will have to normalize\n",
    "by its \"TOTAL\" column. Try several parameterizations and initializations:\n",
    "\n",
    "(A) K=4 uniform random centroid-distributions over the 1000 words (generate 1000 random numbers and normalize the vectors)\n",
    "\n",
    "(B) K=2 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "\n",
    "(C) K=4 perturbation-centroids, randomly perturbed from the aggregated (user-wide) distribution\n",
    "\n",
    "(D) K=4 \"trained\" centroids, determined by the sums across the classes. Use the (row-normalized) class-level aggregates as 'trained' starting centroids (i.e., the training is already done for you!). Note that you do not have to compute the aggregated distribution or the  class-aggregated distributions, which are rows in the auxiliary file:\n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "Row 1: Words\n",
    "\n",
    "Row 2: Aggregated distribution across all classes\n",
    "\n",
    "Row 3-6 class-aggregated distributions for clases 0-3\n",
    "\n",
    "For (A),  we select 4 users randomly from a uniform distribution [1,...,1,000]\n",
    "\n",
    "For (B), (C), and (D)  you will have to use data from the auxiliary file: \n",
    "\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt\n",
    "\n",
    "This file contains 5 special word-frequency distributions:\n",
    "\n",
    "(1) The 1000-user-wide aggregate, which you will perturb for initializations in parts (B) and (C), and (2-5) The 4 class-level aggregates for each of the user-type classes (0/1/2/3)\n",
    "\n",
    "\n",
    "In parts (B) and (C), you will have to perturb the 1000-user aggregate (after initially normalizing by its sum, which is also provided). So if in (B) you want to create 2 perturbations of the aggregate, start with (1), normalize, and generate 1000 random numbers uniformly from the unit interval (0,1) twice (for two centroids), using:\n",
    "\n",
    "from numpy import random\n",
    "numbers = random.sample(1000)\n",
    "\n",
    "Take these 1000 numbers and add them (component-wise) to the 1000-user aggregate, and then renormalize to obtain one of your aggregate-perturbed initial centroids.\n",
    "\n",
    "\n",
    "    ###################################################################################\n",
    "    ## Geneate random initial centroids around the global aggregate\n",
    "    ## Part (B) and (C) of this question\n",
    "    ###################################################################################\n",
    "\n",
    "    def startCentroidsBC(k):\n",
    "        counter = 0\n",
    "        for line in open(\"topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "            if counter == 2:        \n",
    "                data = re.split(\",\",line)\n",
    "                globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "            counter += 1\n",
    "        \\## perturb the global aggregate for the four initializations    \n",
    "        centroids = []\n",
    "        for i in range(k):\n",
    "            rndpoints = random.sample(1000)\n",
    "            peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "            centroids.append(peturpoints)\n",
    "            total = 0\n",
    "            for j in range(len(centroids[i])):\n",
    "                total += centroids[i][j]\n",
    "            for j in range(len(centroids[i])):\n",
    "                centroids[i][j] = centroids[i][j]/total\n",
    "        return centroids\n",
    "\n",
    "\n",
    "\n",
    "——\n",
    "For experiments A, B, C and D and iterate until a threshold (try 0.001) is reached. After convergence, print out a summary of the classes present in each cluster. In particular, report the composition as measured by the total portion of each class type (0-3) contained in each cluster, and discuss your findings and any differences in outcomes across parts A-D."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AAD5G0PHKMgdqTPC1w-2rR2ya/topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv 'topUsers_Apr-Jul_2014_1000-words_summaries.txt?dl=0' \\\n",
    "topUsers_Apr-Jul_2014_1000-words_summaries.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!mv topUsers_Apr-Jul_2014_1000-words_summaries.txt /home/cloudera/w261/HW4/data/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!wget \"https://www.dropbox.com/sh/m0nxsf4vs5cyrp2/AACBUw7kflSmuQJ7jn-uBMV1a/topUsers_Apr-Jul_2014_1000-words.txt?dl=0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!head -n 300 /home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt \\\n",
    "> /home/cloudera/w261/HW4/data/mini_topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting Kmeans.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile Kmeans.py\n",
    "#!/home/cloudera/anaconda2/bin/python\n",
    "\n",
    "from numpy import argmin, array, random\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "## function to determine the most similar cluster to a \n",
    "## data point\n",
    "def MinDist(datapoint, centroid_points):\n",
    "    \n",
    "    ## create np array for data point vector and centroids\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    \n",
    "    ## calculate difference between the data point\n",
    "    ## and each centroid and square the results\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    \n",
    "    ## select centroid with smallest Euclidean distance\n",
    "    ## to the data point and return it\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    # return the index for the closest centroid\n",
    "    return minidx\n",
    "\n",
    "## Check k-means convergence\n",
    "def stop_criterion(centroid_points_old, centroid_points_new,T):\n",
    "    \n",
    "    ## produce array of coordinates for old and new centroids\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    \n",
    "    ## compute the absolute difference between the new\n",
    "    ## and old values for each coordinate point\n",
    "    Diff = [abs(a-b) for a, b in zip(oldvalue, newvalue)]\n",
    "    \n",
    "    Flag = True\n",
    "    \n",
    "    ## Iterate through all parameters' absolute differences\n",
    "    for i in Diff:\n",
    "        \n",
    "        ## if any of the dimensions' differences is above the convergence\n",
    "        ## threshold, then fail the convergence check\n",
    "        if(i>T):\n",
    "            ## False flag entails failure to converge\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRKmeans(MRJob):\n",
    "    \n",
    "    centroid_points=[]\n",
    "    k=4    \n",
    "\n",
    "    ## steps include a mapper_init for the loading of the\n",
    "    ## centroids and a combiner to sum the points for the\n",
    "    ## average reqiured to calculate new centroids in the\n",
    "    ## reducer\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer\n",
    "                    )\n",
    "               ]\n",
    "\n",
    "    ## load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        ## turn each element into a float for each line in the file\n",
    "        ## we split the line by \"\\n\" and take the first element because\n",
    "        ## that is the line. we could also just do strip()\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"/home/cloudera/w261/HW4/src/Centroids.txt\", \"r\").readlines()]\n",
    "    ## mapper input: point\n",
    "    ## mapper output: key   = most similar cluster \n",
    "    ##                value = normalized data point \n",
    "    def mapper(self, _, line):\n",
    "        # convert the line into an array of floats\n",
    "        point = (map(float,line.split(',')))\n",
    "        norm_point = map(lambda x: x/point[2], point[3:])\n",
    "        # yield:\n",
    "        # key: centroid closest to the point\n",
    "        # value: the x- and y- coordinates of the point\n",
    "        #print int(MinDist(norm_point,self.centroid_points)), (1, norm_point)\n",
    "        yield (int(MinDist(norm_point,self.centroid_points)), (1, norm_point))\n",
    "\n",
    "    def combiner(self, idx, inputdata):\n",
    "        ## set the sum of rows to zero\n",
    "        num = 0\n",
    "        ## initialize aggregate for cluster to zero\n",
    "        agg_points = np.array([0.0] * 1000)\n",
    "        points = iter(inputdata)\n",
    "        for point in points:\n",
    "            num = num + point[0]\n",
    "            agg_points = agg_points + point[1:]\n",
    "        ## yield cluster (key), sum of cluster points\n",
    "        ## processed and coordinates sum (values)\n",
    "        yield idx, (num, agg_points.tolist())\n",
    "        \n",
    "    #Combine sum of data points locally\n",
    "    def reducer(self, idx, inputdata):\n",
    "        # initialize centroids list\n",
    "        centroids = []\n",
    "        # create an array of zeros of length k\n",
    "        num = [0]*self.k \n",
    "        # loop through the number of centroids \n",
    "        # and add the coordinates 0,0\n",
    "        for i in range(self.k):\n",
    "            centroids.append(np.array([0.0] * 1000))\n",
    "        points = iter(inputdata)\n",
    "        # for each coordinate in the input data\n",
    "        for point in points:\n",
    "            num[idx] = num[idx] + point[0]\n",
    "            centroids[idx] = centroids[idx] + point[1:]\n",
    "        centroids[idx] = centroids[idx] / num[idx]\n",
    "            \n",
    "        ## yield cluster (key), sum of cluster points\n",
    "        ## processed and coordinates sum (values)\n",
    "        yield idx, centroids[idx].tolist()\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    MRKmeans.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x Kmeans.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (A) ANSWER:\n",
    "\n",
    "Here we tackle part (A), initializing four clusters to four random users in the dataset and computing K-means until convergence. By analyzing the results we find that our clutering algorithm is fairly good at grouping humans together with humans: over 99.8% of humans were clustered in the same cluster by the algorithm.We also saw that, for the most part, spammers are also grouped together with humans in the same cluster: over 94% of all spammers were grouped with humans in the same cluster.\n",
    "\n",
    "|User Type|Predicted Cluster 0|Predicted Cluster 1| Predicted Cluster 2| Predicted Cluster 3|\n",
    "|---|---|---|---|---|\n",
    "|Human (0)|0.00%|99.87%|0.13%|0.00%|\n",
    "|Cyborg (1)|0.00%|2.20%|96.70%|1.10%|\n",
    "|Robot (2)|0.00%|29.63%|70.37%|0.00%|\n",
    "|Spammer (3)|1.94%|94.17%|3.88%|0.00%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151612.607144/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151612.607144/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration1:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151615.054801/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151615.054801/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151617.709914/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151617.709914/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151620.221871/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.151620.221871/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "Centroids found upon convergence!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy\n",
    "from Kmeans import MRKmeans, stop_criterion\n",
    "from itertools import chain\n",
    "\n",
    "mr_job = MRKmeans(args=['/home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt',\n",
    "                        '--file=Centroids.txt'])\n",
    "\n",
    "###########################\n",
    "# Geneate initial centroids\n",
    "###########################\n",
    "centroid_points = []\n",
    "k = 4\n",
    "init_users = numpy.random.choice(300, k)\n",
    "csv = numpy.genfromtxt ('/home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt',\n",
    "                     delimiter=\",\")\n",
    "\n",
    "# Normalizing the centroids chosen\n",
    "for i in init_users:\n",
    "    temp = map(lambda x: x/csv[i,2], csv[i,3:])\n",
    "    centroid_points.append(temp)\n",
    "\n",
    "#############################\n",
    "# Finish generating centroids\n",
    "#############################\n",
    "\n",
    "## Writing init centroids to \n",
    "## disk\n",
    "with open('Centroids.txt', 'w') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid_points[key] = value[0][0]\n",
    "        # Update the centroids for the next iteration\n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "        break\n",
    "print \"Centroids found upon convergence!\\n\"\n",
    "#print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ClustersCountJob.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile ClustersCountJob.py\n",
    "#!/home/cloudera/anaconda2/bin/python\n",
    "\n",
    "from numpy import argmin, array, random\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "\n",
    "## function to determine the most similar cluster to a \n",
    "## data point\n",
    "def MinDist(datapoint, centroid_points):\n",
    "    \n",
    "    ## create np array for data point vector and centroids\n",
    "    datapoint = array(datapoint)\n",
    "    centroid_points = array(centroid_points)\n",
    "    \n",
    "    ## calculate difference between the data point\n",
    "    ## and each centroid and square the results\n",
    "    diff = datapoint - centroid_points \n",
    "    diffsq = diff*diff\n",
    "    \n",
    "    ## select centroid with smallest Euclidean distance\n",
    "    ## to the data point and return it\n",
    "    minidx = argmin(list(diffsq.sum(axis = 1)))\n",
    "    # return the index for the closest centroid\n",
    "    return minidx\n",
    "\n",
    "class ClustersCountJob(MRJob):\n",
    "    \n",
    "    centroid_points=[]\n",
    "    k=4    \n",
    "\n",
    "    ## steps include a mapper_init for the loading of the\n",
    "    ## centroids and a combiner to sum the points for the\n",
    "    ## average reqiured to calculate new centroids in the\n",
    "    ## reducer\n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(mapper_init=self.mapper_init,\n",
    "                   mapper=self.mapper,\n",
    "                   combiner=self.combiner,\n",
    "                   reducer=self.reducer\n",
    "                    )\n",
    "               ]\n",
    "\n",
    "    ## load centroids info from file\n",
    "    def mapper_init(self):\n",
    "        print \"Current path:\", os.path.dirname(os.path.realpath(__file__))\n",
    "        ## turn each element into a float for each line in the file\n",
    "        ## we split the line by \"\\n\" and take the first element because\n",
    "        ## that is the line. we could also just do strip()\n",
    "        self.centroid_points = [map(float,s.split('\\n')[0].split(',')) for s in open(\"/home/cloudera/w261/HW4/src/Centroids.txt\", \"r\").readlines()]\n",
    "    ## mapper input: point\n",
    "    ## mapper output: key   = most similar cluster \n",
    "    ##                value = normalized data point \n",
    "    def mapper(self, _, line):\n",
    "        # convert the line into an array of floats\n",
    "        point = (map(float,line.split(',')))\n",
    "        norm_point = map(lambda x: x/point[2], point[3:])\n",
    "        point_class = point[1]\n",
    "        # yield:\n",
    "        # key: centroid closest to the point\n",
    "        # value: the x- and y- coordinates of the point\n",
    "        yield (int(MinDist(norm_point,self.centroid_points)),point_class), 1\n",
    "\n",
    "    #Combine sum of data points locally\n",
    "    def combiner(self, idx, count):\n",
    "        ## yield cluster (key), sum of cluster:true_type points\n",
    "        yield idx, sum(count)\n",
    "        \n",
    "    #Aggregate sum of cluster/real_type data points\n",
    "    def reducer(self, idx, count):\n",
    "        ## yield cluster (key), sum of cluster:true_type points\n",
    "        yield idx, sum(count)\n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    ClustersCountJob.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x ClustersCountJob.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ClustersCountJob.cloudera.20160612.151055.702285\n",
      "Running step 1 of 1...\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Streaming final output from /tmp/ClustersCountJob.cloudera.20160612.151055.702285/output...\n",
      "[2, 0.0]\t1\n",
      "[2, 1.0]\t88\n",
      "[2, 2.0]\t38\n",
      "[2, 3.0]\t4\n",
      "[3, 1.0]\t1\n",
      "[0, 3.0]\t2\n",
      "[1, 0.0]\t751\n",
      "[1, 1.0]\t2\n",
      "[1, 2.0]\t16\n",
      "[1, 3.0]\t97\n",
      "Removing temp directory /tmp/ClustersCountJob.cloudera.20160612.151055.702285...\n"
     ]
    }
   ],
   "source": [
    "!./ClustersCountJob.py /home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (B) ANSWER:\n",
    "\n",
    "Next, I run the same MRJob framework but for K=2 starting from perturbation centroids derived from the aggregate (user-wide) distribution. Similarly to part (A), we find that our clutering algorithm is fairly good at grouping humans together with humans: over 99.8% of humans were clustered in the same cluster by the algorithm (only one human was placed in another cluster). We also saw that, for the most part, spammers are also grouped together with humans in the same cluster: over 94% of all spammers were grouped with humans in the same cluster. This makes sense  in the case where K=2 because we are forcing a structure onto the data that does not correspond to its natural clusters (there are four types of users as opposed to two).\n",
    "\n",
    "|User Type|Predicted Cluster 0|Predicted Cluster 1|\n",
    "|---|---|---|\n",
    "|Human (0)|0.13%|99.87%|\n",
    "|Cyborg (1)|96.7%|3.3%|\n",
    "|Robot (2)|74.07%|25.93%|\n",
    "|Spammer (3)|3.88%|96.12%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153016.303417/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153016.303417/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration1:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153018.986526/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153018.986526/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153022.091249/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153022.091249/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153025.425049/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153025.425049/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration4:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153029.031329/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.153029.031329/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "Centroids found upon convergence!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmin, array, random\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "import re\n",
    "\n",
    "###################################################################################\n",
    "## Geneate random initial centroids around the global aggregate\n",
    "## Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"/home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 1:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "\n",
    "centroid_points = startCentroidsBC(2)\n",
    "\n",
    "## Writing init centroids to \n",
    "## disk\n",
    "with open('Centroids.txt', 'w') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid_points[key] = value[0][0]\n",
    "        # Update the centroids for the next iteration\n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "        break\n",
    "print \"Centroids found upon convergence!\\n\"\n",
    "#print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ClustersCountJob.cloudera.20160612.153035.497758\n",
      "Running step 1 of 1...\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Streaming final output from /tmp/ClustersCountJob.cloudera.20160612.153035.497758/output...\n",
      "[1, 1.0]\t3\n",
      "[1, 2.0]\t14\n",
      "[1, 3.0]\t99\n",
      "[0, 0.0]\t1\n",
      "[0, 1.0]\t88\n",
      "[0, 2.0]\t40\n",
      "[0, 3.0]\t4\n",
      "[1, 0.0]\t751\n",
      "Removing temp directory /tmp/ClustersCountJob.cloudera.20160612.153035.497758...\n"
     ]
    }
   ],
   "source": [
    "!./ClustersCountJob.py /home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (C) ANSWER:\n",
    "\n",
    "Now, I run the same MRJob framework but for K=4 starting from perturbation centroids derived from the aggregate (user-wide) distribution. We find that in this case, the perturbation centroids lead to less \"pure\" clusters as seen in the table below. Particularly, it seems that our predicted cluster 2 is \"taking\" a sizeable portion of each of the user types (particularly human, robot and spammer).\n",
    "\n",
    "|User Type|Predicted Cluster 0|Predicted Cluster 1| Predicted Cluster 2| Predicted Cluster 3|\n",
    "|---|---|---|---|---|\n",
    "|Human (0)|0.13%|74.34%|25.53%|0.00%|\n",
    "|Cyborg (1)|40.66%|1.10%|2.20%|56.04%|\n",
    "|Robot (2)|70.37%|0.00%|25.93%|3.70%|\n",
    "|Spammer (3)|3.88%|70.87%|25.24%|0.00%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171009.088942/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171009.088942/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration1:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171011.396354/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171011.396354/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171013.426535/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171013.426535/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171018.260057/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171018.260057/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration4:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171020.822001/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171020.822001/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration5:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171023.703347/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171023.703347/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration6:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171026.179401/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171026.179401/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration7:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171029.011156/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171029.011156/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration8:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171031.515514/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171031.515514/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration9:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171033.847986/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171033.847986/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "Centroids found upon convergence!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmin, array, random\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "import re\n",
    "\n",
    "###################################################################################\n",
    "## Geneate random initial centroids around the global aggregate\n",
    "## Part (B) and (C) of this question\n",
    "###################################################################################\n",
    "\n",
    "def startCentroidsBC(k):\n",
    "    counter = 0\n",
    "    for line in open(\"/home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 1:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3])/float(data[2]) for i in range(1000)]\n",
    "        counter += 1\n",
    "    ## perturb the global aggregate for the four initializations    \n",
    "    centroids = []\n",
    "    for i in range(k):\n",
    "        rndpoints = random.sample(1000)\n",
    "        peturpoints = [rndpoints[n]/10+globalAggregate[n] for n in range(1000)]\n",
    "        centroids.append(peturpoints)\n",
    "        total = 0\n",
    "        for j in range(len(centroids[i])):\n",
    "            total += centroids[i][j]\n",
    "        for j in range(len(centroids[i])):\n",
    "            centroids[i][j] = centroids[i][j]/total\n",
    "    return centroids\n",
    "\n",
    "centroid_points = startCentroidsBC(4)\n",
    "\n",
    "## Writing init centroids to \n",
    "## disk\n",
    "with open('Centroids.txt', 'w') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid_points[key] = value[0][0]\n",
    "        # Update the centroids for the next iteration\n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    \n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "        break\n",
    "print \"Centroids found upon convergence!\\n\"\n",
    "# print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ClustersCountJob.cloudera.20160612.153250.204096\n",
      "Running step 1 of 1...\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Streaming final output from /tmp/ClustersCountJob.cloudera.20160612.153250.204096/output...\n",
      "[2, 0.0]\t192\n",
      "[2, 1.0]\t2\n",
      "[2, 2.0]\t14\n",
      "[2, 3.0]\t26\n",
      "[3, 1.0]\t51\n",
      "[3, 2.0]\t2\n",
      "[0, 0.0]\t1\n",
      "[0, 1.0]\t37\n",
      "[0, 2.0]\t38\n",
      "[0, 3.0]\t4\n",
      "[1, 0.0]\t559\n",
      "[1, 1.0]\t1\n",
      "[1, 3.0]\t73\n",
      "Removing temp directory /tmp/ClustersCountJob.cloudera.20160612.153250.204096...\n"
     ]
    }
   ],
   "source": [
    "!./ClustersCountJob.py /home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### (D) ANSWER:\n",
    "\n",
    "Finally, I run the same MRJob framework but for K=4 starting from 'trained' centroids determined by the sums across the classes. Use the (row-normalized) class-level aggregates as 'trained' starting centroids. We find that in this case, the centroids do a better job at clustering than in part (C), but we still get a significant portion of diferent classes in the same cluster (see predicted clusters 0 and 2). Interestingly, we find that predicted cluster 2 is purely made up of Cyborg users and predicted cluster 3 is virtually made up of only Spammer users.\n",
    "\n",
    "|User Type|Predicted Cluster 0|Predicted Cluster 1| Predicted Cluster 2| Predicted Cluster 3|\n",
    "|---|---|---|---|---|\n",
    "|Human (0)|99.60%|0.00%|0.13%|0.27%|\n",
    "|Cyborg (1)|3.30%|56.04%|40.66%|0.00%|\n",
    "|Robot (2)|25.93%|0.00%|74.07%|0.00%|\n",
    "|Spammer (3)|40.78%|0.00%|3.88%|55.34%|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration0:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171943.457253/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171943.457253/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration1:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171945.786934/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171945.786934/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration2:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171947.749945/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171947.749945/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration3:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171949.736340/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171949.736340/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration4:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171951.643949/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171951.643949/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "iteration5:\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171953.707155/job_local_dir/0/mapper/0\n",
      "Current path: /tmp/Kmeans.cloudera.20160612.171953.707155/job_local_dir/0/mapper/1\n",
      "\n",
      "\n",
      "Centroids found upon convergence!\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from numpy import argmin, array, random\n",
    "import numpy as np\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "from itertools import chain\n",
    "import os\n",
    "import re\n",
    "\n",
    "\n",
    "def startCentroidsD(k):\n",
    "    counter = 0\n",
    "    centroids = []\n",
    "    for line in open(\"/home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words_summaries.txt\").readlines():\n",
    "        if counter == 1:        \n",
    "            data = re.split(\",\",line)\n",
    "            globalAggregate = [float(data[i+3]) for i in range(1000)]\n",
    "        elif counter > 1:\n",
    "            data = re.split(\",\",line)\n",
    "            trainedpoints = [float(data[n+3])/globalAggregate[n] for n in range(1000)]\n",
    "            centroids.append(trainedpoints)\n",
    "            total = 0\n",
    "            for j in range(len(centroids[counter-2])):\n",
    "                total += centroids[counter-2][j]\n",
    "            for j in range(len(centroids[counter-2])):\n",
    "                centroids[counter-2][j] = centroids[counter-2][j]/total\n",
    "        counter += 1\n",
    "    return centroids\n",
    "\n",
    "centroid_points = startCentroidsD(4)\n",
    "\n",
    "# Writing init centroids to \n",
    "# disk\n",
    "with open('Centroids.txt', 'w') as f:\n",
    "        f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "\n",
    "# Update centroids iteratively\n",
    "i = 0\n",
    "while(1):\n",
    "    # save previous centoids to check convergency\n",
    "    centroid_points_old = centroid_points[:]\n",
    "    print \"iteration\"+str(i)+\":\"\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        # stream_output: get access of the output \n",
    "        for line in runner.stream_output():\n",
    "            key,value =  mr_job.parse_output_line(line)\n",
    "            centroid_points[key] = value[0][0]\n",
    "        # Update the centroids for the next iteration\n",
    "        with open('Centroids.txt', 'w') as f:\n",
    "            f.writelines(','.join(str(j) for j in i) + '\\n' for i in centroid_points)\n",
    "    print \"\\n\"\n",
    "    i = i + 1\n",
    "    if(stop_criterion(centroid_points_old,centroid_points,0.001)):\n",
    "        break\n",
    "print \"Centroids found upon convergence!\\n\"\n",
    "#print centroid_points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No configs found; falling back on auto-configuration\n",
      "Creating temp directory /tmp/ClustersCountJob.cloudera.20160612.172025.738234\n",
      "Running step 1 of 1...\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Current path: /home/cloudera/w261/HW4/src\n",
      "Streaming final output from /tmp/ClustersCountJob.cloudera.20160612.172025.738234/output...\n",
      "[2, 1.0]\t37\n",
      "[2, 2.0]\t40\n",
      "[2, 3.0]\t4\n",
      "[3, 0.0]\t2\n",
      "[3, 3.0]\t57\n",
      "[0, 0.0]\t749\n",
      "[0, 1.0]\t3\n",
      "[0, 2.0]\t14\n",
      "[0, 3.0]\t42\n",
      "[1, 1.0]\t51\n",
      "[2, 0.0]\t1\n",
      "Removing temp directory /tmp/ClustersCountJob.cloudera.20160612.172025.738234...\n"
     ]
    }
   ],
   "source": [
    "!./ClustersCountJob.py /home/cloudera/w261/HW4/data/topUsers_Apr-Jul_2014_1000-words.txt"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
