{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "application/javascript": [
       "/**********************************************************************************************\n",
       "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
       "https://github.com/mathjax/MathJax/issues/1300\n",
       "A quick hack to fix this based on stackoverflow discussions: \n",
       "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
       "**********************************************************************************************/\n",
       "\n",
       "$('.math>span').css(\"border-left-color\",\"transparent\")"
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%javascript\n",
    "/**********************************************************************************************\n",
    "Known Mathjax Issue with Chrome - a rounding issue adds a border to the right of mathjax markup\n",
    "https://github.com/mathjax/MathJax/issues/1300\n",
    "A quick hack to fix this based on stackoverflow discussions: \n",
    "http://stackoverflow.com/questions/34277967/chrome-rendering-mathjax-equations-with-a-trailing-vertical-line\n",
    "**********************************************************************************************/\n",
    "\n",
    "$('.math>span').css(\"border-left-color\",\"transparent\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# DATASCI W261 - Machine Learning At Scale\n",
    "## Assignment - Week 09\n",
    "---\n",
    "__Name:__  Carlos Eduardo Rodriguez Castillo   \n",
    "__Class:__ W261 - Section 2     \n",
    "__Email:__  cerodriguez@ischool.berkeley.edu     \n",
    "__Week:__   09 \n",
    "\n",
    "\n",
    "---\n",
    "### Instructions\n",
    "\n",
    "Due by 07/17/2016\n",
    "\n",
    "[Submission Link - Google Form](https://docs.google.com/forms/d/1ZOr9RnIe_A06AcZDB6K1mJN4vrLeSmS2PD6Xm3eOiis/viewform?usp=send_form) \n",
    "\n",
    "### Documents:\n",
    "* IPython Notebook, published and viewable online.\n",
    "* PDF export of IPython Notebook.\n",
    "    \n",
    "### Useful References\n",
    "\n",
    "* Data-intensive text processing with MapReduce. San Rafael, CA: Morgan & Claypool Publishers. Chapter 5. \n",
    "\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\">HW 9 Dataset</h2>\n",
    "\n",
    "Note that all referenced files life in the enclosing directory. [Checkout the Data subdirectory on Dropbox](https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0) or the AWS S3 buckets (details contained each question). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.0: Short answer questions </h2>\n",
    "\n",
    "__ What is PageRank and what is it used for in the context of web search?__\n",
    "\n",
    "##### ANSWER:\n",
    "\n",
    "PageRank is an algorithm for ranking the relevance or popularity of nodes in a graph based on the steady-state probability distribution of the nodes in the graph.\n",
    "\n",
    "PageRank is the backbone of modern web search. Larry Page and Sergei Brin used PageRank as the core of the Google search engine which critically aided them in making Google the most popular web search solution in the world.\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ What modifications have to be made to the webgraph in order to leverage the machinery of Markov Chains to compute the Steady State Distibution? __\n",
    "\n",
    "In order to leverage the machinery of Markov Chains to compute the steady-state distribution\n",
    "\n",
    "need to deal with dangling node (no outlinks; needs to be processed for the purposes of not having ) and teleportation\n",
    "\n",
    "<hr>\n",
    "\n",
    "__ OPTIONAL: In topic-specific pagerank, how can we ensure that the irreducible property is satifsied? (HINT: see HW9.4) __\n",
    "\n",
    "\n",
    "\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.1: MRJob implementation of basic PageRank </h2>\n",
    "\n",
    "Write a basic MRJob implementation of the iterative PageRank algorithm that takes sparse adjacency lists as input (as explored in HW 7).\n",
    "\n",
    "Make sure that you implementation utilizes teleportation (1-damping/the number of nodes in the network), and further, distributes the mass of dangling nodes with each iteration so that the output of each iteration is correctly normalized (sums to 1).\n",
    "\n",
    "\n",
    "[NOTE: The PageRank algorithm assumes that a random surfer (walker), starting from a random web page, chooses the next page to which it will move by clicking at random, with probability d,one of the hyperlinks in the current page. This probability is represented by a so-called *damping factor* d, where d ∈ (0, 1). Otherwise, with probability (1 − d), the surfer jumps to any web page in the network. If a page is a dangling end, meaning it has no outgoing hyperlinks, the random surfer selects an arbitrary web page from a uniform distribution and “teleports” to that page]\n",
    "\n",
    "\n",
    "As you build your code, use the test data:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/PageRank-test.txt\n",
    "\n",
    "Or under the Data Subfolder for HW7 on Dropbox with the same file name. \n",
    "> Dropbox: https://www.dropbox.com/sh/2c0k5adwz36lkcw/AAAAKsjQfF9uHfv-X9mCqr9wa?dl=0\n",
    "\n",
    "with teleportation parameter set to 0.15 (1-d, where d, the damping factor is set to 0.85), and crosscheck your work with the true result, displayed in the first image in the [Wikipedia article](https://en.wikipedia.org/wiki/PageRank)\n",
    "and here for reference are the corresponding PageRank probabilities:\n",
    "<pre>\n",
    "\n",
    "A, 0.033\n",
    "B, 0.384\n",
    "C, 0.343\n",
    "D, 0.039\n",
    "E, 0.081\n",
    "F, 0.039\n",
    "G, 0.016\n",
    "H, 0.016\n",
    "I, 0.016\n",
    "J, 0.016\n",
    "K, 0.016\n",
    "\n",
    "</pre>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.1 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2016-07-13 15:29:36--  https://www.dropbox.com/sh/2c0k5adwz36lkcw/AACf_33TeM7rKfEhjEoNmMCaa/PageRank-test_indexed.txt?dl=0\n",
      "Resolving www.dropbox.com... 162.125.4.1\n",
      "Connecting to www.dropbox.com|162.125.4.1|:443... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://dl.dropboxusercontent.com/content_link/dEcSLmCwpZqwbtawvqUGR1Wc2pfe4C9n5O5zKQqFGi7U2MlOxGfUnW8iMgvJiHh7/file [following]\n",
      "--2016-07-13 15:29:36--  https://dl.dropboxusercontent.com/content_link/dEcSLmCwpZqwbtawvqUGR1Wc2pfe4C9n5O5zKQqFGi7U2MlOxGfUnW8iMgvJiHh7/file\n",
      "Resolving dl.dropboxusercontent.com... 108.160.173.5\n",
      "Connecting to dl.dropboxusercontent.com|108.160.173.5|:443... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 168 [text/plain]\n",
      "Saving to: “/home/cloudera/w261/HW9/data/toy?example.txt”\n",
      "\n",
      "100%[======================================>] 168         --.-K/s   in 0s      \n",
      "\n",
      "2016-07-13 15:29:38 (12.8 MB/s) - “/home/cloudera/w261/HW9/data/toy?example.txt” saved [168/168]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!wget \"https://www.dropbox.com/sh/2c0k5adwz36lkcw/AACf_33TeM7rKfEhjEoNmMCaa/PageRank-test_indexed.txt?dl=0#\" \\\n",
    "-O /home/cloudera/w261/HW9/data/toy?example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t{'3': 1}\r\n",
      "3\t{'2': 1}\r\n",
      "4\t{'1': 1, '2': 1}\r\n",
      "5\t{'4': 1, '2': 1, '6': 1}\r\n",
      "6\t{'2': 1, '5': 1}\r\n",
      "7\t{'2': 1, '5': 1}\r\n",
      "8\t{'2': 1, '5': 1}\r\n",
      "9\t{'2': 1, '5': 1}\r\n",
      "10\t{'5': 1}\r\n",
      "11\t{'5': 1}\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW9/data/toy/example.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_graph_prep.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_graph_prep.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRpageRankGraphPrep(MRJob):\n",
    "    \n",
    "\n",
    "    def steps(self):\n",
    "        JOBCONF2 = {\n",
    "              'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "              'mapred.text.key.comparator.options': '-k1n -k2nr',\n",
    "              'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "        return [MRStep(jobconf=JOBCONF2,\n",
    "                    mapper=self.mapper, \n",
    "                    reducer=self.reducer)\n",
    "                ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        neighbors = ast.literal_eval(value)\n",
    "        neighbors_string = \"\"\n",
    "        for n in neighbors.keys():\n",
    "            if neighbors_string == \"\":\n",
    "                neighbors_string = n\n",
    "            else:\n",
    "                neighbors_string = neighbors_string+\",\"+n\n",
    "        yield int(key), neighbors_string+\",\"+\"1\"\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        for v in values:\n",
    "            yield key, v\n",
    "    \n",
    "if __name__ == \"__main__\":\n",
    "    MRpageRankGraphPrep.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!chmod a+x PageRank_graph_prep.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/PageRank_graph_prep.cloudera.20160715.012934.416849\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep.cloudera.20160715.012934.416849/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Detected hadoop configuration property names that do not match hadoop version 2.6.0:\n",
      "The have been translated as follows\n",
      " mapred.output.key.comparator.class: mapreduce.job.output.key.comparator.class\n",
      "mapred.reduce.tasks: mapreduce.job.reduces\n",
      "mapred.text.key.comparator.options: mapreduce.partition.keycomparator.options\n",
      "Running step 1 of 1...\n",
      "  mapred.output.key.comparator.class is deprecated. Instead, use mapreduce.job.output.key.comparator.class\n",
      "  mapred.reduce.tasks is deprecated. Instead, use mapreduce.job.reduces\n",
      "  mapred.text.key.comparator.options is deprecated. Instead, use mapreduce.partition.keycomparator.options\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8602910582170859018.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1468516795796_0014\n",
      "  Submitted application application_1468516795796_0014\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1468516795796_0014/\n",
      "  Running job: job_1468516795796_0014\n",
      "  Job job_1468516795796_0014 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1468516795796_0014 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep.cloudera.20160715.012934.416849/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=252\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=96\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=105\n",
      "\t\tFILE: Number of bytes written=368713\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=610\n",
      "\t\tHDFS: Number of bytes written=96\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3452928\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=916352\n",
      "\t\tTotal time spent by all map tasks (ms)=26976\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3452928\n",
      "\t\tTotal time spent by all reduce tasks (ms)=7159\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=916352\n",
      "\t\tTotal vcore-seconds taken by all map tasks=26976\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=7159\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2180\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=230\n",
      "\t\tInput split bytes=358\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=96\n",
      "\t\tMap output materialized bytes=128\n",
      "\t\tMap output records=10\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=398610432\n",
      "\t\tReduce input groups=10\n",
      "\t\tReduce input records=10\n",
      "\t\tReduce output records=10\n",
      "\t\tReduce shuffle bytes=128\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=20\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\t\tVirtual memory (bytes) snapshot=2098704384\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep.cloudera.20160715.012934.416849/output...\n",
      "2\t\"3,1\"\n",
      "3\t\"2,1\"\n",
      "4\t\"1,2,1\"\n",
      "5\t\"2,4,6,1\"\n",
      "6\t\"2,5,1\"\n",
      "7\t\"2,5,1\"\n",
      "8\t\"2,5,1\"\n",
      "9\t\"2,5,1\"\n",
      "10\t\"5,1\"\n",
      "11\t\"5,1\"\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep.cloudera.20160715.012934.416849...\n",
      "Removing temp directory /tmp/PageRank_graph_prep.cloudera.20160715.012934.416849...\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!./PageRank_graph_prep.py -r hadoop /home/cloudera/w261/HW9/data/toy/example_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_graph_prep_1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_graph_prep_1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRpageRankGraphPrep(MRJob):\n",
    "    \n",
    "    num_nodes = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        JOB_CONF_STEP = {\n",
    "            'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1n -k2nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        neighbors = ast.literal_eval(value)\n",
    "        neighbors_array = []\n",
    "        for n in neighbors.keys():\n",
    "#             if neighbors_string == \"\":\n",
    "            neighbors_array.append(n)\n",
    "#             else:\n",
    "#                 neighbors_string = neighbors_string+\",\"+n\n",
    "            yield \"*\"+n, \"*\"\n",
    "            yield n, \"-\"\n",
    "        yield \"*\"+key, \"*\"\n",
    "        yield key, neighbors_array\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        neighbors = []\n",
    "        if key[0] == \"*\":\n",
    "#             self.increment_counter(\"Nodes\",\"Num_nodes\",1)\n",
    "            self.num_nodes += 1\n",
    "            sys.stderr.write(\"The number of nodes so far is %d\\n\"%self.num_nodes)\n",
    "        else:\n",
    "            for v in values:\n",
    "                if neighbors == \"\":\n",
    "                    if v != \"-\":\n",
    "                        neighbors = v\n",
    "                else:\n",
    "                    if v != \"-\":\n",
    "                        neighbors.extend(v)\n",
    "            yield key, (neighbors, 1.0/float(self.num_nodes))\n",
    "#         for v in values:\n",
    "#             yield key, v[0]\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRpageRankGraphPrep.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x PageRank_graph_prep_1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/PageRank_graph_prep_1.cloudera.20160715.175947.910971\n",
      "Running step 1 of 1...\n",
      "Traceback (most recent call last):\n",
      "  File \"./PageRank_graph_prep_1.py\", line 60, in <module>\n",
      "    MRpageRankGraphPrep.run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 430, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 448, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 160, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 230, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/runner.py\", line 473, in run\n",
      "    self._run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/sim.py\", line 185, in _run\n",
      "    self._invoke_step(step_num, 'reducer')\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/sim.py\", line 259, in _invoke_step\n",
      "    working_dir, env)\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/inline.py\", line 157, in _run_step\n",
      "    child_instance.execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 445, in execute\n",
      "    self.run_reducer(self.options.step_num)\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 549, in run_reducer\n",
      "    for out_key, out_value in reducer(key, values) or ():\n",
      "  File \"./PageRank_graph_prep_1.py\", line 55, in reducer\n",
      "    yield key, (neighbors, 1.0/float(self.num_nodes))\n",
      "ZeroDivisionError: float division by zero\n"
     ]
    }
   ],
   "source": [
    "!./PageRank_graph_prep_1.py /home/cloudera/w261/HW9/data/toy/example_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/PageRank_graph_prep_1.cloudera.20160716.044352.810563\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.044352.810563/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob4556544410363699565.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1468516795796_0057\n",
      "  Submitted application application_1468516795796_0057\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1468516795796_0057/\n",
      "  Running job: job_1468516795796_0057\n",
      "  Job job_1468516795796_0057 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1468516795796_0057 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.044352.810563/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=252\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=408\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=260\n",
      "\t\tFILE: Number of bytes written=368055\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=616\n",
      "\t\tHDFS: Number of bytes written=408\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3251840\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=741376\n",
      "\t\tTotal time spent by all map tasks (ms)=25405\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3251840\n",
      "\t\tTotal time spent by all reduce tasks (ms)=5792\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=741376\n",
      "\t\tTotal vcore-seconds taken by all map tasks=25405\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=5792\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=1730\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=107\n",
      "\t\tInput split bytes=364\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=518\n",
      "\t\tMap output materialized bytes=311\n",
      "\t\tMap output records=54\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=389160960\n",
      "\t\tReduce input groups=22\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=311\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=108\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\t\tVirtual memory (bytes) snapshot=2098880512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.044352.810563/output...\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.044352.810563...\n",
      "Removing temp directory /tmp/PageRank_graph_prep_1.cloudera.20160716.044352.810563...\n"
     ]
    }
   ],
   "source": [
    "!mkdir -p /home/cloudera/w261/HW9/data/toy_prepared\n",
    "!./PageRank_graph_prep_1.py -r hadoop /home/cloudera/w261/HW9/data/toy/example_1.txt \\\n",
    "> /home/cloudera/w261/HW9/data/toy_prepared/input_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_graph_prep_2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_graph_prep_2.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRpageRankGraphPrep(MRJob):\n",
    "    \n",
    "    num_nodes = 0\n",
    "    \n",
    "    def step(self):\n",
    "        \n",
    "        JOB_CONF_STEP = {\n",
    "            'mapred.output.key.comparator.class':'org.apache.hadoop.mapred.lib.KeyFieldBasedComparator',\n",
    "            'mapred.text.key.comparator.options': '-k1n -k2nr',\n",
    "            'mapred.reduce.tasks': '1'\n",
    "        }\n",
    "        \n",
    "        return [\n",
    "            MRStep(#jobconf=JOB_CONF_STEP,\n",
    "                   mapper=self.mapper,\n",
    "                   reducer=self.reducer)\n",
    "        ]\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        neighbors = ast.literal_eval(value)\n",
    "        neighbors_array = []\n",
    "        for n in neighbors.keys():\n",
    "            neighbors_array.append(n)\n",
    "            yield n, \"-\"\n",
    "        yield key, neighbors_array\n",
    "        \n",
    "    def reducer(self, key, values):\n",
    "        neighbors = []\n",
    "        self.increment_counter(\"Nodes\",\"Num_nodes\",1)\n",
    "        for v in values:\n",
    "            if neighbors == \"\":\n",
    "                if v != \"-\":\n",
    "                    neighbors = v\n",
    "            else:\n",
    "                if v != \"-\":\n",
    "                    neighbors.extend(v)\n",
    "        yield key, (neighbors, 1.0)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRpageRankGraphPrep.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x PageRank_graph_prep_2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/PageRank_graph_prep_2.cloudera.20160716.221234.792099\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_2.cloudera.20160716.221234.792099/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob3587476104058141132.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1468516795796_0062\n",
      "  Submitted application application_1468516795796_0062\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1468516795796_0062/\n",
      "  Running job: job_1468516795796_0062\n",
      "  Job job_1468516795796_0062 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1468516795796_0062 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_2.cloudera.20160716.221234.792099/output\n",
      "Counters: 50\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=252\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=221\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=183\n",
      "\t\tFILE: Number of bytes written=367889\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=616\n",
      "\t\tHDFS: Number of bytes written=221\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=4716928\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1464192\n",
      "\t\tTotal time spent by all map tasks (ms)=36851\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=4716928\n",
      "\t\tTotal time spent by all reduce tasks (ms)=11439\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1464192\n",
      "\t\tTotal vcore-seconds taken by all map tasks=36851\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=11439\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2790\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=213\n",
      "\t\tInput split bytes=364\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=273\n",
      "\t\tMap output materialized bytes=222\n",
      "\t\tMap output records=27\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=358289408\n",
      "\t\tReduce input groups=11\n",
      "\t\tReduce input records=27\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=222\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=54\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\t\tVirtual memory (bytes) snapshot=2098888704\n",
      "\tNodes\n",
      "\t\tNum_nodes=11\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_2.cloudera.20160716.221234.792099/output...\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_2.cloudera.20160716.221234.792099...\n",
      "Removing temp directory /tmp/PageRank_graph_prep_2.cloudera.20160716.221234.792099...\n"
     ]
    }
   ],
   "source": [
    "!./PageRank_graph_prep_2.py -r hadoop /home/cloudera/w261/HW9/data/toy/example_1.txt \\\n",
    "> /home/cloudera/w261/HW9/data/toy_prepared_graph_input/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\t{'3': 1}\r\n",
      "3\t{'2': 1}\r\n",
      "4\t{'1': 1, '2': 1}\r\n",
      "5\t{'4': 1, '2': 1, '6': 1}\r\n",
      "6\t{'2': 1, '5': 1}\r\n",
      "7\t{'2': 1, '5': 1}\r\n",
      "8\t{'2': 1, '5': 1}\r\n",
      "9\t{'2': 1, '5': 1}\r\n",
      "10\t{'5': 1}\r\n",
      "11\t{'5': 1}\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW9/data/toy/example_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /tmp/no_script.cloudera.20160716.223709.782630\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/no_script.cloudera.20160716.223709.782630/files/...\n",
      "j-1I28YD5XHICD3\n"
     ]
    }
   ],
   "source": [
    "## first we spool up a cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-2 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `/home/cloudera/w261/HW9/data/toy_directed_graph_input': No such file or directory\n",
      "NUM NODES IS 11\n",
      "iteration 0 of Pagerank algorithm\n",
      "iteration 1 of Pagerank algorithm\n",
      "iteration 2 of Pagerank algorithm\n",
      "iteration 3 of Pagerank algorithm\n",
      "iteration 4 of Pagerank algorithm\n",
      "iteration 5 of Pagerank algorithm\n",
      "iteration 6 of Pagerank algorithm\n",
      "iteration 7 of Pagerank algorithm\n",
      "iteration 8 of Pagerank algorithm\n",
      "iteration 9 of Pagerank algorithm\n",
      "iteration 10 of Pagerank algorithm\n",
      "iteration 11 of Pagerank algorithm\n",
      "iteration 12 of Pagerank algorithm\n",
      "iteration 13 of Pagerank algorithm\n",
      "iteration 14 of Pagerank algorithm\n",
      "iteration 15 of Pagerank algorithm\n",
      "iteration 16 of Pagerank algorithm\n",
      "iteration 17 of Pagerank algorithm\n",
      "iteration 18 of Pagerank algorithm\n",
      "iteration 19 of Pagerank algorithm\n",
      "iteration 20 of Pagerank algorithm\n",
      "iteration 21 of Pagerank algorithm\n",
      "iteration 22 of Pagerank algorithm\n",
      "iteration 23 of Pagerank algorithm\n",
      "iteration 24 of Pagerank algorithm\n",
      "iteration 25 of Pagerank algorithm\n",
      "iteration 26 of Pagerank algorithm\n",
      "iteration 27 of Pagerank algorithm\n",
      "iteration 28 of Pagerank algorithm\n",
      "iteration 29 of Pagerank algorithm\n",
      "iteration 30 of Pagerank algorithm\n",
      "iteration 31 of Pagerank algorithm\n",
      "iteration 32 of Pagerank algorithm\n",
      "iteration 33 of Pagerank algorithm\n",
      "iteration 34 of Pagerank algorithm\n",
      "iteration 35 of Pagerank algorithm\n",
      "iteration 36 of Pagerank algorithm\n",
      "iteration 37 of Pagerank algorithm\n",
      "iteration 38 of Pagerank algorithm\n",
      "iteration 39 of Pagerank algorithm\n",
      "\"1\"\t[[], 0.03278149316111876]\n",
      "\"10\"\t[[\"5\"], 0.016169479017118762]\n",
      "\"11\"\t[[\"5\"], 0.016169479017118762]\n",
      "\"2\"\t[[\"3\"], 0.3842426353874476]\n",
      "\"3\"\t[[\"2\"], 0.3430685989242862]\n",
      "\"4\"\t[[\"1\", \"2\"], 0.03908709210193935]\n",
      "\"5\"\t[[\"2\", \"4\", \"6\"], 0.08088569323767447]\n",
      "\"6\"\t[[\"2\", \"5\"], 0.03908709210193935]\n",
      "\"7\"\t[[\"2\", \"5\"], 0.016169479017118762]\n",
      "\"8\"\t[[\"2\", \"5\"], 0.016169479017118762]\n",
      "\"9\"\t[[\"2\", \"5\"], 0.016169479017118762]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random,array\n",
    "# from PageRank_step1 import MRpageRank1\n",
    "from PageRank_graph_prep_2 import MRpageRankGraphPrep\n",
    "from PageRank_step2 import MRpageRank2\n",
    "import ast\n",
    "import os\n",
    "\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_output\n",
    "!mkdir /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy_directed_graph_input\n",
    "\n",
    "mr_job = MRpageRankGraphPrep(args=[\"/home/cloudera/w261/HW9/data/toy/example_1.txt\",\n",
    "                                    \"--output-dir\",\n",
    "                                    \"/home/cloudera/w261/HW9/data/toy_prepared_graph_input\"])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        counters = runner.counters()[0]\n",
    "        num_nodes = counters['Nodes']['Num_nodes']\n",
    "\n",
    "print \"NUM NODES IS %d\"%num_nodes\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "\n",
    "    if i == 0:\n",
    "        first_run = 1\n",
    "    else:\n",
    "        first_run = 0\n",
    "    mr_job = MRpageRank2(args=[\"/home/cloudera/w261/HW9/data/toy_prepared_graph_input\",\n",
    "                               \"--alpha\",0.15,\n",
    "                               \"--num-nodes\", str(num_nodes),\n",
    "                               \"--first-run\", str(first_run),\n",
    "                                         \"--output-dir\",\n",
    "                               \"/home/cloudera/w261/HW9/data/toy_prepared_graph_output\"])\n",
    "    \n",
    "    print \"iteration \"+str(i)+\" of Pagerank algorithm\"\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "    i = i + 1\n",
    "\n",
    "    !rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "    !cp -rT /home/cloudera/w261/HW9/data/toy_prepared_graph_output /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "\n",
    "    if i >=40:\n",
    "        break\n",
    "!cat /home/cloudera/w261/HW9/data/toy_prepared_graph_output/*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "node: \"5\"\tpageRank: 0.081\n",
      "node: \"6\"\tpageRank: 0.039\n",
      "node: \"7\"\tpageRank: 0.016\n",
      "node: \"8\"\tpageRank: 0.016\n",
      "node: \"9\"\tpageRank: 0.016\n",
      "node: \"1\"\tpageRank: 0.033\n",
      "node: \"10\"\tpageRank: 0.016\n",
      "node: \"11\"\tpageRank: 0.016\n",
      "node: \"2\"\tpageRank: 0.384\n",
      "node: \"3\"\tpageRank: 0.343\n",
      "node: \"4\"\tpageRank: 0.039\n"
     ]
    }
   ],
   "source": [
    "import ast\n",
    "\n",
    "output_dir = \"/home/cloudera/w261/HW9/data/toy_prepared_graph_output/\"\n",
    "\n",
    "for name in os.listdir(input_dir):\n",
    "    with open(input_dir+name) as f:\n",
    "        for line in f:\n",
    "            key, value = line.split(\"\\t\")\n",
    "            value = ast.literal_eval(value)\n",
    "            PR = value[1]\n",
    "            print \"node: %s\\tpageRank: %.3f\"%(key,PR)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A, 0.033\n",
    "B, 0.384\n",
    "C, 0.343\n",
    "D, 0.039\n",
    "E, 0.081\n",
    "F, 0.039\n",
    "G, 0.016\n",
    "H, 0.016\n",
    "I, 0.016\n",
    "J, 0.016\n",
    "K, 0.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NUM NODES IS\n",
      "11\n",
      "iteration 0 of Pagerank algorithm\n",
      "iteration 1 of Pagerank algorithm\n",
      "iteration 2 of Pagerank algorithm\n",
      "iteration 3 of Pagerank algorithm\n",
      "iteration 4 of Pagerank algorithm\n",
      "iteration 5 of Pagerank algorithm\n",
      "iteration 6 of Pagerank algorithm\n",
      "iteration 7 of Pagerank algorithm\n",
      "iteration 8 of Pagerank algorithm\n",
      "iteration 9 of Pagerank algorithm\n",
      "iteration 10 of Pagerank algorithm\n",
      "iteration 11 of Pagerank algorithm\n",
      "iteration 12 of Pagerank algorithm\n",
      "iteration 13 of Pagerank algorithm\n",
      "iteration 14 of Pagerank algorithm\n",
      "iteration 15 of Pagerank algorithm\n",
      "iteration 16 of Pagerank algorithm\n",
      "iteration 17 of Pagerank algorithm\n",
      "iteration 18 of Pagerank algorithm\n",
      "iteration 19 of Pagerank algorithm\n",
      "iteration 20 of Pagerank algorithm\n",
      "iteration 21 of Pagerank algorithm\n",
      "iteration 22 of Pagerank algorithm\n",
      "iteration 23 of Pagerank algorithm\n",
      "iteration 24 of Pagerank algorithm\n",
      "iteration 25 of Pagerank algorithm\n",
      "iteration 26 of Pagerank algorithm\n",
      "iteration 27 of Pagerank algorithm\n",
      "iteration 28 of Pagerank algorithm\n",
      "iteration 29 of Pagerank algorithm\n",
      "iteration 30 of Pagerank algorithm\n",
      "iteration 31 of Pagerank algorithm\n",
      "iteration 32 of Pagerank algorithm\n",
      "iteration 33 of Pagerank algorithm\n",
      "iteration 34 of Pagerank algorithm\n",
      "iteration 35 of Pagerank algorithm\n",
      "iteration 36 of Pagerank algorithm\n",
      "iteration 37 of Pagerank algorithm\n",
      "iteration 38 of Pagerank algorithm\n",
      "iteration 39 of Pagerank algorithm\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random,array\n",
    "from PageRank_step2 import MRpageRank2\n",
    "from PageRank_graph_prep_2 import MRpageRankGraphPrep\n",
    "import ast\n",
    "import os\n",
    "\n",
    "!aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_input --recursive --quiet\n",
    "!aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_output --recursive --quiet\n",
    "\n",
    "mr_job = MRpageRankGraphPrep(args=[\"-r\",\"emr\",\n",
    "                                    \"s3://cerc-w261/HW9/toy/raw_input\",\n",
    "                                   \"--cluster-id=j-1I28YD5XHICD3\",\n",
    "                                   \"--aws-region=us-west-2\",\n",
    "                                    \"--output-dir\",\n",
    "                                    \"s3://cerc-w261/HW9/toy/prepared_graph_input\"])\n",
    "\n",
    "with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "        counters = runner.counters()[0]\n",
    "        num_nodes = counters['Nodes']['Num_nodes']\n",
    "print \"NUM NODES IS\"\n",
    "print num_nodes\n",
    "i = 0\n",
    "while(1):\n",
    "\n",
    "    if i == 0:\n",
    "        first_run = 1\n",
    "    else:\n",
    "        first_run = 0\n",
    "    mr_job = MRpageRank2(args=[\"-r\",\"emr\",\n",
    "                               \"s3://cerc-w261/HW9/toy/prepared_graph_input\",\n",
    "                               \"--alpha\", str(0.15),\n",
    "                               \"--num-nodes\", str(num_nodes),\n",
    "                               \"--first-run\", str(first_run),\n",
    "                                         \"--output-dir\",\n",
    "                               \"s3://cerc-w261/HW9/toy/prepared_graph_output\",\n",
    "                              \"--cluster-id=j-1I28YD5XHICD3\",\n",
    "                              \"--aws-region=us-west-2\"])\n",
    "    \n",
    "    print \"iteration \"+str(i)+\" of Pagerank algorithm\"\n",
    "\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "    i = i + 1\n",
    "\n",
    "    !aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_input/ --recursive --quiet\n",
    "    !aws s3 cp s3://cerc-w261/HW9/toy/prepared_graph_output s3://cerc-w261/HW9/toy/prepared_graph_input --recursive --quiet\n",
    "    !aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_output --recursive --quiet\n",
    "\n",
    "    if i >=40:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\"\t[[], 0.090909090909090912]\r\n",
      "\"10\"\t[[\"5\"], 0.090909090909090912]\r\n",
      "\"11\"\t[[\"5\"], 0.090909090909090912]\r\n",
      "\"2\"\t[[\"3\"], 0.090909090909090912]\r\n",
      "\"3\"\t[[\"2\"], 0.090909090909090912]\r\n",
      "\"4\"\t[[\"1\", \"2\"], 0.090909090909090912]\r\n",
      "\"5\"\t[[\"2\", \"4\", \"6\"], 0.090909090909090912]\r\n",
      "\"6\"\t[[\"2\", \"5\"], 0.090909090909090912]\r\n",
      "\"7\"\t[[\"2\", \"5\"], 0.090909090909090912]\r\n",
      "\"8\"\t[[\"2\", \"5\"], 0.090909090909090912]\r\n",
      "\"9\"\t[[\"2\", \"5\"], 0.090909090909090912]\r\n"
     ]
    }
   ],
   "source": [
    "!cat  /home/cloudera/w261/HW9/data/toy_prepared/input_1.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_step2.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_step2.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRpageRank2(MRJob):\n",
    "    \n",
    "    ## Hardcoding the number of nodes in the graph\n",
    "    ## these are substituted with runtime parameters\n",
    "    N = 1\n",
    "    alpha = 0.0\n",
    "    first_run = 0\n",
    "    \n",
    "    ## function permits us to pass an arbitrary alpha\n",
    "    ## to our mrjob\n",
    "    def configure_options(self):\n",
    "        super(MRpageRank2, self).configure_options()\n",
    "        self.add_passthrough_option('--alpha', type='float', default=0.1, help='The damping factor; must be a float!')\n",
    "        self.add_passthrough_option('--first-run', type='int', default=0, help='Indicates whether we are dealing with the first run of algo')\n",
    "        self.add_passthrough_option('--num-nodes', type='int', default=1, help='The number of nodes in the graph; must be an int!')\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "            mapper_init=self.mapper_init,\n",
    "            mapper=self.mapper,\n",
    "            reducer_init=self.reducer_init,\n",
    "            reducer=self.reducer),\n",
    "            MRStep(\n",
    "                reducer_init=self.reducer2_init,\n",
    "            reducer=self.reducer2)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.N = self.options.num_nodes\n",
    "        self.first_run = self.options.first_run\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        key = key.strip('\"')\n",
    "        value = ast.literal_eval(value)\n",
    "        neighbors = value[0]\n",
    "#         print \"THESE ARE THE NEIGHBORS\"\n",
    "#         print neighbors\n",
    "        if self.first_run == 1:\n",
    "            PR = float(value[1]) / float(self.N)\n",
    "        else:\n",
    "            PR = float(value[1])\n",
    "        if neighbors == []:\n",
    "            yield \"*dangling\", (\"-\", float(PR))\n",
    "            yield key, (neighbors, 0.0)\n",
    "        else:\n",
    "#             Before, neighbors where a comma-separated string\n",
    "#             neighbors = neighbors.split(\",\")\n",
    "#             Given the literal_eval above, this one may not be necessary under the new world\n",
    "#             neighbors = ast.literal_eval(neighbors)\n",
    "            num_nieghbors = len(neighbors)\n",
    "            for n in neighbors:\n",
    "                yield n, (\"-\",float(PR)/float(num_nieghbors))\n",
    "            yield key, (neighbors, 0.0)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.N = self.options.num_nodes\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        dangling_mass = 0.0\n",
    "        if key == \"*dangling\":\n",
    "            for v in values:\n",
    "                dangling_mass += v[1]\n",
    "            for i in range(1,self.N + 1):\n",
    "                yield str(i), (\"-\",float(dangling_mass)/float(self.N))\n",
    "        else:\n",
    "            new_PR = 0.0\n",
    "            neighbors = \"\"\n",
    "            for v in values:\n",
    "                new_PR += v[1]\n",
    "                if v[0] != \"-\":\n",
    "                    neighbors = v[0]\n",
    "            yield key, (neighbors, new_PR)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.alpha = self.options.alpha\n",
    "        self.N = self.options.num_nodes\n",
    "        \n",
    "    def reducer2(self, key, values):\n",
    "        new_PR = 0.0\n",
    "        neighbors = \"\"\n",
    "#         values = ast.literal_eval(values)\n",
    "        for v in values:\n",
    "            new_PR += float(v[1])\n",
    "            if v[0] != \"-\":\n",
    "                neighbors = v[0]\n",
    "        new_PR = self.alpha * 1.00/float(self.N) + (1 - self.alpha) * new_PR\n",
    "        yield key, (neighbors, new_PR)\n",
    "#         for v in values:\n",
    "#             yield v, None\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRpageRank2.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x PageRank_step2.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%%writefile PageRank_step3.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "class MRpageRank3(MRJob):\n",
    "    \n",
    "    ## Hardcoding the number of nodes in the graph\n",
    "    ## these are substituted with runtime parameters\n",
    "    N = 1\n",
    "    alpha = 0.0\n",
    "    first_run = 0\n",
    "    dangling_mass = 0.0\n",
    "    \n",
    "    ## function permits us to pass an arbitrary alpha\n",
    "    ## to our mrjob\n",
    "    def configure_options(self):\n",
    "        super(MRpageRank3, self).configure_options()\n",
    "        self.add_passthrough_option('--alpha', type='float', default=0.1, help='The damping factor; must be a float!')\n",
    "        self.add_passthrough_option('--first-run', type='int', default=0, help='Indicates whether we are dealing with the first run of algo')\n",
    "        self.add_passthrough_option('--num-nodes', type='int', default=1, help='The number of nodes in the graph; must be an int!')\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "            mapper_init=self.mapper_init,\n",
    "            mapper=self.mapper,\n",
    "            reducer_init=self.reducer_init,\n",
    "            reducer=self.reducer),\n",
    "            MRStep(\n",
    "                reducer_init=self.reducer2_init,\n",
    "            reducer=self.reducer2)\n",
    "        ]\n",
    "    \n",
    "    def mapper_init(self):\n",
    "        self.N = self.options.num_nodes\n",
    "        self.first_run = self.options.first_run\n",
    "    \n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        key = key.strip('\"')\n",
    "        value = ast.literal_eval(value)\n",
    "        neighbors = value[0]\n",
    "#         print \"THESE ARE THE NEIGHBORS\"\n",
    "#         print neighbors\n",
    "        if self.first_run == 1:\n",
    "            PR = float(value[1]) / float(self.N)\n",
    "        else:\n",
    "            PR = float(value[1])\n",
    "        if neighbors == []:\n",
    "            yield \"*dangling\", (\"-\", float(PR))\n",
    "            yield key, (neighbors, 0.0)\n",
    "        else:\n",
    "#             Before, neighbors where a comma-separated string\n",
    "#             neighbors = neighbors.split(\",\")\n",
    "#             Given the literal_eval above, this one may not be necessary under the new world\n",
    "#             neighbors = ast.literal_eval(neighbors)\n",
    "            num_nieghbors = len(neighbors)\n",
    "            for n in neighbors:\n",
    "                yield n, (\"-\",float(PR)/float(num_nieghbors))\n",
    "            yield key, (neighbors, 0.0)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.N = self.options.num_nodes\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        dangling_mass = 0.0\n",
    "        if key == \"*dangling\":\n",
    "            for v in values:\n",
    "                dangling_mass += v[1]\n",
    "            self.increment_counter(\"dangling_nodes\",\"mass\", dangling_mass)\n",
    "            # Commenting out the below as I am attempting to pass\n",
    "            # the dangling mass through a counter\n",
    "#             for i in range(1,self.N + 1):\n",
    "#                 yield str(i), (\"-\",float(dangling_mass)/float(self.N))\n",
    "        else:\n",
    "            new_PR = 0.0\n",
    "            neighbors = \"\"\n",
    "            for v in values:\n",
    "                new_PR += v[1]\n",
    "                if v[0] != \"-\":\n",
    "                    neighbors = v[0]\n",
    "            yield key, (neighbors, new_PR)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.alpha = self.options.alpha\n",
    "        self.N = self.options.num_nodes\n",
    "        self.dangling_mass = self.counters()[0]['dangling_nodes']['mass']\n",
    "        \n",
    "    def reducer2(self, key, values):\n",
    "        new_PR = 0.0\n",
    "        neighbors = \"\"\n",
    "        for v in values:\n",
    "            new_PR += float(v[1])\n",
    "            if v[0] != \"-\":\n",
    "                neighbors = v[0]\n",
    "        # Here we distribute the dangling mass that was captured through the counter\n",
    "        new_PR = self.alpha * 1.00/float(self.N) + (1 - self.alpha) * (new_PR  + (self.dangling_mass / float(self.N)))\n",
    "        yield key, (neighbors, new_PR)\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRpageRank3.run()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "self.increment_counter(\"Nodes\",\"Num_nodes\",1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting PageRank_step1.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile PageRank_step1.py\n",
    "#!/usr/bin/env python\n",
    "\n",
    "from mrjob.job import MRJob\n",
    "from mrjob.step import MRStep\n",
    "import sys\n",
    "import os\n",
    "import ast\n",
    "from mrjob.protocol import RawValueProtocol\n",
    "from mrjob.protocol import RawProtocol\n",
    "\n",
    "## Check k-means convergence\n",
    "def stop_criterion(nodes_old, nodes_new,T):\n",
    "    \n",
    "    ## produce array of coordinates for old and new centroids\n",
    "    oldvalue = list(chain(*centroid_points_old))\n",
    "    newvalue = list(chain(*centroid_points_new))\n",
    "    \n",
    "    ## compute the absolute difference between the new\n",
    "    ## and old values for each coordinate point\n",
    "    Diff = [abs(a-b) for a, b in zip(oldvalue, newvalue)]\n",
    "    \n",
    "    Flag = True\n",
    "    \n",
    "    ## Iterate through all parameters' absolute differences\n",
    "    for i in Diff:\n",
    "        \n",
    "        ## if any of the dimensions' differences is above the convergence\n",
    "        ## threshold, then fail the convergence check\n",
    "        if(i>T):\n",
    "            ## False flag entails failure to converge\n",
    "            Flag = False\n",
    "            break\n",
    "    return Flag\n",
    "\n",
    "class MRpageRank1(MRJob):\n",
    "    \n",
    "    ## Hardcoding the number of nodes in the graph\n",
    "    ## these are substituted with runtime parameters\n",
    "    N = 1\n",
    "    alpha = 0.0\n",
    "    \n",
    "    ## function permits us to pass an arbitrary alpha\n",
    "    ## to our mrjob\n",
    "    def configure_options(self):\n",
    "        super(MRpageRank1, self).configure_options()\n",
    "        self.add_passthrough_option('--alpha', type='float', default=0.1, help='The damping factor; must be a float!')\n",
    "        self.add_passthrough_option('--num-nodes', type='int', default=1, help='The number of nodes in the graph; must be an int!')\n",
    "    \n",
    "    def steps(self):\n",
    "        return [\n",
    "            MRStep(\n",
    "            mapper=self.mapper,\n",
    "            reducer_init=self.reducer_init,\n",
    "            reducer=self.reducer),\n",
    "            MRStep(\n",
    "                reducer_init=self.reducer2_init,\n",
    "            reducer=self.reducer2)\n",
    "        ]\n",
    "    def mapper(self, _, line):\n",
    "        line = line.strip()\n",
    "        key, value = line.split(\"\\t\")\n",
    "        key = key.strip('\"')\n",
    "        value = ast.literal_eval(value)\n",
    "        neighbors = value[0]\n",
    "#         print \"THESE ARE THE NEIGHBORS\"\n",
    "#         print neighbors\n",
    "        PR = value[1]\n",
    "        if neighbors == []:\n",
    "            yield \"*dangling\", (\"-\", float(PR))\n",
    "            yield key, (neighbors, 0.0)\n",
    "        else:\n",
    "#             Before, neighbors where a comma-separated string\n",
    "#             neighbors = neighbors.split(\",\")\n",
    "#             Given the literal_eval above, this one may not be necessary under the new world\n",
    "#             neighbors = ast.literal_eval(neighbors)\n",
    "            num_nieghbors = len(neighbors)\n",
    "            for n in neighbors:\n",
    "                yield n, (\"-\",float(PR)/float(num_nieghbors))\n",
    "            yield key, (neighbors, 0.0)\n",
    "    \n",
    "    def reducer_init(self):\n",
    "        self.N = self.options.num_nodes\n",
    "    \n",
    "    def reducer(self, key, values):\n",
    "        dangling_mass = 0.0\n",
    "        if key == \"*dangling\":\n",
    "            for v in values:\n",
    "                dangling_mass += v[1]\n",
    "            for i in range(1,self.N + 1):\n",
    "                yield str(i), (\"-\",float(dangling_mass)/float(self.N))\n",
    "        else:\n",
    "            new_PR = 0.0\n",
    "            neighbors = \"\"\n",
    "            for v in values:\n",
    "                new_PR += v[1]\n",
    "                if v[0] != \"-\":\n",
    "                    neighbors = v[0]\n",
    "            yield key, (neighbors, new_PR)\n",
    "    \n",
    "    def reducer2_init(self):\n",
    "        self.alpha = self.options.alpha\n",
    "        self.N = self.options.num_nodes\n",
    "        \n",
    "    def reducer2(self, key, values):\n",
    "        new_PR = 0.0\n",
    "        neighbors = \"\"\n",
    "#         values = ast.literal_eval(values)\n",
    "        for v in values:\n",
    "            new_PR += float(v[1])\n",
    "            if v[0] != \"-\":\n",
    "                neighbors = v[0]\n",
    "        new_PR = self.alpha * 1.00/float(self.N) + (1 - self.alpha) * new_PR\n",
    "        yield key, (neighbors, new_PR)\n",
    "#         for v in values:\n",
    "#             yield v, None\n",
    "        \n",
    "if __name__ == \"__main__\":\n",
    "    MRpageRank1.run()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\"1\"\t[\"-\", 0.0082644628099173556]\n",
    "\"2\"\t[\"-\", 0.0082644628099173556]\n",
    "\"3\"\t[\"-\", 0.0082644628099173556]\n",
    "\"4\"\t[\"-\", 0.0082644628099173556]\n",
    "\"5\"\t[\"-\", 0.0082644628099173556]\n",
    "\"6\"\t[\"-\", 0.0082644628099173556]\n",
    "\"7\"\t[\"-\", 0.0082644628099173556]\n",
    "\"8\"\t[\"-\", 0.0082644628099173556]\n",
    "\"9\"\t[\"-\", 0.0082644628099173556]\n",
    "\"10\"\t[\"-\", 0.0082644628099173556]\n",
    "\"11\"\t[\"-\", 0.0082644628099173556]\n",
    "\"1\"\t[\"\", 0.045454545454545456]\n",
    "\"10\"\t[[\"5\"], 0.0]\n",
    "\"11\"\t[[\"5\"], 0.0]\n",
    "\"2\"\t[[\"3\"], 0.34848484848484851]\n",
    "\"3\"\t[[\"2\"], 0.090909090909090912]\n",
    "\"4\"\t[[\"1\", \"2\"], 0.030303030303030304]\n",
    "\"5\"\t[[\"2\", \"4\", \"6\"], 0.36363636363636365]\n",
    "\"6\"\t[[\"2\", \"5\"], 0.030303030303030304]\n",
    "\"7\"\t[[\"2\", \"5\"], 0.0]\n",
    "\"8\"\t[[\"2\", \"5\"], 0.0]\n",
    "\"9\"\t[[\"2\", \"5\"], 0.0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "!chmod a+x PageRank_step1.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\r\n",
      "Creating temp directory /tmp/PageRank_step1.cloudera.20160716.162802.547998\r\n",
      "Running step 1 of 2...\r\n",
      "Running step 2 of 2...\r\n",
      "Streaming final output from /tmp/PageRank_step1.cloudera.20160716.162802.547998/output...\r\n",
      "\"5\"\t[[\"2\", \"4\", \"6\"], 0.32975206611570246]\r\n",
      "\"6\"\t[[\"2\", \"5\"], 0.046418732782369146]\r\n",
      "\"7\"\t[[\"2\", \"5\"], 0.02066115702479339]\r\n",
      "\"8\"\t[[\"2\", \"5\"], 0.02066115702479339]\r\n",
      "\"9\"\t[[\"2\", \"5\"], 0.02066115702479339]\r\n",
      "\"1\"\t[[], 0.059297520661157024]\r\n",
      "\"10\"\t[[\"5\"], 0.02066115702479339]\r\n",
      "\"11\"\t[[\"5\"], 0.02066115702479339]\r\n",
      "\"2\"\t[[\"3\"], 0.3168732782369146]\r\n",
      "\"3\"\t[[\"2\"], 0.09793388429752066]\r\n",
      "\"4\"\t[[\"1\", \"2\"], 0.046418732782369146]\r\n",
      "Removing temp directory /tmp/PageRank_step1.cloudera.20160716.162802.547998...\r\n"
     ]
    }
   ],
   "source": [
    "# !cat /home/cloudera/w261/HW9/data/toy_prepared/input_1.txt\n",
    "num_nodes = 11\n",
    "!./PageRank_step1.py --alpha=0.15 --num-nodes=$num_nodes \\\n",
    "/home/cloudera/w261/HW9/data/toy_prepared/input_1.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Local PageRank Iterative algorithm - Toy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rm: cannot remove `/home/cloudera/w261/HW9/data/toy_directed_graph_input': No such file or directory\n",
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/PageRank_graph_prep_1.cloudera.20160716.162826.693499\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.162826.693499/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob7007675728165044674.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:2\n",
      "  Submitting tokens for job: job_1468516795796_0061\n",
      "  Submitted application application_1468516795796_0061\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1468516795796_0061/\n",
      "  Running job: job_1468516795796_0061\n",
      "  Job job_1468516795796_0061 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "   map 100% reduce 100%\n",
      "  Job job_1468516795796_0061 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.162826.693499/output\n",
      "Counters: 49\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=252\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=408\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=260\n",
      "\t\tFILE: Number of bytes written=368055\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=616\n",
      "\t\tHDFS: Number of bytes written=408\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=9\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=2\n",
      "\t\tLaunched map tasks=2\n",
      "\t\tLaunched reduce tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=3616000\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=1086848\n",
      "\t\tTotal time spent by all map tasks (ms)=28250\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=3616000\n",
      "\t\tTotal time spent by all reduce tasks (ms)=8491\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=1086848\n",
      "\t\tTotal vcore-seconds taken by all map tasks=28250\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=8491\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=2400\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=168\n",
      "\t\tInput split bytes=364\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=518\n",
      "\t\tMap output materialized bytes=311\n",
      "\t\tMap output records=54\n",
      "\t\tMerged Map outputs=2\n",
      "\t\tPhysical memory (bytes) snapshot=367886336\n",
      "\t\tReduce input groups=22\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=311\n",
      "\t\tShuffled Maps =2\n",
      "\t\tSpilled Records=108\n",
      "\t\tTotal committed heap usage (bytes)=152174592\n",
      "\t\tVirtual memory (bytes) snapshot=2098880512\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.162826.693499/output...\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/PageRank_graph_prep_1.cloudera.20160716.162826.693499...\n",
      "Removing temp directory /tmp/PageRank_graph_prep_1.cloudera.20160716.162826.693499...\n",
      "NUM NODES IS 11\n",
      "iteration 0 of Pagerank algorithm\n",
      "iteration 1 of Pagerank algorithm\n",
      "iteration 2 of Pagerank algorithm\n",
      "iteration 3 of Pagerank algorithm\n",
      "iteration 4 of Pagerank algorithm\n",
      "iteration 5 of Pagerank algorithm\n",
      "iteration 6 of Pagerank algorithm\n",
      "iteration 7 of Pagerank algorithm\n",
      "iteration 8 of Pagerank algorithm\n",
      "iteration 9 of Pagerank algorithm\n",
      "iteration 10 of Pagerank algorithm\n",
      "iteration 11 of Pagerank algorithm\n",
      "iteration 12 of Pagerank algorithm\n",
      "iteration 13 of Pagerank algorithm\n",
      "iteration 14 of Pagerank algorithm\n",
      "iteration 15 of Pagerank algorithm\n",
      "iteration 16 of Pagerank algorithm\n",
      "iteration 17 of Pagerank algorithm\n",
      "iteration 18 of Pagerank algorithm\n",
      "iteration 19 of Pagerank algorithm\n",
      "iteration 20 of Pagerank algorithm\n",
      "iteration 21 of Pagerank algorithm\n",
      "iteration 22 of Pagerank algorithm\n",
      "iteration 23 of Pagerank algorithm\n",
      "iteration 24 of Pagerank algorithm\n",
      "iteration 25 of Pagerank algorithm\n",
      "iteration 26 of Pagerank algorithm\n",
      "iteration 27 of Pagerank algorithm\n",
      "iteration 28 of Pagerank algorithm\n",
      "iteration 29 of Pagerank algorithm\n",
      "iteration 30 of Pagerank algorithm\n",
      "iteration 31 of Pagerank algorithm\n",
      "iteration 32 of Pagerank algorithm\n",
      "iteration 33 of Pagerank algorithm\n",
      "iteration 34 of Pagerank algorithm\n",
      "iteration 35 of Pagerank algorithm\n",
      "iteration 36 of Pagerank algorithm\n",
      "iteration 37 of Pagerank algorithm\n",
      "iteration 38 of Pagerank algorithm\n",
      "iteration 39 of Pagerank algorithm\n",
      "iteration 40 of Pagerank algorithm\n",
      "iteration 41 of Pagerank algorithm\n",
      "iteration 42 of Pagerank algorithm\n",
      "iteration 43 of Pagerank algorithm\n",
      "iteration 44 of Pagerank algorithm\n",
      "iteration 45 of Pagerank algorithm\n",
      "iteration 46 of Pagerank algorithm\n",
      "iteration 47 of Pagerank algorithm\n",
      "iteration 48 of Pagerank algorithm\n",
      "iteration 49 of Pagerank algorithm\n",
      "\"1\"\t[[], 0.03278149315934767]\n",
      "\"10\"\t[[\"5\"], 0.01616947901685893]\n",
      "\"11\"\t[[\"5\"], 0.01616947901685893]\n",
      "\"2\"\t[[\"3\"], 0.3843697809528769]\n",
      "\"3\"\t[[\"2\"], 0.3429414533690357]\n",
      "\"4\"\t[[\"1\", \"2\"], 0.03908709209997012]\n",
      "\"5\"\t[[\"2\", \"4\", \"6\"], 0.08088569323450433]\n",
      "\"6\"\t[[\"2\", \"5\"], 0.03908709209997012]\n",
      "\"7\"\t[[\"2\", \"5\"], 0.01616947901685893]\n",
      "\"8\"\t[[\"2\", \"5\"], 0.01616947901685893]\n",
      "\"9\"\t[[\"2\", \"5\"], 0.01616947901685893]\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random,array\n",
    "from PageRank_step1 import MRpageRank1\n",
    "import ast\n",
    "import os\n",
    "\n",
    "## Setting up input and output folders for \n",
    "## the iterative algorithm\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_output\n",
    "!mkdir /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy_directed_graph_input\n",
    "\n",
    "!./PageRank_graph_prep_1.py -r hadoop /home/cloudera/w261/HW9/data/toy/example_1.txt \\\n",
    "> /home/cloudera/w261/HW9/data/toy_prepared_graph_input/input.txt\n",
    "\n",
    "# !./PageRank_step1.py \\\n",
    "# /home/cloudera/w261/HW9/data/toy_prepared/input_1.txt \\\n",
    "# > /home/cloudera/w261/HW9/data/toy_prepared_graph_input/input.txt\n",
    "\n",
    "input_dir = \"/home/cloudera/w261/HW9/data/toy_prepared_graph_input/\"\n",
    "\n",
    "for name in os.listdir(input_dir):\n",
    "    with open(input_dir+name) as f:\n",
    "        num_nodes = 0\n",
    "        for line in f:\n",
    "            num_nodes += 1\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "#     frontier_exists = False\n",
    "#     j = i + 1\n",
    "    mr_job = MRpageRank1(args=[\"/home/cloudera/w261/HW9/data/toy_prepared_graph_input\",\n",
    "                               \"--alpha\",0.15,\n",
    "                               \"--num-nodes\", str(num_nodes),\n",
    "                                         \"--output-dir\", \"/home/cloudera/w261/HW9/data/toy_prepared_graph_output\"])\n",
    "    print \"iteration \"+str(i)+\" of Pagerank algorithm\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "#         for line in runner.stream_output():\n",
    "#             node, payload = line.split(\"\\t\")\n",
    "#             node = node.strip('\"')\n",
    "#             payload_raw = payload\n",
    "#             payload = ast.literal_eval(payload)\n",
    "#             state = payload[2]\n",
    "#             if state == \"Q\":\n",
    "#                 frontier_exists = True\n",
    "    i = i + 1\n",
    "\n",
    "    !rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "    !cp -rT /home/cloudera/w261/HW9/data/toy_prepared_graph_output /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "#     if not frontier_exists:\n",
    "#         break\n",
    "    if i >=50:\n",
    "        break\n",
    "!cat /home/cloudera/w261/HW9/data/toy_prepared_graph_output/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### EMR PageRank Iterative algorithm - Toy graph"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /tmp/no_script.cloudera.20160716.165027.210554\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/no_script.cloudera.20160716.165027.210554/files/...\n",
      "j-2TFUMR7W2PF1V\n"
     ]
    }
   ],
   "source": [
    "## first we spool up a cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-2 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "upload: ../data/toy/example_1.txt to s3://cerc-w261/HW9/toy/raw_input/example_1.txt\r\n"
     ]
    }
   ],
   "source": [
    "!aws s3 cp /home/cloudera/w261/HW9/data/toy/example_1.txt s3://cerc-w261/HW9/toy/raw_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "part-00000  part-00001\r\n"
     ]
    }
   ],
   "source": [
    "!ls /home/cloudera/w261/HW9/data/toy_prepared_graph_input/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/PageRank_graph_prep_1.cloudera.20160716.171336.184035\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/PageRank_graph_prep_1.cloudera.20160716.171336.184035/files/...\n",
      "Adding our job to existing cluster j-2TFUMR7W2PF1V\n",
      "Waiting for step 1 of 1 (s-3DL634XJO222Y) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40247/cluster\n",
      "  RUNNING for 8.5s\n",
      "   100.0% complete\n",
      "  RUNNING for 43.4s\n",
      "     5.0% complete\n",
      "  RUNNING for 74.7s\n",
      "    80.0% complete\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3DL634XJO222Y on ec2-54-149-192-241.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-192-241.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3DL634XJO222Y/syslog\n",
      "Counters: 54\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=756\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=274\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=365\n",
      "\t\tFILE: Number of bytes written=1141585\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=784\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=756\n",
      "\t\tS3: Number of bytes written=274\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=8\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tLaunched reduce tasks=3\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=200118240\n",
      "\t\tTotal megabyte-seconds taken by all reduce tasks=87223680\n",
      "\t\tTotal time spent by all map tasks (ms)=138971\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=6253695\n",
      "\t\tTotal time spent by all reduce tasks (ms)=30286\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=2725740\n",
      "\t\tTotal vcore-seconds taken by all map tasks=138971\n",
      "\t\tTotal vcore-seconds taken by all reduce tasks=30286\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=11080\n",
      "\t\tCombine input records=0\n",
      "\t\tCombine output records=0\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=2589\n",
      "\t\tInput split bytes=784\n",
      "\t\tMap input records=10\n",
      "\t\tMap output bytes=518\n",
      "\t\tMap output materialized bytes=889\n",
      "\t\tMap output records=54\n",
      "\t\tMerged Map outputs=24\n",
      "\t\tPhysical memory (bytes) snapshot=4838699008\n",
      "\t\tReduce input groups=22\n",
      "\t\tReduce input records=54\n",
      "\t\tReduce output records=11\n",
      "\t\tReduce shuffle bytes=889\n",
      "\t\tShuffled Maps =24\n",
      "\t\tSpilled Records=108\n",
      "\t\tTotal committed heap usage (bytes)=5593104384\n",
      "\t\tVirtual memory (bytes) snapshot=25479315456\n",
      "\tShuffle Errors\n",
      "\t\tBAD_ID=0\n",
      "\t\tCONNECTION=0\n",
      "\t\tIO_ERROR=0\n",
      "\t\tWRONG_LENGTH=0\n",
      "\t\tWRONG_MAP=0\n",
      "\t\tWRONG_REDUCE=0\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/PageRank_graph_prep_1.cloudera.20160716.171336.184035/...\n",
      "Removing temp directory /tmp/PageRank_graph_prep_1.cloudera.20160716.171336.184035...\n",
      "Killing our SSH tunnel (pid 12444)\n"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "!aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_input --recursive --quiet\n",
    "!aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_output --recursive --quiet\n",
    "\n",
    "# !./PageRank_graph_prep_1.py -r hadoop /home/cloudera/w261/HW9/data/toy/example_1.txt \\\n",
    "# > /home/cloudera/w261/HW9/data/toy_prepared_graph_input/input.txt\n",
    "\n",
    "!python PageRank_graph_prep_1.py \\\n",
    "-r emr s3://cerc-w261/HW9/toy/raw_input \\\n",
    "    --cluster-id=j-2TFUMR7W2PF1V \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW9/toy/prepared_graph_input \\\n",
    "    --no-output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Traceback (most recent call last):\n",
      "  File \"PageRank_graph_prep_1.py\", line 62, in <module>\n",
      "    MRpageRankGraphPrep.run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 430, in run\n",
      "    mr_job.execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/job.py\", line 448, in execute\n",
      "    super(MRJob, self).execute()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 160, in execute\n",
      "    self.run_job()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/launch.py\", line 230, in run_job\n",
      "    runner.run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/runner.py\", line 473, in run\n",
      "    self._run()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.py\", line 862, in _run\n",
      "    self._launch()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.py\", line 866, in _launch\n",
      "    self._prepare_for_launch()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.py\", line 870, in _prepare_for_launch\n",
      "    self._check_input_exists()\n",
      "  File \"/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.py\", line 890, in _check_input_exists\n",
      "    'Input path %s does not exist!' % (path,))\n",
      "AssertionError: Input path j-2TFUMR7W2PF1V does not exist!\n",
      "iteration 0 of Pagerank algorithm\n"
     ]
    },
    {
     "ename": "AssertionError",
     "evalue": "Input path s3://cerc-w261/HW9/toy/prepared_graph_input does not exist!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-303-5b50d25b5868>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     60\u001b[0m     \u001b[0mgraph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     61\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mmr_job\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmake_runner\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mrunner\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 62\u001b[1;33m         \u001b[0mrunner\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     63\u001b[0m     \u001b[0mi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/runner.pyc\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Job already ran!\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ran_job\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_run\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    860\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    861\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 862\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    863\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_wait_for_steps_to_complete\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_launch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    864\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    865\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 866\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_prepare_for_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    867\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_launch_emr_job\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_prepare_for_launch\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    869\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_prepare_for_launch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 870\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_input_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    871\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_check_output_not_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    872\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_create_setup_wrapper_script\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m/home/cloudera/anaconda2/lib/python2.7/site-packages/mrjob/emr.pyc\u001b[0m in \u001b[0;36m_check_input_exists\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    888\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    889\u001b[0m                     raise AssertionError(\n\u001b[1;32m--> 890\u001b[1;33m                         'Input path %s does not exist!' % (path,))\n\u001b[0m\u001b[0;32m    891\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    892\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_output_not_exists\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAssertionError\u001b[0m: Input path s3://cerc-w261/HW9/toy/prepared_graph_input does not exist!"
     ]
    }
   ],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "from numpy import random,array\n",
    "from PageRank_step1 import MRpageRank1\n",
    "import ast\n",
    "import os\n",
    "\n",
    "## Setting up input and output folders for \n",
    "## the iterative algorithm\n",
    "# !rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "# !rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_output\n",
    "# !mkdir /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "# !rm -r /home/cloudera/w261/HW9/data/toy_directed_graph_input\n",
    "\n",
    "!aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_input --recursive --quiet\n",
    "!aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_output --recursive --quiet\n",
    "\n",
    "# !./PageRank_graph_prep_1.py -r hadoop /home/cloudera/w261/HW9/data/toy/example_1.txt \\\n",
    "# > /home/cloudera/w261/HW9/data/toy_prepared_graph_input/input.txt\n",
    "\n",
    "!python PageRank_graph_prep_1.py \\\n",
    "-r emr s3://cerc-w261/HW9/toy/raw_input \\\n",
    "    --cluster-id=j-2TFUMR7W2PF1V \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW9/toy/prepared_graph_input \\\n",
    "    --no-output\n",
    "    \n",
    "# !aws s3 rm s3://cerc-w261/HW7/synonyms_eda --recursive\n",
    "# !python /home/cloudera/w261/HW7/src/synonyms_eda.py \\\n",
    "# -r emr s3://cerc-w261/HW7/synonyms_input \\\n",
    "#     --cluster-id=j-TK4RCWUIOMPL \\\n",
    "#     --aws-region=us-west-2 \\\n",
    "#     --output-dir=s3://cerc-w261/HW7/synonyms_eda \\\n",
    "#         --no-output\n",
    "\n",
    "# !./PageRank_step1.py \\\n",
    "# /home/cloudera/w261/HW9/data/toy_prepared/input_1.txt \\\n",
    "# > /home/cloudera/w261/HW9/data/toy_prepared_graph_input/input.txt\n",
    "\n",
    "## THIS SHIT AINT GONNA WORK CAUSE I WOULD NEED TO DOWNLOAD THE WHOLE\n",
    "## GRAPH TO LOCAL TO OBTAIN THE NUMBER OF NODES IN THE GRAPH\n",
    "## GONNA NEED TO WRITE NUM_NODES DIRECTLY TO S3 FROM THE PREPROCESSING\n",
    "## JOB!!!!\n",
    "input_dir = \"/home/cloudera/w261/HW9/data/toy_prepared_graph_input/\"\n",
    "\n",
    "for name in os.listdir(input_dir):\n",
    "    with open(input_dir+name) as f:\n",
    "        num_nodes = 0\n",
    "        for line in f:\n",
    "            num_nodes += 1\n",
    "\n",
    "i = 0\n",
    "while(1):\n",
    "#     frontier_exists = False\n",
    "#     j = i + 1\n",
    "    mr_job = MRpageRank1(args=[\"-r\",\"emr\",\n",
    "                               \"s3://cerc-w261/HW9/toy/prepared_graph_input\",\n",
    "                               \"--alpha\",0.15,\n",
    "                               \"--num-nodes\", str(num_nodes), \"--output-dir\",\n",
    "                               \"s3://cerc-w261/HW9/toy/prepared_graph_output\",\n",
    "                              \"--cluster-id=j-2TFUMR7W2PF1V\",\n",
    "                              \"--aws-region=us-west-2\"])\n",
    "    print \"iteration \"+str(i)+\" of Pagerank algorithm\"\n",
    "    graph = []\n",
    "    with mr_job.make_runner() as runner: \n",
    "        runner.run()\n",
    "    i = i + 1\n",
    "\n",
    "#     !rm -r /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "#     !cp -rT /home/cloudera/w261/HW9/data/toy_prepared_graph_output /home/cloudera/w261/HW9/data/toy_prepared_graph_input\n",
    "    \n",
    "    !aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_input/ --recursive --quiet\n",
    "    !aws s3 cp s3://cerc-w261/HW9/toy/prepared_graph_output s3://cerc-w261/HW9/toy/prepared_graph_input --recursive --quiet\n",
    "    !aws s3 rm s3://cerc-w261/HW9/toy/prepared_graph_output --recursive --quiet\n",
    "#     if not frontier_exists:\n",
    "#         break\n",
    "    if i >=20:\n",
    "        break\n",
    "# !cat /home/cloudera/w261/HW9/data/toy_prepared_graph_output/*"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "A, 0.033\n",
    "B, 0.384\n",
    "C, 0.343\n",
    "D, 0.039\n",
    "E, 0.081\n",
    "F, 0.039\n",
    "G, 0.016\n",
    "H, 0.016\n",
    "I, 0.016\n",
    "J, 0.016\n",
    "K, 0.016"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"1\"\t[\"\", 0.090909090909090912]\r\n",
      "\"10\"\t[\"5\", 0.090909090909090912]\r\n",
      "\"11\"\t[\"5\", 0.090909090909090912]\r\n",
      "\"2\"\t[\"3\", 0.090909090909090912]\r\n",
      "\"3\"\t[\"2\", 0.090909090909090912]\r\n",
      "\"4\"\t[\"1,2\", 0.090909090909090912]\r\n",
      "\"5\"\t[\"2,4,6\", 0.090909090909090912]\r\n",
      "\"6\"\t[\"2,5\", 0.090909090909090912]\r\n",
      "\"7\"\t[\"2,5\", 0.090909090909090912]\r\n",
      "\"8\"\t[\"2,5\", 0.090909090909090912]\r\n",
      "\"9\"\t[\"2,5\", 0.090909090909090912]\r\n"
     ]
    }
   ],
   "source": [
    "!cat /home/cloudera/w261/HW9/data/toy_prepared/input.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Creating temp directory /tmp/PageRank_step1.cloudera.20160716.044136.325462\n",
      "Looking for hadoop binary in $PATH...\n",
      "Found hadoop binary: /usr/bin/hadoop\n",
      "Using Hadoop version 2.6.0\n",
      "Copying local files to hdfs:///user/cloudera/tmp/mrjob/PageRank_step1.cloudera.20160716.044136.325462/files/...\n",
      "Looking for Hadoop streaming jar in /home/hadoop/contrib...\n",
      "Looking for Hadoop streaming jar in /usr/lib/hadoop-mapreduce...\n",
      "Found Hadoop streaming jar: /usr/lib/hadoop-mapreduce/hadoop-streaming.jar\n",
      "Running step 1 of 1...\n",
      "  packageJobJar: [] [/usr/jars/hadoop-streaming-2.6.0-cdh5.7.0.jar] /tmp/streamjob8553147381462418970.jar tmpDir=null\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Connecting to ResourceManager at quickstart.cloudera/10.0.2.15:8032\n",
      "  Total input paths to process : 1\n",
      "  number of splits:1\n",
      "  Submitting tokens for job: job_1468516795796_0056\n",
      "  Submitted application application_1468516795796_0056\n",
      "  The url to track the job: http://quickstart.cloudera:8088/proxy/application_1468516795796_0056/\n",
      "  Running job: job_1468516795796_0056\n",
      "  Job job_1468516795796_0056 running in uber mode : false\n",
      "   map 0% reduce 0%\n",
      "   map 100% reduce 0%\n",
      "  Job job_1468516795796_0056 completed successfully\n",
      "  Output directory: hdfs:///user/cloudera/tmp/mrjob/PageRank_step1.cloudera.20160716.044136.325462/output\n",
      "Counters: 30\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=0\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=0\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=121999\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=172\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=5\n",
      "\t\tHDFS: Number of write operations=2\n",
      "\tJob Counters \n",
      "\t\tLaunched map tasks=1\n",
      "\t\tOther local map tasks=1\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=678912\n",
      "\t\tTotal time spent by all map tasks (ms)=5304\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=678912\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=5304\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=450\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=32\n",
      "\t\tInput split bytes=172\n",
      "\t\tMap input records=0\n",
      "\t\tMap output records=0\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=119562240\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=50724864\n",
      "\t\tVirtual memory (bytes) snapshot=692166656\n",
      "Streaming final output from hdfs:///user/cloudera/tmp/mrjob/PageRank_step1.cloudera.20160716.044136.325462/output...\n",
      "Removing HDFS temp directory hdfs:///user/cloudera/tmp/mrjob/PageRank_step1.cloudera.20160716.044136.325462...\n",
      "Removing temp directory /tmp/PageRank_step1.cloudera.20160716.044136.325462...\n"
     ]
    }
   ],
   "source": [
    "!./PageRank_step1.py -r hadoop \\\n",
    "/home/cloudera/w261/HW9/data/toy_prepared/input_1.txt #> \\\n",
    "# /home/cloudera/w261/HW9/data/toy_prepared/input_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "!cat /home/cloudera/w261/HW9/data/toy_prepared/input_2.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating persistent cluster to run several jobs in...\n",
      "Creating temp directory /tmp/no_script.cloudera.20160714.031502.443347\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/no_script.cloudera.20160714.031502.443347/files/...\n",
      "j-17BJ3A7Y7825Q\n"
     ]
    }
   ],
   "source": [
    "## first we spool up a cluster\n",
    "!mrjob create-cluster \\\n",
    "--max-hours-idle 1 \\\n",
    "--aws-region=us-west-2 -c ~/.mrjob.conf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "delete: s3://cerc-w261/HW9/toy/output/_SUCCESS\n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00005 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00006 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00003 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00001 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00004 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00007 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00000 \n",
      "delete: s3://cerc-w261/HW9/toy/output/part-00002 \n",
      "rm: cannot remove `/home/cloudera/w261/HW9/data/toy-output': No such file or directory\n",
      "Using configs in /home/cloudera/.mrjob.conf\n",
      "Unexpected option hadoop from /home/cloudera/.mrjob.conf\n",
      "Using s3://mrjob-4b78edd1b5baad75/tmp/ as our temp dir on S3\n",
      "Creating temp directory /tmp/PageRank_graph_prep.cloudera.20160714.043202.503127\n",
      "Copying local files to s3://mrjob-4b78edd1b5baad75/tmp/PageRank_graph_prep.cloudera.20160714.043202.503127/files/...\n",
      "Adding our job to existing cluster j-17BJ3A7Y7825Q\n",
      "Waiting for step 1 of 1 (s-3M09ZY4ZZUA9U) to complete...\n",
      "  Opening ssh tunnel to resource manager...\n",
      "  Connect to resource manager at: http://localhost:40059/cluster\n",
      "  RUNNING for 11.1s\n",
      "Unable to connect to resource manager\n",
      "  RUNNING for 42.8s\n",
      "  RUNNING for 73.9s\n",
      "  COMPLETED\n",
      "Attempting to fetch counters from logs...\n",
      "Looking for step log in /mnt/var/log/hadoop/steps/s-3M09ZY4ZZUA9U on ec2-54-149-78-127.us-west-2.compute.amazonaws.com...\n",
      "  Parsing step log: ssh://ec2-54-149-78-127.us-west-2.compute.amazonaws.com/mnt/var/log/hadoop/steps/s-3M09ZY4ZZUA9U/syslog\n",
      "Counters: 35\n",
      "\tFile Input Format Counters \n",
      "\t\tBytes Read=756\n",
      "\tFile Output Format Counters \n",
      "\t\tBytes Written=276\n",
      "\tFile System Counters\n",
      "\t\tFILE: Number of bytes read=0\n",
      "\t\tFILE: Number of bytes written=826013\n",
      "\t\tFILE: Number of large read operations=0\n",
      "\t\tFILE: Number of read operations=0\n",
      "\t\tFILE: Number of write operations=0\n",
      "\t\tHDFS: Number of bytes read=1224\n",
      "\t\tHDFS: Number of bytes written=0\n",
      "\t\tHDFS: Number of large read operations=0\n",
      "\t\tHDFS: Number of read operations=8\n",
      "\t\tHDFS: Number of write operations=0\n",
      "\t\tS3: Number of bytes read=756\n",
      "\t\tS3: Number of bytes written=276\n",
      "\t\tS3: Number of large read operations=0\n",
      "\t\tS3: Number of read operations=0\n",
      "\t\tS3: Number of write operations=0\n",
      "\tJob Counters \n",
      "\t\tData-local map tasks=8\n",
      "\t\tLaunched map tasks=8\n",
      "\t\tTotal megabyte-seconds taken by all map tasks=189730080\n",
      "\t\tTotal time spent by all map tasks (ms)=131757\n",
      "\t\tTotal time spent by all maps in occupied slots (ms)=5929065\n",
      "\t\tTotal time spent by all reduces in occupied slots (ms)=0\n",
      "\t\tTotal vcore-seconds taken by all map tasks=131757\n",
      "\tMap-Reduce Framework\n",
      "\t\tCPU time spent (ms)=9900\n",
      "\t\tFailed Shuffles=0\n",
      "\t\tGC time elapsed (ms)=1809\n",
      "\t\tInput split bytes=1224\n",
      "\t\tMap input records=10\n",
      "\t\tMap output records=20\n",
      "\t\tMerged Map outputs=0\n",
      "\t\tPhysical memory (bytes) snapshot=2295492608\n",
      "\t\tSpilled Records=0\n",
      "\t\tTotal committed heap usage (bytes)=2889875456\n",
      "\t\tVirtual memory (bytes) snapshot=15673810944\n",
      "Streaming final output from s3://cerc-w261/HW9/toy/output/...\n",
      "0\t[\"count\", 1]\n",
      "2\t[\"3\", 1]\n",
      "0\t[\"count\", 1]\n",
      "3\t[\"2\", 1]\n",
      "0\t[\"count\", 1]\n",
      "4\t[\"1,2\", 1]\n",
      "0\t[\"count\", 1]\n",
      "5\t[\"2,4,6\", 1]\n",
      "0\t[\"count\", 1]\n",
      "6\t[\"2,5\", 1]\n",
      "0\t[\"count\", 1]\n",
      "7\t[\"2,5\", 1]\n",
      "0\t[\"count\", 1]\n",
      "8\t[\"2,5\", 1]\n",
      "0\t[\"count\", 1]\n",
      "9\t[\"2,5\", 1]\n",
      "0\t[\"count\", 1]\n",
      "10\t[\"5\", 1]\n",
      "0\t[\"count\", 1]\n",
      "11\t[\"5\", 1]\n",
      "Removing s3 temp directory s3://mrjob-4b78edd1b5baad75/tmp/PageRank_graph_prep.cloudera.20160714.043202.503127/...\n",
      "Removing temp directory /tmp/PageRank_graph_prep.cloudera.20160714.043202.503127...\n",
      "Killing our SSH tunnel (pid 7072)\n"
     ]
    }
   ],
   "source": [
    "!aws s3 rm s3://cerc-w261/HW9/toy/output --recursive\n",
    "!rm -r /home/cloudera/w261/HW9/data/toy-output\n",
    "!python PageRank_graph_prep_1.py \\\n",
    "-r emr /home/cloudera/w261/HW9/data/toy/example.txt \\\n",
    "    --cluster-id=j-17BJ3A7Y7825Q \\\n",
    "    --aws-region=us-west-2 \\\n",
    "    --output-dir=s3://cerc-w261/HW9/toy/output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## THINGS TO CONSIDER:\n",
    "### Passed in N as a configuration parameter, that made that easy.\n",
    "### def get_total_nodes(ofile):\n",
    "#    ls_output = !hdfs dfs -ls $ofile\n",
    "#    num_output_files = len(ls_output)-3\n",
    "#    for i in range(num_output_files):\n",
    "#        snum = str(i)\n",
    "#        zeros = '0'*(5-len(snum))\n",
    "#        rfile = ofile+'/part-'+zeros+snum\n",
    "#        grep_output = !hdfs dfs -cat $rfile | grep '*total_nodes'\n",
    "#        if len(grep_output) == 2:\n",
    "#            _, total_nodes = grep_output[1].strip().split('\\t')\n",
    "#            break\n",
    "#    return total_nodes\n",
    "## COOL CODE:\n",
    "#    def steps(self):\n",
    "#        return ([MRStep(mapper=self.send_score, reducer=self.receive_score)] *\n",
    "#                self.options.iterations)\n",
    "## Here's my whole algorithm. Hope this helps:\n",
    "\n",
    "# Mapper 1: if this node has outlinks, yield current pr/outdegree to each of them. If not, yeild (\"dangling\", mass). Also yeild the graph structure.\n",
    "# Reducer 1: if the key is \"dangling\", sum all the values and yield (i, mass/N) for all values of i in the graph. Otherwise, sum the masses and yield the graph structure with the sum\n",
    "# Reducer 2: add the mass to the number in the current graph and calulate the final pagerank for this iteration new_p = (1-df)*(1.0/N) + df*mass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.1 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.2: Exploring PageRank teleportation and network plots </h2>\n",
    "\n",
    "* In order to overcome  problems such as disconnected components, the damping factor (a typical value for d is 0.85) can be varied. \n",
    "* Using the graph in HW1, plot the test graph (using networkx, https://networkx.github.io/) for several values of the damping parameter alpha, so that each nodes radius is proportional to its PageRank score. \n",
    "* In particular you should do this for the following damping factors: [0,0.25,0.5,0.75, 0.85, 1]. \n",
    "* Note your plots should look like the following: https://en.wikipedia.org/wiki/PageRank#/media/File:PageRanks-Example.svg"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.2 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.2 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.3: Applying PageRank to the Wikipedia hyperlinks network </h2>\n",
    "\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 5 iterations, and display the top 100 ranked nodes (with alpha = 0.85).\n",
    "* Run your PageRank implementation on the Wikipedia dataset for 10 iterations, and display the top 100 ranked nodes (with teleportation factor of 0.15).\n",
    "* Have the top 100 ranked pages changed? Comment on your findings. \n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run. Then plot the pagerank values for the same 100 pages that resulted from the 10 iterations run.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.3 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " <h2 style=\"color:darkgreen\">  HW 9.3 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.4: Topic-specific PageRank implementation using MRJob </h2>\n",
    "\n",
    "Modify your PageRank implementation to produce a topic specific PageRank implementation, as described in:\n",
    "\n",
    "http://www-cs-students.stanford.edu/~taherh/papers/topic-sensitive-pagerank.pdf\n",
    "\n",
    "Note in this article that there is a special caveat to ensure that the transition matrix is irreducible.   \n",
    "This caveat lies in footnote 3 on page 3:\n",
    "```\n",
    "\tA minor caveat: to ensure that M is irreducible when p\n",
    "\tcontains any 0 entries, nodes not reachable from nonzero\n",
    "\tnodes in p should be removed. In practice this is not problematic.\n",
    "```\n",
    "and must be adhered to for convergence to be guaranteed.   \n",
    "\n",
    "Run topic specific PageRank on the following randomly generated network of 100 nodes:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet.txt (also available on Dropbox)\n",
    "\n",
    "which are organized into ten topics, as described in the file:\n",
    "\n",
    "> s3://ucb-mids-mls-networks/randNet_topics.txt  (also available on Dropbox)\n",
    "\n",
    "Since there are 10 topics, your result should be 11 PageRank vectors (one for the vanilla PageRank implementation in 9.1, and one for each topic with the topic specific implementation). Print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses.\n",
    "\n",
    "One final and important comment here:  please consider the requirements for irreducibility with topic-specific PageRank. In particular, the literature ensures irreducibility by requiring that nodes not reachable from in-topic nodes be removed from the network.\n",
    "\n",
    "This is not a small task, especially as it it must be performed separately for each of the (10) topics.\n",
    "\n",
    "So, instead of using this method for irreducibility, please comment on why the literature's method is difficult to implement, and what what extra computation it will require.   \n",
    "\n",
    "Then for your code, please use the alternative, non-uniform damping vector:\n",
    "\n",
    "```\n",
    "vji = beta*(1/|Tj|); if node i lies in topic Tj\n",
    "\n",
    "vji = (1-beta)*(1/(N - |Tj|)); if node i lies outside of topic Tj\n",
    "```\n",
    "for beta in (0,1) close to 1. \n",
    "\n",
    "With this approach, you will not have to delete any nodes. If beta > 0.5, PageRank is topic-sensitive, and if beta < 0.5, the PageRank is anti-topic-sensitive. For any value of beta irreducibility should hold, so please try beta=0.99, and perhaps some other values locally, on the smaller networks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.4 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.4 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h3 style='color:darkblue'>---------  OPTIONAL QUESTIONS SECTION --------</h3></div></center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.5: (OPTIONAL) Applying topic-specific PageRank to Wikipedia</h2>\n",
    "\n",
    "Here you will apply your topic-specific PageRank implementation to Wikipedia, defining topics (very arbitrarily) for each page by the length (number of characters) of the name of the article mod 10, so that there are 10 topics. \n",
    "\n",
    "* Once again, print out the top ten ranking nodes and their topics for each of the 11 versions, and comment on your result. Assume a teleportation factor of 0.15 in all your analyses. Run for 10 iterations.\n",
    "* Plot the pagerank values for the top 100 pages resulting from the 5 iterations run in HW 9.3. \n",
    "* Then plot the pagerank values for the same 100 pages that result from the topic specific pagerank after 10 iterations run. \n",
    "* Comment on your findings. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.5 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.5 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:red\"> HW 9.6:  (OPTIONAL) TextRank</h2>\n",
    "\n",
    "* What is TextRank? Describe the main steps in the algorithm. Why does TextRank work?\n",
    "* Implement TextRank in MrJob for keyword phrases (not just unigrams) extraction using co-occurrence based similarity measure with with sizes of N = 2 and 3. And evaluate your code using the following example using precision, recall, and FBeta (Beta=1):\n",
    "```\n",
    "\"Compatibility of systems of linear constraints over the set of natural numbers\n",
    "Criteria of compatibility of a system of linear Diophantine equations, strict \n",
    "inequations, and nonstrict inequations are considered. Upper bounds for\n",
    "components of a minimal set of solutions and algorithms of construction of \n",
    "minimal generating sets of solutions for all types of systems are given. \n",
    "These criteria and the corresponding algorithms for constructing a minimal \n",
    "supporting set of solutions can be used in solving all the considered types of \n",
    "systems and systems of mixed types.\" \n",
    "```\n",
    "* The extracted keywords should in the following set:\n",
    "```\n",
    "linear constraints, linear diophantine equations, natural numbers, non-strict inequations, strict inequations, upper bounds\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\"> HW 9.6 Implementation </h2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Code goes here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Drivers & Runners"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Run Scripts, S3 Sync"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2 style=\"color:darkgreen\">  HW 9.6 Analysis </h2>\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "<br><br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><div class='jumbotron'><h2 style='color:green'>-------  END OF HWK 9 --------</h2></div></center>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
